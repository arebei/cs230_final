{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c449fb5-4ac1-4574-a873-c576ed567dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_warmup as warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab64b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496d1e8-00ab-4590-8c5b-323c4a06352e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55bde549-e0a3-47a8-bac2-1631e8605049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n",
      "train images: 17651 images.\n",
      "test images: 3115 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20766/20766 [00:08<00:00, 2539.88it/s]\n"
     ]
    }
   ],
   "source": [
    "split = 0.85\n",
    "data_dir = 'chart_data/'\n",
    "\n",
    "print('loading dataset...')\n",
    "src_paths = []\n",
    "for file in os.listdir(data_dir):\n",
    "    path = os.path.join(data_dir, file)\n",
    "    #if imghdr.what(path) == None:\n",
    "    #    continue\n",
    "    src_paths.append(path)\n",
    "random.shuffle(src_paths)\n",
    "\n",
    "# separate the paths\n",
    "border = int(split * len(src_paths))\n",
    "train_paths = src_paths[:border]\n",
    "test_paths = src_paths[border:]\n",
    "print('train images: %d images.' % len(train_paths))\n",
    "print('test images: %d images.' % len(test_paths))\n",
    "\n",
    "# create dst directories\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "if (os.path.exists(train_dir) == False) and (os.path.exists(test_dir) == False):\n",
    "    os.makedirs(train_dir)\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "    # move the image files\n",
    "    pbar = tqdm.tqdm(total=len(src_paths))\n",
    "    for dset_paths, dset_dir in zip([train_paths, test_paths], [train_dir, test_dir]):\n",
    "        for src_path in dset_paths:\n",
    "            dst_path = os.path.join(dset_dir, os.path.basename(src_path))\n",
    "            shutil.move(src_path, dst_path)\n",
    "            pbar.update()\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ab3c4-1498-432b-a2ee-d744470f5a58",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c71b58-dec3-4ae2-b787-e9b3feea65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "\n",
    "    '''\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "    '''\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        #noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "\n",
    "        interval = self.patch_embed.patch_size[0]-2\n",
    "        noise = torch.zeros((N, L), device=x.device)\n",
    "        #noise[:, interval - 2::interval] = 1\n",
    "        noise[:, interval - 1::interval] = 1\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "    \n",
    "        \n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio):\n",
    "        mask_ratio = 1/(self.patch_embed.patch_size[0]-2)\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a0f4ae-d9d5-456a-b129-77dc7e308ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    image = torch.einsum('chw->hwc', image) \n",
    "    \n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    \n",
    "    plt.imshow(torch.clip((image.cpu() * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=11)\n",
    "    plt.axis('off')\n",
    "    return torch.clip((image.cpu() * imagenet_std + imagenet_mean) * 255, 0, 255).int()\n",
    "\n",
    "def load_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = img.resize((224, 224))\n",
    "    img = np.array(img) / 255.\n",
    "\n",
    "    assert img.shape == (224, 224, 3)\n",
    "\n",
    "    # normalize by ImageNet mean and std\n",
    "    img = img - imagenet_mean\n",
    "    img = img / imagenet_std\n",
    "    \n",
    "    return torch.tensor(img)\n",
    "\n",
    "def load_data(data_path):\n",
    "    files = os.listdir(data_path)\n",
    "    num_images = len(files)\n",
    "    \n",
    "    train = torch.empty((num_images, 224, 224, 3))\n",
    "    \n",
    "    for id, file in enumerate(files):\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        train[id] = load_image(file_path)\n",
    "        \n",
    "    train = torch.einsum('nhwc->nchw', train)\n",
    "    return train \n",
    "        \n",
    "\n",
    "def run_one_image(img, model):\n",
    "    x = img.unsqueeze(dim=0)\n",
    "\n",
    "    # run MAE\n",
    "    with torch.no_grad():\n",
    "        _, y, mask = model(x.float(), mask_ratio=1/(model.patch_embed.patch_size[0] - 2))\n",
    "        y = model.unpatchify(y)\n",
    "        y = y.detach()\n",
    "\n",
    "    # visualize the mask\n",
    "    mask = mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = mask.detach()\n",
    "    \n",
    "    print(mask.size(), x.size())\n",
    "    \n",
    "    # masked image\n",
    "    im_masked = x * (1 - mask)\n",
    "\n",
    "    # MAE reconstruction pasted with visible patches\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 24]\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(y[0], \"reconstruction\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    output = show_image(im_paste[0], \"reconstruction + visible\")\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62eb462-025f-4e39-ae70-82eb291a9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "#train = load_data(\"chart_data/train\")\n",
    "test = load_data(\"chart_data/test\")\n",
    "\n",
    "model = model.to(device)\n",
    "#train = train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15fd793c-b903-4e03-992a-53b38e2982a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebei\\AppData\\Local\\Temp\\ipykernel_32576\\1020273444.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"temp_model_parameters40.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 329541888\n",
      "Trainable parameters: 329239296\n"
     ]
    }
   ],
   "source": [
    "#checkpoint = torch.load('mae_visualize_vit_large_ganloss.pth')\n",
    "#model.load_state_dict(checkpoint['model'], strict=False)\n",
    "model.load_state_dict(torch.load(\"temp_model_parameters40.pth\"))\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51b598b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "#scaler = GradScaler()\n",
    "\n",
    "def set_discrim_lr(model, lr, lr_decay):\n",
    "    \"\"\"\n",
    "    Set discriminative learning rate across model layers,\n",
    "    using lower learning rates on deeper layers of the network by a factor of `lr_decay`\n",
    "    \"\"\"  \n",
    "    parameters = []\n",
    "    \n",
    "    cur_params = []\n",
    "    cur_layer = None\n",
    "\n",
    "    for name, param in reversed(list(model.named_parameters())):\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        \n",
    "        if name.count('.') == 0:\n",
    "            layer = name\n",
    "        else:\n",
    "            layer = name.split('.')[:-1]\n",
    "        \n",
    "        if cur_params and layer != cur_layer:\n",
    "            parameters.append({'params': cur_params, 'lr': lr})\n",
    "            cur_params = []\n",
    "            \n",
    "            lr *= lr_decay\n",
    "            \n",
    "        cur_params.append(param)\n",
    "        cur_layer = layer\n",
    "        \n",
    "    parameters.append({'params': cur_params, 'lr': lr})\n",
    "    return parameters\n",
    "      \n",
    "def fine_tune(model, train, num_epochs=40, batch_size=96):\n",
    "    lr = 7e-4\n",
    "    lr_decay = 0.97\n",
    "     \n",
    "    param_groups = set_discrim_lr(model, lr, lr_decay)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        param_groups,\n",
    "        betas=(0.9, 0.999),\n",
    "        lr=lr,\n",
    "        weight_decay=0.05,\n",
    "    ) \n",
    "    \n",
    "    #warmup_epochs = 1 #5\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    #warmup_scheduler = warmup.UntunedLinearWarmup(optimizer, last_step=warmup_epochs)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True) \n",
    "    for epoch in range(1, num_epochs):\n",
    "        print([params['lr'] for params in optimizer.param_groups])\n",
    "        #model.train(True)\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            with autocast():  # Mixed precision\n",
    "                loss, _, _ = model(batch, mask_ratio=1/(model.patch_embed.patch_size[0] - 2))\n",
    "            \n",
    "            # Backward pass\n",
    "            #scaler.scale(loss).backward()\n",
    "            #scaler.step(optimizer)\n",
    "            #scaler.update()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #with warmup_scheduler.dampening():\n",
    "            #    scheduler.step() \n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            mem = torch.cuda.memory_allocated(device)\n",
    "                \n",
    "            print(f'Epoch {epoch+1}, Batch {batch_idx}, Train Loss: {loss.item()}, Memory (GB): {mem >> 30}')\n",
    "            #print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "\n",
    "        '''\n",
    "        if epoch % 3 == 0:\n",
    "            # switch to evaluation mode\n",
    "            model.eval()\n",
    "            for batch_idx, batch in enumerate(test_loader):\n",
    "                batch = batch.to(device)\n",
    "    \n",
    "                # Forward pass\n",
    "                with autocast():  # Mixed precision\n",
    "                    loss, _, _ = model(batch, mask_ratio=1/(model.patch_embed.patch_size[0] - 2))\n",
    "    \n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                mem = torch.cuda.memory_allocated(device)\n",
    "                    \n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Test Loss: {loss.item()}, Memory (GB): {mem >> 30}')\n",
    "                print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "579c3a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5007503751875938e-06, 1.4257128564282141e-06, 1.3544272136068034e-06, 1.2867058529264632e-06, 1.22237056028014e-06, 1.1612520322661327e-06, 1.103189430652826e-06, 1.0480299591201848e-06, 9.956284611641754e-07, 9.458470381059664e-07, 8.98554686200668e-07, 8.536269518906346e-07, 8.109456042961028e-07, 7.703983240812976e-07, 7.318784078772328e-07, 6.952844874833711e-07, 6.605202631092025e-07, 6.274942499537424e-07, 5.961195374560553e-07, 5.663135605832524e-07, 5.379978825540898e-07, 5.110979884263852e-07, 4.85543089005066e-07, 4.612659345548126e-07, 4.382026378270719e-07, 4.162925059357183e-07, 3.9547788063893234e-07, 3.757039866069857e-07, 3.569187872766364e-07, 3.3907284791280456e-07, 3.2211920551716433e-07, 3.060132452413061e-07, 2.907125829792408e-07, 2.7617695383027873e-07, 2.6236810613876475e-07, 2.492497008318265e-07, 2.3678721579023518e-07, 2.249478550007234e-07, 2.137004622506872e-07, 2.0301543913815282e-07, 1.9286466718124518e-07, 1.832214338221829e-07, 1.7406036213107374e-07, 1.6535734402452004e-07, 1.5708947682329405e-07, 1.4923500298212934e-07, 1.4177325283302286e-07, 1.346845901913717e-07, 1.2795036068180312e-07, 1.2155284264771295e-07, 1.154752005153273e-07, 1.0970144048956095e-07, 1.0421636846508289e-07, 9.900555004182874e-08, 9.405527253973731e-08, 8.935250891275043e-08, 8.488488346711291e-08, 8.064063929375725e-08, 7.660860732906939e-08, 7.277817696261591e-08, 6.91392681144851e-08, 6.568230470876085e-08, 6.23981894733228e-08, 5.9278279999656654e-08, 5.631436599967382e-08, 5.349864769969013e-08, 5.082371531470561e-08, 4.828252954897033e-08, 4.586840307152181e-08, 4.357498291794572e-08, 4.139623377204843e-08, 3.9326422083446006e-08, 3.736010097927371e-08, 3.549209593031002e-08, 3.3717491133794516e-08, 3.2031616577104784e-08, 3.0430035748249546e-08, 2.8908533960837066e-08, 2.7463107262795215e-08, 2.6089951899655453e-08, 2.4785454304672685e-08, 2.3546181589439047e-08, 2.2368872509967094e-08, 2.125042888446874e-08, 2.01879074402453e-08, 1.9178512068233036e-08, 1.8219586464821384e-08, 1.7308607141580313e-08, 1.64431767845013e-08, 1.5621017945276233e-08, 1.4839967048012418e-08, 1.4097968695611797e-08, 1.3393070260831207e-08, 1.2723416747789647e-08, 1.2087245910400165e-08, 1.1482883614880156e-08, 1.0908739434136149e-08, 1.0363302462429341e-08, 9.845137339307873e-09, 9.35288047234248e-09, 8.885236448725354e-09, 8.440974626289086e-09, 8.018925894974633e-09, 7.6179796002259e-09, 7.237080620214606e-09, 6.875226589203875e-09, 6.531465259743681e-09, 6.204891996756496e-09, 5.894647396918671e-09, 5.599915027072737e-09, 5.3199192757191e-09, 5.053923311933145e-09, 4.801227146336487e-09, 4.561165789019663e-09, 4.333107499568679e-09, 4.116452124590245e-09, 3.910629518360733e-09, 3.7150980424426963e-09, 3.529343140320561e-09, 3.3528759833045328e-09, 3.185232184139306e-09, 3.02597057493234e-09, 2.8746720461857236e-09, 2.730938443876437e-09, 2.5943915216826147e-09, 2.464671945598484e-09, 2.34143834831856e-09, 2.2243664309026317e-09, 2.1131481093575e-09, 2.0074907038896252e-09, 1.9071161686951437e-09, 1.8117603602603864e-09, 1.7211723422473673e-09, 1.6351137251349989e-09, 1.5533580388782488e-09, 1.4756901369343362e-09, 1.4019056300876193e-09, 1.3318103485832382e-09, 1.2652198311540764e-09, 1.2019588395963725e-09, 1.1418608976165538e-09, 1.084767852735726e-09, 1.0305294600989397e-09, 9.790029870939926e-10, 9.30052837739293e-10, 8.835501958523283e-10, 8.393726860597119e-10, 7.974040517567262e-10, 7.575338491688897e-10, 7.196571567104452e-10, 6.836742988749229e-10, 6.494905839311768e-10, 6.170160547346179e-10, 5.86165251997887e-10, 5.568569893979926e-10, 5.290141399280929e-10, 5.025634329316883e-10, 4.774352612851038e-10, 4.535634982208486e-10, 4.3088532330980614e-10, 4.093410571443158e-10, 3.8887400428709995e-10, 3.6943030407274496e-10, 3.5095878886910765e-10, 3.3341084942565226e-10, 3.1674030695436965e-10, 3.009032916066511e-10, 2.8585812702631855e-10, 2.715652206750026e-10, 2.5798695964125245e-10, 2.450876116591898e-10, 2.3283323107623035e-10, 2.2119156952241885e-10, 2.101319910462979e-10, 1.9962539149398298e-10, 1.8964412191928384e-10, 1.8016191582331965e-10, 1.7115382003215365e-10, 1.6259612903054596e-10, 1.5446632257901867e-10, 1.4674300645006772e-10, 1.3940585612756433e-10, 1.324355633211861e-10, 1.258137851551268e-10, 1.1952309589737046e-10, 1.135469411025019e-10, 1.078695940473768e-10, 1.0247611434500796e-10, 9.735230862775756e-11, 9.248469319636967e-11, 8.786045853655117e-11, 8.346743560972361e-11, 7.929406382923743e-11, 7.532936063777555e-11, 7.156289260588677e-11, 6.798474797559243e-11, 6.458551057681282e-11, 6.135623504797217e-11, 5.828842329557355e-11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebei\\AppData\\Local\\Temp\\ipykernel_7876\\2620436565.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Train Loss: 0.39434942603111267, Memory (GB): 4\n",
      "Epoch 1, Batch 1, Train Loss: 0.3899467885494232, Memory (GB): 4\n",
      "Epoch 1, Batch 2, Train Loss: 0.43875449895858765, Memory (GB): 4\n",
      "Epoch 1, Batch 3, Train Loss: 0.37732642889022827, Memory (GB): 4\n",
      "Epoch 1, Batch 4, Train Loss: 0.4096970558166504, Memory (GB): 4\n",
      "Epoch 1, Batch 5, Train Loss: 0.40361282229423523, Memory (GB): 4\n",
      "Epoch 1, Batch 6, Train Loss: 0.39373573660850525, Memory (GB): 4\n",
      "Epoch 1, Batch 7, Train Loss: 0.4254646897315979, Memory (GB): 4\n",
      "Epoch 1, Batch 8, Train Loss: 0.4329836666584015, Memory (GB): 4\n",
      "Epoch 1, Batch 9, Train Loss: 0.4537976384162903, Memory (GB): 4\n",
      "Epoch 1, Batch 10, Train Loss: 0.443233460187912, Memory (GB): 4\n",
      "Epoch 1, Batch 11, Train Loss: 0.4639301002025604, Memory (GB): 4\n",
      "Epoch 1, Batch 12, Train Loss: 0.38897642493247986, Memory (GB): 4\n",
      "Epoch 1, Batch 13, Train Loss: 0.3787388801574707, Memory (GB): 4\n",
      "Epoch 1, Batch 14, Train Loss: 0.4357461929321289, Memory (GB): 4\n",
      "Epoch 1, Batch 15, Train Loss: 0.4060526490211487, Memory (GB): 4\n",
      "Epoch 1, Batch 16, Train Loss: 0.3787408471107483, Memory (GB): 4\n",
      "Epoch 1, Batch 17, Train Loss: 0.4105556309223175, Memory (GB): 4\n",
      "Epoch 1, Batch 18, Train Loss: 0.429370641708374, Memory (GB): 4\n",
      "Epoch 1, Batch 19, Train Loss: 0.36518386006355286, Memory (GB): 4\n",
      "Epoch 1, Batch 20, Train Loss: 0.37115034461021423, Memory (GB): 4\n",
      "Epoch 1, Batch 21, Train Loss: 0.41411566734313965, Memory (GB): 4\n",
      "Epoch 1, Batch 22, Train Loss: 0.43096041679382324, Memory (GB): 4\n",
      "Epoch 1, Batch 23, Train Loss: 0.41526883840560913, Memory (GB): 4\n",
      "Epoch 1, Batch 24, Train Loss: 0.3988994061946869, Memory (GB): 4\n",
      "Epoch 1, Batch 25, Train Loss: 0.37284865975379944, Memory (GB): 4\n",
      "Epoch 1, Batch 26, Train Loss: 0.3776209056377411, Memory (GB): 4\n",
      "Epoch 1, Batch 27, Train Loss: 0.379930704832077, Memory (GB): 4\n",
      "Epoch 1, Batch 28, Train Loss: 0.37198442220687866, Memory (GB): 4\n",
      "Epoch 1, Batch 29, Train Loss: 0.3551768660545349, Memory (GB): 4\n",
      "Epoch 1, Batch 30, Train Loss: 0.34915056824684143, Memory (GB): 4\n",
      "Epoch 1, Batch 31, Train Loss: 0.3338932394981384, Memory (GB): 4\n",
      "Epoch 1, Batch 32, Train Loss: 0.38072314858436584, Memory (GB): 4\n",
      "Epoch 1, Batch 33, Train Loss: 0.33989253640174866, Memory (GB): 4\n",
      "Epoch 1, Batch 34, Train Loss: 0.39556100964546204, Memory (GB): 4\n",
      "Epoch 1, Batch 35, Train Loss: 0.37434524297714233, Memory (GB): 4\n",
      "Epoch 1, Batch 36, Train Loss: 0.41362181305885315, Memory (GB): 4\n",
      "Epoch 1, Batch 37, Train Loss: 0.4186384677886963, Memory (GB): 4\n",
      "Epoch 1, Batch 38, Train Loss: 0.3449116051197052, Memory (GB): 4\n",
      "Epoch 1, Batch 39, Train Loss: 0.3725074231624603, Memory (GB): 4\n",
      "Epoch 1, Batch 40, Train Loss: 0.35608527064323425, Memory (GB): 4\n",
      "Epoch 1, Batch 41, Train Loss: 0.3374309241771698, Memory (GB): 4\n",
      "Epoch 1, Batch 42, Train Loss: 0.3469403088092804, Memory (GB): 4\n",
      "Epoch 1, Batch 43, Train Loss: 0.39029237627983093, Memory (GB): 4\n",
      "Epoch 1, Batch 44, Train Loss: 0.37870314717292786, Memory (GB): 4\n",
      "Epoch 1, Batch 45, Train Loss: 0.3249492645263672, Memory (GB): 4\n",
      "Epoch 1, Batch 46, Train Loss: 0.3987804055213928, Memory (GB): 4\n",
      "Epoch 1, Batch 47, Train Loss: 0.3274269700050354, Memory (GB): 4\n",
      "Epoch 1, Batch 48, Train Loss: 0.35239118337631226, Memory (GB): 4\n",
      "Epoch 1, Batch 49, Train Loss: 0.3678443729877472, Memory (GB): 4\n",
      "Epoch 1, Batch 50, Train Loss: 0.33406320214271545, Memory (GB): 4\n",
      "Epoch 1, Batch 51, Train Loss: 0.34467002749443054, Memory (GB): 4\n",
      "Epoch 1, Batch 52, Train Loss: 0.37135955691337585, Memory (GB): 4\n",
      "Epoch 1, Batch 53, Train Loss: 0.3332159221172333, Memory (GB): 4\n",
      "Epoch 1, Batch 54, Train Loss: 0.3624208867549896, Memory (GB): 4\n",
      "Epoch 1, Batch 55, Train Loss: 0.36106300354003906, Memory (GB): 4\n",
      "Epoch 1, Batch 56, Train Loss: 0.33465856313705444, Memory (GB): 4\n",
      "Epoch 1, Batch 57, Train Loss: 0.3635706305503845, Memory (GB): 4\n",
      "Epoch 1, Batch 58, Train Loss: 0.3739866316318512, Memory (GB): 4\n",
      "Epoch 1, Batch 59, Train Loss: 0.27481064200401306, Memory (GB): 4\n",
      "Epoch 1, Batch 60, Train Loss: 0.3807072341442108, Memory (GB): 4\n",
      "Epoch 1, Batch 61, Train Loss: 0.3414219319820404, Memory (GB): 4\n",
      "Epoch 1, Batch 62, Train Loss: 0.36105188727378845, Memory (GB): 4\n",
      "Epoch 1, Batch 63, Train Loss: 0.3284589946269989, Memory (GB): 4\n",
      "Epoch 1, Batch 64, Train Loss: 0.3736291527748108, Memory (GB): 4\n",
      "Epoch 1, Batch 65, Train Loss: 0.4046688377857208, Memory (GB): 4\n",
      "Epoch 1, Batch 66, Train Loss: 0.36308860778808594, Memory (GB): 4\n",
      "Epoch 1, Batch 67, Train Loss: 0.31324419379234314, Memory (GB): 4\n",
      "Epoch 1, Batch 68, Train Loss: 0.31873247027397156, Memory (GB): 4\n",
      "Epoch 1, Batch 69, Train Loss: 0.32471174001693726, Memory (GB): 4\n",
      "Epoch 1, Batch 70, Train Loss: 0.32706063985824585, Memory (GB): 4\n",
      "Epoch 1, Batch 71, Train Loss: 0.3879433274269104, Memory (GB): 4\n",
      "Epoch 1, Batch 72, Train Loss: 0.3408629596233368, Memory (GB): 4\n",
      "Epoch 1, Batch 73, Train Loss: 0.34130197763442993, Memory (GB): 4\n",
      "Epoch 1, Batch 74, Train Loss: 0.32570597529411316, Memory (GB): 4\n",
      "Epoch 1, Batch 75, Train Loss: 0.487633615732193, Memory (GB): 4\n",
      "Epoch 1, Batch 76, Train Loss: 0.3989681601524353, Memory (GB): 4\n",
      "Epoch 1, Batch 77, Train Loss: 0.31737634539604187, Memory (GB): 4\n",
      "Epoch 1, Batch 78, Train Loss: 0.33179330825805664, Memory (GB): 4\n",
      "Epoch 1, Batch 79, Train Loss: 0.38873791694641113, Memory (GB): 4\n",
      "Epoch 1, Batch 80, Train Loss: 0.34171873331069946, Memory (GB): 4\n",
      "Epoch 1, Batch 81, Train Loss: 0.3435399830341339, Memory (GB): 4\n",
      "Epoch 1, Batch 82, Train Loss: 0.32843896746635437, Memory (GB): 4\n",
      "Epoch 1, Batch 83, Train Loss: 0.30603358149528503, Memory (GB): 4\n",
      "Epoch 1, Batch 84, Train Loss: 0.3758247196674347, Memory (GB): 4\n",
      "Epoch 1, Batch 85, Train Loss: 0.39301198720932007, Memory (GB): 4\n",
      "Epoch 1, Batch 86, Train Loss: 0.3351169526576996, Memory (GB): 4\n",
      "Epoch 1, Batch 87, Train Loss: 0.320441871881485, Memory (GB): 4\n",
      "Epoch 1, Batch 88, Train Loss: 0.383173406124115, Memory (GB): 4\n",
      "Epoch 1, Batch 89, Train Loss: 0.3771931827068329, Memory (GB): 4\n",
      "Epoch 1, Batch 90, Train Loss: 0.37933874130249023, Memory (GB): 4\n",
      "Epoch 1, Batch 91, Train Loss: 0.33528757095336914, Memory (GB): 4\n",
      "Epoch 1, Batch 92, Train Loss: 0.29427221417427063, Memory (GB): 4\n",
      "Epoch 1, Batch 93, Train Loss: 0.3601932227611542, Memory (GB): 4\n",
      "Epoch 1, Batch 94, Train Loss: 0.36016204953193665, Memory (GB): 4\n",
      "Epoch 1, Batch 95, Train Loss: 0.3970901370048523, Memory (GB): 4\n",
      "Epoch 1, Batch 96, Train Loss: 0.31767144799232483, Memory (GB): 4\n",
      "Epoch 1, Batch 97, Train Loss: 0.33847516775131226, Memory (GB): 4\n",
      "Epoch 1, Batch 98, Train Loss: 0.4059801697731018, Memory (GB): 4\n",
      "Epoch 1, Batch 99, Train Loss: 0.2921389043331146, Memory (GB): 4\n",
      "Epoch 1, Batch 100, Train Loss: 0.38952308893203735, Memory (GB): 4\n",
      "Epoch 1, Batch 101, Train Loss: 0.33343178033828735, Memory (GB): 4\n",
      "Epoch 1, Batch 102, Train Loss: 0.42133182287216187, Memory (GB): 4\n",
      "Epoch 1, Batch 103, Train Loss: 0.3508930206298828, Memory (GB): 4\n",
      "Epoch 1, Batch 104, Train Loss: 0.36158570647239685, Memory (GB): 4\n",
      "Epoch 1, Batch 105, Train Loss: 0.30765146017074585, Memory (GB): 4\n",
      "Epoch 1, Batch 106, Train Loss: 0.35007375478744507, Memory (GB): 4\n",
      "Epoch 1, Batch 107, Train Loss: 0.3250844180583954, Memory (GB): 4\n",
      "Epoch 1, Batch 108, Train Loss: 0.35196733474731445, Memory (GB): 4\n",
      "Epoch 1, Batch 109, Train Loss: 0.3680116534233093, Memory (GB): 4\n",
      "Epoch 1, Batch 110, Train Loss: 0.30380359292030334, Memory (GB): 4\n",
      "Epoch 1, Batch 111, Train Loss: 0.34257784485816956, Memory (GB): 4\n",
      "Epoch 1, Batch 112, Train Loss: 0.36405134201049805, Memory (GB): 4\n",
      "Epoch 1, Batch 113, Train Loss: 0.3866380751132965, Memory (GB): 4\n",
      "Epoch 1, Batch 114, Train Loss: 0.3866356313228607, Memory (GB): 4\n",
      "Epoch 1, Batch 115, Train Loss: 0.3635155260562897, Memory (GB): 4\n",
      "Epoch 1, Batch 116, Train Loss: 0.34229132533073425, Memory (GB): 4\n",
      "Epoch 1, Batch 117, Train Loss: 0.3635821044445038, Memory (GB): 4\n",
      "Epoch 1, Batch 118, Train Loss: 0.32227781414985657, Memory (GB): 4\n",
      "Epoch 1, Batch 119, Train Loss: 0.3317907750606537, Memory (GB): 4\n",
      "Epoch 1, Batch 120, Train Loss: 0.29588809609413147, Memory (GB): 4\n",
      "Epoch 1, Batch 121, Train Loss: 0.33311447501182556, Memory (GB): 4\n",
      "Epoch 1, Batch 122, Train Loss: 0.3518216907978058, Memory (GB): 4\n",
      "Epoch 1, Batch 123, Train Loss: 0.3802911937236786, Memory (GB): 4\n",
      "Epoch 1, Batch 124, Train Loss: 0.32132965326309204, Memory (GB): 4\n",
      "Epoch 1, Batch 125, Train Loss: 0.37278667092323303, Memory (GB): 4\n",
      "Epoch 1, Batch 126, Train Loss: 0.3193293511867523, Memory (GB): 4\n",
      "Epoch 1, Batch 127, Train Loss: 0.34695568680763245, Memory (GB): 4\n",
      "Epoch 1, Batch 128, Train Loss: 0.3352203071117401, Memory (GB): 4\n",
      "Epoch 1, Batch 129, Train Loss: 0.3207986056804657, Memory (GB): 4\n",
      "Epoch 1, Batch 130, Train Loss: 0.3616328239440918, Memory (GB): 4\n",
      "Epoch 1, Batch 131, Train Loss: 0.3349129855632782, Memory (GB): 4\n",
      "Epoch 1, Batch 132, Train Loss: 0.36373934149742126, Memory (GB): 4\n",
      "Epoch 1, Batch 133, Train Loss: 0.35380154848098755, Memory (GB): 4\n",
      "Epoch 1, Batch 134, Train Loss: 0.3557964861392975, Memory (GB): 4\n",
      "Epoch 1, Batch 135, Train Loss: 0.3608519732952118, Memory (GB): 4\n",
      "Epoch 1, Batch 136, Train Loss: 0.3332068622112274, Memory (GB): 4\n",
      "Epoch 1, Batch 137, Train Loss: 0.3721194267272949, Memory (GB): 4\n",
      "Epoch 1, Batch 138, Train Loss: 0.38553598523139954, Memory (GB): 4\n",
      "Epoch 1, Batch 139, Train Loss: 0.31737980246543884, Memory (GB): 4\n",
      "Epoch 1, Batch 140, Train Loss: 0.2999563217163086, Memory (GB): 4\n",
      "Epoch 1, Batch 141, Train Loss: 0.341135174036026, Memory (GB): 4\n",
      "Epoch 1, Batch 142, Train Loss: 0.29954877495765686, Memory (GB): 4\n",
      "Epoch 1, Batch 143, Train Loss: 0.3904416859149933, Memory (GB): 4\n",
      "Epoch 1, Batch 144, Train Loss: 0.31001409888267517, Memory (GB): 4\n",
      "Epoch 1, Batch 145, Train Loss: 0.2899599075317383, Memory (GB): 4\n",
      "Epoch 1, Batch 146, Train Loss: 0.3638595640659332, Memory (GB): 4\n",
      "Epoch 1, Batch 147, Train Loss: 0.3059253394603729, Memory (GB): 4\n",
      "Epoch 1, Batch 148, Train Loss: 0.33649942278862, Memory (GB): 4\n",
      "Epoch 1, Batch 149, Train Loss: 0.3117073178291321, Memory (GB): 4\n",
      "Epoch 1, Batch 150, Train Loss: 0.30412033200263977, Memory (GB): 4\n",
      "Epoch 1, Batch 151, Train Loss: 0.36930033564567566, Memory (GB): 4\n",
      "Epoch 1, Batch 152, Train Loss: 0.32458043098449707, Memory (GB): 4\n",
      "Epoch 1, Batch 153, Train Loss: 0.37502309679985046, Memory (GB): 4\n",
      "Epoch 1, Batch 154, Train Loss: 0.31036171317100525, Memory (GB): 4\n",
      "Epoch 1, Batch 155, Train Loss: 0.2845604717731476, Memory (GB): 4\n",
      "Epoch 1, Batch 156, Train Loss: 0.33061376214027405, Memory (GB): 4\n",
      "Epoch 1, Batch 157, Train Loss: 0.28439536690711975, Memory (GB): 4\n",
      "Epoch 1, Batch 158, Train Loss: 0.334724485874176, Memory (GB): 4\n",
      "Epoch 1, Batch 159, Train Loss: 0.31316375732421875, Memory (GB): 4\n",
      "Epoch 1, Batch 160, Train Loss: 0.3021215796470642, Memory (GB): 4\n",
      "Epoch 1, Batch 161, Train Loss: 0.3335537314414978, Memory (GB): 4\n",
      "Epoch 1, Batch 162, Train Loss: 0.34682661294937134, Memory (GB): 4\n",
      "Epoch 1, Batch 163, Train Loss: 0.31818440556526184, Memory (GB): 4\n",
      "Epoch 1, Batch 164, Train Loss: 0.32425007224082947, Memory (GB): 4\n",
      "Epoch 1, Batch 165, Train Loss: 0.33913999795913696, Memory (GB): 4\n",
      "Epoch 1, Batch 166, Train Loss: 0.3364982008934021, Memory (GB): 4\n",
      "Epoch 1, Batch 167, Train Loss: 0.3327026963233948, Memory (GB): 4\n",
      "Epoch 1, Batch 168, Train Loss: 0.33819910883903503, Memory (GB): 4\n",
      "Epoch 1, Batch 169, Train Loss: 0.26446181535720825, Memory (GB): 4\n",
      "Epoch 1, Batch 170, Train Loss: 0.33171412348747253, Memory (GB): 4\n",
      "Epoch 1, Batch 171, Train Loss: 0.30930373072624207, Memory (GB): 4\n",
      "Epoch 1, Batch 172, Train Loss: 0.3588268458843231, Memory (GB): 4\n",
      "Epoch 1, Batch 173, Train Loss: 0.3002118468284607, Memory (GB): 4\n",
      "Epoch 1, Batch 174, Train Loss: 0.3037447929382324, Memory (GB): 4\n",
      "Epoch 1, Batch 175, Train Loss: 0.3301657736301422, Memory (GB): 4\n",
      "Epoch 1, Batch 176, Train Loss: 0.31767600774765015, Memory (GB): 4\n",
      "Epoch 1, Batch 177, Train Loss: 0.30025508999824524, Memory (GB): 4\n",
      "Epoch 1, Batch 178, Train Loss: 0.3009799122810364, Memory (GB): 4\n",
      "Epoch 1, Batch 179, Train Loss: 0.3218677043914795, Memory (GB): 4\n",
      "Epoch 1, Batch 180, Train Loss: 0.3607187569141388, Memory (GB): 4\n",
      "Epoch 1, Batch 181, Train Loss: 0.31159546971321106, Memory (GB): 4\n",
      "Epoch 1, Batch 182, Train Loss: 0.400424063205719, Memory (GB): 4\n",
      "Epoch 1, Batch 183, Train Loss: 0.31522658467292786, Memory (GB): 4\n",
      "Epoch 1, Batch 184, Train Loss: 0.41152334213256836, Memory (GB): 4\n",
      "Epoch 1, Batch 185, Train Loss: 0.3092234432697296, Memory (GB): 4\n",
      "Epoch 1, Batch 186, Train Loss: 0.31560882925987244, Memory (GB): 4\n",
      "Epoch 1, Batch 187, Train Loss: 0.2879772186279297, Memory (GB): 4\n",
      "Epoch 1, Batch 188, Train Loss: 0.28119340538978577, Memory (GB): 4\n",
      "Epoch 1, Batch 189, Train Loss: 0.281444251537323, Memory (GB): 4\n",
      "Epoch 1, Batch 190, Train Loss: 0.29592177271842957, Memory (GB): 4\n",
      "Epoch 1, Batch 191, Train Loss: 0.3173220455646515, Memory (GB): 4\n",
      "Epoch 1, Batch 192, Train Loss: 0.28482767939567566, Memory (GB): 4\n",
      "Epoch 1, Batch 193, Train Loss: 0.3118381202220917, Memory (GB): 4\n",
      "Epoch 1, Batch 194, Train Loss: 0.3276461064815521, Memory (GB): 4\n",
      "Epoch 1, Batch 195, Train Loss: 0.29406866431236267, Memory (GB): 4\n",
      "Epoch 1, Batch 196, Train Loss: 0.33575859665870667, Memory (GB): 4\n",
      "Epoch 1, Batch 197, Train Loss: 0.3483775556087494, Memory (GB): 4\n",
      "Epoch 1, Batch 198, Train Loss: 0.3012058436870575, Memory (GB): 4\n",
      "Epoch 1, Batch 199, Train Loss: 0.3548431694507599, Memory (GB): 4\n",
      "Epoch 1, Batch 200, Train Loss: 0.3409275412559509, Memory (GB): 4\n",
      "Epoch 1, Batch 201, Train Loss: 0.3308960497379303, Memory (GB): 4\n",
      "Epoch 1, Batch 202, Train Loss: 0.275621235370636, Memory (GB): 4\n",
      "Epoch 1, Batch 203, Train Loss: 0.33502671122550964, Memory (GB): 4\n",
      "Epoch 1, Batch 204, Train Loss: 0.2816055417060852, Memory (GB): 4\n",
      "Epoch 1, Batch 205, Train Loss: 0.32213762402534485, Memory (GB): 4\n",
      "Epoch 1, Batch 206, Train Loss: 0.27930396795272827, Memory (GB): 4\n",
      "Epoch 1, Batch 207, Train Loss: 0.3411513864994049, Memory (GB): 4\n",
      "Epoch 1, Batch 208, Train Loss: 0.3235309422016144, Memory (GB): 4\n",
      "Epoch 1, Batch 209, Train Loss: 0.36115771532058716, Memory (GB): 4\n",
      "Epoch 1, Batch 210, Train Loss: 0.2894933521747589, Memory (GB): 4\n",
      "Epoch 1, Batch 211, Train Loss: 0.3133426010608673, Memory (GB): 4\n",
      "Epoch 1, Batch 212, Train Loss: 0.3108689785003662, Memory (GB): 4\n",
      "Epoch 1, Batch 213, Train Loss: 0.30342772603034973, Memory (GB): 4\n",
      "Epoch 1, Batch 214, Train Loss: 0.2800211012363434, Memory (GB): 4\n",
      "Epoch 1, Batch 215, Train Loss: 0.3003138601779938, Memory (GB): 4\n",
      "Epoch 1, Batch 216, Train Loss: 0.28890323638916016, Memory (GB): 4\n",
      "Epoch 1, Batch 217, Train Loss: 0.3065159320831299, Memory (GB): 4\n",
      "Epoch 1, Batch 218, Train Loss: 0.3135609030723572, Memory (GB): 4\n",
      "Epoch 1, Batch 219, Train Loss: 0.3624401092529297, Memory (GB): 4\n",
      "Epoch 1, Batch 220, Train Loss: 0.34723636507987976, Memory (GB): 4\n",
      "Epoch 1, Batch 221, Train Loss: 0.281747430562973, Memory (GB): 4\n",
      "Epoch 1, Batch 222, Train Loss: 0.35210323333740234, Memory (GB): 4\n",
      "Epoch 1, Batch 223, Train Loss: 0.3118135333061218, Memory (GB): 4\n",
      "Epoch 1, Batch 224, Train Loss: 0.28178301453590393, Memory (GB): 4\n",
      "Epoch 1, Batch 225, Train Loss: 0.3470020592212677, Memory (GB): 4\n",
      "Epoch 1, Batch 226, Train Loss: 0.31799620389938354, Memory (GB): 4\n",
      "Epoch 1, Batch 227, Train Loss: 0.32306957244873047, Memory (GB): 4\n",
      "Epoch 1, Batch 228, Train Loss: 0.2910589575767517, Memory (GB): 4\n",
      "Epoch 1, Batch 229, Train Loss: 0.2851061522960663, Memory (GB): 4\n",
      "Epoch 1, Batch 230, Train Loss: 0.3009004592895508, Memory (GB): 4\n",
      "Epoch 1, Batch 231, Train Loss: 0.28065764904022217, Memory (GB): 4\n",
      "Epoch 1, Batch 232, Train Loss: 0.2972165048122406, Memory (GB): 4\n",
      "Epoch 1, Batch 233, Train Loss: 0.3499593734741211, Memory (GB): 4\n",
      "Epoch 1, Batch 234, Train Loss: 0.3752327561378479, Memory (GB): 4\n",
      "Epoch 1, Batch 235, Train Loss: 0.2934349477291107, Memory (GB): 4\n",
      "Epoch 1, Batch 236, Train Loss: 0.290285587310791, Memory (GB): 4\n",
      "Epoch 1, Batch 237, Train Loss: 0.29643040895462036, Memory (GB): 4\n",
      "Epoch 1, Batch 238, Train Loss: 0.29547232389450073, Memory (GB): 4\n",
      "Epoch 1, Batch 239, Train Loss: 0.29692405462265015, Memory (GB): 4\n",
      "Epoch 1, Batch 240, Train Loss: 0.313153475522995, Memory (GB): 4\n",
      "Epoch 1, Batch 241, Train Loss: 0.2682752013206482, Memory (GB): 4\n",
      "Epoch 1, Batch 242, Train Loss: 0.3130611777305603, Memory (GB): 4\n",
      "Epoch 1, Batch 243, Train Loss: 0.31513795256614685, Memory (GB): 4\n",
      "Epoch 1, Batch 244, Train Loss: 0.2915802001953125, Memory (GB): 4\n",
      "Epoch 1, Batch 245, Train Loss: 0.3007981479167938, Memory (GB): 4\n",
      "Epoch 1, Batch 246, Train Loss: 0.2953733503818512, Memory (GB): 4\n",
      "Epoch 1, Batch 247, Train Loss: 0.33030253648757935, Memory (GB): 4\n",
      "Epoch 1, Batch 248, Train Loss: 0.2795428931713104, Memory (GB): 4\n",
      "Epoch 1, Batch 249, Train Loss: 0.28068336844444275, Memory (GB): 4\n",
      "Epoch 1, Batch 250, Train Loss: 0.32484859228134155, Memory (GB): 4\n",
      "Epoch 1, Batch 251, Train Loss: 0.28990426659584045, Memory (GB): 4\n",
      "Epoch 1, Batch 252, Train Loss: 0.2957499027252197, Memory (GB): 4\n",
      "Epoch 1, Batch 253, Train Loss: 0.31346482038497925, Memory (GB): 4\n",
      "Epoch 1, Batch 254, Train Loss: 0.3099845051765442, Memory (GB): 4\n",
      "Epoch 1, Batch 255, Train Loss: 0.346116304397583, Memory (GB): 4\n",
      "Epoch 1, Batch 256, Train Loss: 0.2653706669807434, Memory (GB): 4\n",
      "Epoch 1, Batch 257, Train Loss: 0.3194836974143982, Memory (GB): 4\n",
      "Epoch 1, Batch 258, Train Loss: 0.32440826296806335, Memory (GB): 4\n",
      "Epoch 1, Batch 259, Train Loss: 0.3017466962337494, Memory (GB): 4\n",
      "Epoch 1, Batch 260, Train Loss: 0.28301528096199036, Memory (GB): 4\n",
      "Epoch 1, Batch 261, Train Loss: 0.30917003750801086, Memory (GB): 4\n",
      "Epoch 1, Batch 262, Train Loss: 0.3363439738750458, Memory (GB): 4\n",
      "Epoch 1, Batch 263, Train Loss: 0.31780344247817993, Memory (GB): 4\n",
      "Epoch 1, Batch 264, Train Loss: 0.3208010494709015, Memory (GB): 4\n",
      "Epoch 1, Batch 265, Train Loss: 0.30879926681518555, Memory (GB): 4\n",
      "Epoch 1, Batch 266, Train Loss: 0.31426361203193665, Memory (GB): 4\n",
      "Epoch 1, Batch 267, Train Loss: 0.3237206041812897, Memory (GB): 4\n",
      "Epoch 1, Batch 268, Train Loss: 0.2528332769870758, Memory (GB): 4\n",
      "Epoch 1, Batch 269, Train Loss: 0.29641255736351013, Memory (GB): 4\n",
      "[2.8147730396236866e-05, 2.674034387642498e-05, 2.540332668260374e-05, 2.4133160348473558e-05, 2.292650233104989e-05, 2.1780177214497385e-05, 2.0691168353772535e-05, 1.96566099360839e-05, 1.8673779439279702e-05, 1.7740090467315706e-05, 1.685308594394992e-05, 1.601043164675242e-05, 1.5209910064414811e-05, 1.4449414561194069e-05, 1.3726943833134363e-05, 1.3040596641477657e-05, 1.2388566809403763e-05, 1.1769138468933568e-05, 1.1180681545486895e-05, 1.0621647468212556e-05, 1.009056509480192e-05, 9.58603684006182e-06, 9.106734998058725e-06, 8.651398248155787e-06, 8.218828335747995e-06, 7.807886918960595e-06, 7.417492573012575e-06, 7.046617944361944e-06, 6.6942870471438445e-06, 6.359572694786646e-06, 6.041594060047312e-06, 5.739514357044952e-06, 5.452538639192708e-06, 5.179911707233066e-06, 4.920916121871411e-06, 4.674870315777843e-06, 4.441126799988953e-06, 4.219070459989504e-06, 4.0081169369900335e-06, 3.8077110901405268e-06, 3.6173255356335e-06, 3.436459258851824e-06, 3.2646362959092303e-06, 3.101404481113771e-06, 2.9463342570580795e-06, 2.7990175442051767e-06, 2.659066666994919e-06, 2.5261133336451734e-06, 2.3998076669629155e-06, 2.279817283614769e-06, 2.1658264194340297e-06, 2.057535098462328e-06, 1.9546583435392113e-06, 1.8569254263622485e-06, 1.7640791550441392e-06, 1.6758751972919314e-06, 1.5920814374273337e-06, 1.5124773655559692e-06, 1.4368534972781692e-06, 1.365010822414262e-06, 1.2967602812935482e-06, 1.231922267228873e-06, 1.1703261538674275e-06, 1.111809846174055e-06, 1.0562193538653523e-06, 1.0034083861720857e-06, 9.532379668634803e-07, 9.055760685203073e-07, 8.60297265094291e-07, 8.172824018395763e-07, 7.764182817475978e-07, 7.375973676602185e-07, 7.007174992772064e-07, 6.656816243133466e-07, 6.323975430976789e-07, 6.007776659427955e-07, 5.707387826456543e-07, 5.422018435133733e-07, 5.150917513377044e-07, 4.893371637708183e-07, 4.648703055822783e-07, 4.4162679030316333e-07, 4.1954545078800553e-07, 3.985681782486055e-07, 3.786397693361754e-07, 3.597077808693662e-07, 3.417223918258978e-07, 3.246362722346034e-07, 3.0840445862287304e-07, 2.929842356917295e-07, 2.783350239071431e-07, 2.644182727117856e-07, 2.5119735907619646e-07, 2.3863749112238623e-07, 2.2670561656626741e-07, 2.1537033573795374e-07, 2.0460181895105624e-07, 1.9437172800350326e-07, 1.8465314160332814e-07, 1.7542048452316188e-07, 1.6664946029700374e-07, 1.5831698728215348e-07, 1.5040113791804585e-07, 1.4288108102214345e-07, 1.3573702697103643e-07, 1.2895017562248454e-07, 1.2250266684136032e-07, 1.1637753349929232e-07, 1.105586568243276e-07, 1.0503072398311129e-07, 9.977918778395558e-08, 9.479022839475784e-08, 9.005071697501991e-08, 8.554818112626903e-08, 8.127077206995557e-08, 7.720723346645774e-08, 7.334687179313489e-08, 6.967952820347807e-08, 6.619555179330422e-08, 6.288577420363904e-08, 5.974148549345701e-08, 5.675441121878424e-08, 5.3916690657844955e-08, 5.12208561249527e-08, 4.8659813318705084e-08, 4.622682265276982e-08, 4.391548152013133e-08, 4.171970744412477e-08, 3.963372207191856e-08, 3.7652035968322606e-08, 3.5769434169906445e-08, 3.398096246141114e-08, 3.228191433834057e-08, 3.0667818621423566e-08, 2.9134427690352348e-08, 2.7677706305834746e-08, 2.6293820990542986e-08, 2.497912994101586e-08, 2.3730173443965077e-08, 2.254366477176681e-08, 2.1416481533178457e-08, 2.0345657456519553e-08, 1.9328374583693577e-08, 1.8361955854508898e-08, 1.744385806178344e-08, 1.6571665158694247e-08, 1.5743081900759546e-08, 1.495592780572158e-08, 1.4208131415435512e-08, 1.3497724844663726e-08, 1.2822838602430526e-08, 1.2181696672309008e-08, 1.1572611838693563e-08, 1.0993981246758872e-08, 1.0444282184420941e-08, 9.922068075199887e-09, 9.425964671439893e-09, 8.954666437867898e-09, 8.506933115974505e-09, 8.081586460175773e-09, 7.677507137166985e-09, 7.293631780308637e-09, 6.92895019129321e-09, 6.582502681728541e-09, 6.253377547642112e-09, 5.940708670260009e-09, 5.643673236747005e-09, 5.361489574909659e-09, 5.093415096164172e-09, 4.838744341355963e-09, 4.596807124288166e-09, 4.366966768073761e-09, 4.1486184296700765e-09, 3.941187508186567e-09, 3.744128132777236e-09, 3.5569217261383774e-09, 3.3790756398314577e-09, 3.2101218578398866e-09, 3.049615764947888e-09, 2.8971349767004946e-09, 2.7522782278654717e-09, 2.6146643164721983e-09, 2.483931100648587e-09, 2.359734545616159e-09, 2.2417478183353493e-09, 2.1296604274185826e-09, 2.0231774060476527e-09, 1.9220185357452704e-09, 1.8259176089580067e-09, 1.7346217285101065e-09, 1.6478906420846001e-09, 1.5654961099803713e-09, 1.487221304481352e-09, 1.412860239257285e-09, 1.3422172272944198e-09, 1.2751063659297005e-09, 1.211351047633215e-09, 1.1507834952515526e-09, 1.0932443204889763e-09]\n",
      "Epoch 2, Batch 0, Train Loss: 0.2604476809501648, Memory (GB): 4\n",
      "Epoch 2, Batch 1, Train Loss: 0.34557873010635376, Memory (GB): 4\n",
      "Epoch 2, Batch 2, Train Loss: 0.29383888840675354, Memory (GB): 4\n",
      "Epoch 2, Batch 3, Train Loss: 0.2800070643424988, Memory (GB): 4\n",
      "Epoch 2, Batch 4, Train Loss: 0.31644853949546814, Memory (GB): 4\n",
      "Epoch 2, Batch 5, Train Loss: 0.34639468789100647, Memory (GB): 4\n",
      "Epoch 2, Batch 6, Train Loss: 0.2881970703601837, Memory (GB): 4\n",
      "Epoch 2, Batch 7, Train Loss: 0.3030025064945221, Memory (GB): 4\n",
      "Epoch 2, Batch 8, Train Loss: 0.2741009593009949, Memory (GB): 4\n",
      "Epoch 2, Batch 9, Train Loss: 0.26411059498786926, Memory (GB): 4\n",
      "Epoch 2, Batch 10, Train Loss: 0.2718117833137512, Memory (GB): 4\n",
      "Epoch 2, Batch 11, Train Loss: 0.32558679580688477, Memory (GB): 4\n",
      "Epoch 2, Batch 12, Train Loss: 0.2783921957015991, Memory (GB): 4\n",
      "Epoch 2, Batch 13, Train Loss: 0.3035929501056671, Memory (GB): 4\n",
      "Epoch 2, Batch 14, Train Loss: 0.3461959660053253, Memory (GB): 4\n",
      "Epoch 2, Batch 15, Train Loss: 0.2798732817173004, Memory (GB): 4\n",
      "Epoch 2, Batch 16, Train Loss: 0.29492154717445374, Memory (GB): 4\n",
      "Epoch 2, Batch 17, Train Loss: 0.33692508935928345, Memory (GB): 4\n",
      "Epoch 2, Batch 18, Train Loss: 0.2948005497455597, Memory (GB): 4\n",
      "Epoch 2, Batch 19, Train Loss: 0.33792808651924133, Memory (GB): 4\n",
      "Epoch 2, Batch 20, Train Loss: 0.29919275641441345, Memory (GB): 4\n",
      "Epoch 2, Batch 21, Train Loss: 0.3574467599391937, Memory (GB): 4\n",
      "Epoch 2, Batch 22, Train Loss: 0.2814410626888275, Memory (GB): 4\n",
      "Epoch 2, Batch 23, Train Loss: 0.2866049110889435, Memory (GB): 4\n",
      "Epoch 2, Batch 24, Train Loss: 0.3006671369075775, Memory (GB): 4\n",
      "Epoch 2, Batch 25, Train Loss: 0.29051661491394043, Memory (GB): 4\n",
      "Epoch 2, Batch 26, Train Loss: 0.3035689890384674, Memory (GB): 4\n",
      "Epoch 2, Batch 27, Train Loss: 0.27670639753341675, Memory (GB): 4\n",
      "Epoch 2, Batch 28, Train Loss: 0.29011422395706177, Memory (GB): 4\n",
      "Epoch 2, Batch 29, Train Loss: 0.2901402413845062, Memory (GB): 4\n",
      "Epoch 2, Batch 30, Train Loss: 0.2742586135864258, Memory (GB): 4\n",
      "Epoch 2, Batch 31, Train Loss: 0.32808351516723633, Memory (GB): 4\n",
      "Epoch 2, Batch 32, Train Loss: 0.2715984284877777, Memory (GB): 4\n",
      "Epoch 2, Batch 33, Train Loss: 0.30984196066856384, Memory (GB): 4\n",
      "Epoch 2, Batch 34, Train Loss: 0.2988237738609314, Memory (GB): 4\n",
      "Epoch 2, Batch 35, Train Loss: 0.29223933815956116, Memory (GB): 4\n",
      "Epoch 2, Batch 36, Train Loss: 0.24783870577812195, Memory (GB): 4\n",
      "Epoch 2, Batch 37, Train Loss: 0.2872655689716339, Memory (GB): 4\n",
      "Epoch 2, Batch 38, Train Loss: 0.2977437376976013, Memory (GB): 4\n",
      "Epoch 2, Batch 39, Train Loss: 0.2812545895576477, Memory (GB): 4\n",
      "Epoch 2, Batch 40, Train Loss: 0.30107131600379944, Memory (GB): 4\n",
      "Epoch 2, Batch 41, Train Loss: 0.3063998520374298, Memory (GB): 4\n",
      "Epoch 2, Batch 42, Train Loss: 0.2515404522418976, Memory (GB): 4\n",
      "Epoch 2, Batch 43, Train Loss: 0.3144891560077667, Memory (GB): 4\n",
      "Epoch 2, Batch 44, Train Loss: 0.3037634491920471, Memory (GB): 4\n",
      "Epoch 2, Batch 45, Train Loss: 0.3224340081214905, Memory (GB): 4\n",
      "Epoch 2, Batch 46, Train Loss: 0.28479161858558655, Memory (GB): 4\n",
      "Epoch 2, Batch 47, Train Loss: 0.286405086517334, Memory (GB): 4\n",
      "Epoch 2, Batch 48, Train Loss: 0.298877090215683, Memory (GB): 4\n",
      "Epoch 2, Batch 49, Train Loss: 0.2680628001689911, Memory (GB): 4\n",
      "Epoch 2, Batch 50, Train Loss: 0.2713322043418884, Memory (GB): 4\n",
      "Epoch 2, Batch 51, Train Loss: 0.2943863868713379, Memory (GB): 4\n",
      "Epoch 2, Batch 52, Train Loss: 0.2809102535247803, Memory (GB): 4\n",
      "Epoch 2, Batch 53, Train Loss: 0.351256400346756, Memory (GB): 4\n",
      "Epoch 2, Batch 54, Train Loss: 0.285389244556427, Memory (GB): 4\n",
      "Epoch 2, Batch 55, Train Loss: 0.32175788283348083, Memory (GB): 4\n",
      "Epoch 2, Batch 56, Train Loss: 0.28753089904785156, Memory (GB): 4\n",
      "Epoch 2, Batch 57, Train Loss: 0.30246493220329285, Memory (GB): 4\n",
      "Epoch 2, Batch 58, Train Loss: 0.29767173528671265, Memory (GB): 4\n",
      "Epoch 2, Batch 59, Train Loss: 0.2986783981323242, Memory (GB): 4\n",
      "Epoch 2, Batch 60, Train Loss: 0.2971058487892151, Memory (GB): 4\n",
      "Epoch 2, Batch 61, Train Loss: 0.27080297470092773, Memory (GB): 4\n",
      "Epoch 2, Batch 62, Train Loss: 0.306871235370636, Memory (GB): 4\n",
      "Epoch 2, Batch 63, Train Loss: 0.31142547726631165, Memory (GB): 4\n",
      "Epoch 2, Batch 64, Train Loss: 0.2647518217563629, Memory (GB): 4\n",
      "Epoch 2, Batch 65, Train Loss: 0.2783520817756653, Memory (GB): 4\n",
      "Epoch 2, Batch 66, Train Loss: 0.31246083974838257, Memory (GB): 4\n",
      "Epoch 2, Batch 67, Train Loss: 0.30388143658638, Memory (GB): 4\n",
      "Epoch 2, Batch 68, Train Loss: 0.31181052327156067, Memory (GB): 4\n",
      "Epoch 2, Batch 69, Train Loss: 0.2700916826725006, Memory (GB): 4\n",
      "Epoch 2, Batch 70, Train Loss: 0.2909867465496063, Memory (GB): 4\n",
      "Epoch 2, Batch 71, Train Loss: 0.28285908699035645, Memory (GB): 4\n",
      "Epoch 2, Batch 72, Train Loss: 0.2725975215435028, Memory (GB): 4\n",
      "Epoch 2, Batch 73, Train Loss: 0.2902749478816986, Memory (GB): 4\n",
      "Epoch 2, Batch 74, Train Loss: 0.29751911759376526, Memory (GB): 4\n",
      "Epoch 2, Batch 75, Train Loss: 0.3159290850162506, Memory (GB): 4\n",
      "Epoch 2, Batch 76, Train Loss: 0.3068232834339142, Memory (GB): 4\n",
      "Epoch 2, Batch 77, Train Loss: 0.2855101227760315, Memory (GB): 4\n",
      "Epoch 2, Batch 78, Train Loss: 0.30106958746910095, Memory (GB): 4\n",
      "Epoch 2, Batch 79, Train Loss: 0.28756067156791687, Memory (GB): 4\n",
      "Epoch 2, Batch 80, Train Loss: 0.2970021665096283, Memory (GB): 4\n",
      "Epoch 2, Batch 81, Train Loss: 0.288379430770874, Memory (GB): 4\n",
      "Epoch 2, Batch 82, Train Loss: 0.3082088828086853, Memory (GB): 4\n",
      "Epoch 2, Batch 83, Train Loss: 0.3210356533527374, Memory (GB): 4\n",
      "Epoch 2, Batch 84, Train Loss: 0.30777809023857117, Memory (GB): 4\n",
      "Epoch 2, Batch 85, Train Loss: 0.32581254839897156, Memory (GB): 4\n",
      "Epoch 2, Batch 86, Train Loss: 0.28708645701408386, Memory (GB): 4\n",
      "Epoch 2, Batch 87, Train Loss: 0.2942185401916504, Memory (GB): 4\n",
      "Epoch 2, Batch 88, Train Loss: 0.34211087226867676, Memory (GB): 4\n",
      "Epoch 2, Batch 89, Train Loss: 0.30458885431289673, Memory (GB): 4\n",
      "Epoch 2, Batch 90, Train Loss: 0.308586061000824, Memory (GB): 4\n",
      "Epoch 2, Batch 91, Train Loss: 0.3014942705631256, Memory (GB): 4\n",
      "Epoch 2, Batch 92, Train Loss: 0.2685200273990631, Memory (GB): 4\n",
      "Epoch 2, Batch 93, Train Loss: 0.27704861760139465, Memory (GB): 4\n",
      "Epoch 2, Batch 94, Train Loss: 0.2893311083316803, Memory (GB): 4\n",
      "Epoch 2, Batch 95, Train Loss: 0.30292177200317383, Memory (GB): 4\n",
      "Epoch 2, Batch 96, Train Loss: 0.28198176622390747, Memory (GB): 4\n",
      "Epoch 2, Batch 97, Train Loss: 0.30786651372909546, Memory (GB): 4\n",
      "Epoch 2, Batch 98, Train Loss: 0.2999238669872284, Memory (GB): 4\n",
      "Epoch 2, Batch 99, Train Loss: 0.3102634847164154, Memory (GB): 4\n",
      "Epoch 2, Batch 100, Train Loss: 0.2621046304702759, Memory (GB): 4\n",
      "Epoch 2, Batch 101, Train Loss: 0.3243075907230377, Memory (GB): 4\n",
      "Epoch 2, Batch 102, Train Loss: 0.3129395842552185, Memory (GB): 4\n",
      "Epoch 2, Batch 103, Train Loss: 0.32078689336776733, Memory (GB): 4\n",
      "Epoch 2, Batch 104, Train Loss: 0.2932319641113281, Memory (GB): 4\n",
      "Epoch 2, Batch 105, Train Loss: 0.2757757306098938, Memory (GB): 4\n",
      "Epoch 2, Batch 106, Train Loss: 0.29099684953689575, Memory (GB): 4\n",
      "Epoch 2, Batch 107, Train Loss: 0.290387362241745, Memory (GB): 4\n",
      "Epoch 2, Batch 108, Train Loss: 0.3021547496318817, Memory (GB): 4\n",
      "Epoch 2, Batch 109, Train Loss: 0.29313531517982483, Memory (GB): 4\n",
      "Epoch 2, Batch 110, Train Loss: 0.28196969628334045, Memory (GB): 4\n",
      "Epoch 2, Batch 111, Train Loss: 0.3745459020137787, Memory (GB): 4\n",
      "Epoch 2, Batch 112, Train Loss: 0.2619091868400574, Memory (GB): 4\n",
      "Epoch 2, Batch 113, Train Loss: 0.2838588356971741, Memory (GB): 4\n",
      "Epoch 2, Batch 114, Train Loss: 0.29065948724746704, Memory (GB): 4\n",
      "Epoch 2, Batch 115, Train Loss: 0.33993005752563477, Memory (GB): 4\n",
      "Epoch 2, Batch 116, Train Loss: 0.33213236927986145, Memory (GB): 4\n",
      "Epoch 2, Batch 117, Train Loss: 0.3018634021282196, Memory (GB): 4\n",
      "Epoch 2, Batch 118, Train Loss: 0.2756749987602234, Memory (GB): 4\n",
      "Epoch 2, Batch 119, Train Loss: 0.3185272812843323, Memory (GB): 4\n",
      "Epoch 2, Batch 120, Train Loss: 0.2714066505432129, Memory (GB): 4\n",
      "Epoch 2, Batch 121, Train Loss: 0.2573170065879822, Memory (GB): 4\n",
      "Epoch 2, Batch 122, Train Loss: 0.30169057846069336, Memory (GB): 4\n",
      "Epoch 2, Batch 123, Train Loss: 0.30471566319465637, Memory (GB): 4\n",
      "Epoch 2, Batch 124, Train Loss: 0.319071501493454, Memory (GB): 4\n",
      "Epoch 2, Batch 125, Train Loss: 0.24866250157356262, Memory (GB): 4\n",
      "Epoch 2, Batch 126, Train Loss: 0.30155083537101746, Memory (GB): 4\n",
      "Epoch 2, Batch 127, Train Loss: 0.2953464686870575, Memory (GB): 4\n",
      "Epoch 2, Batch 128, Train Loss: 0.3123615086078644, Memory (GB): 4\n",
      "Epoch 2, Batch 129, Train Loss: 0.28234925866127014, Memory (GB): 4\n",
      "Epoch 2, Batch 130, Train Loss: 0.32088056206703186, Memory (GB): 4\n",
      "Epoch 2, Batch 131, Train Loss: 0.2715533673763275, Memory (GB): 4\n",
      "Epoch 2, Batch 132, Train Loss: 0.3240136504173279, Memory (GB): 4\n",
      "Epoch 2, Batch 133, Train Loss: 0.2870919704437256, Memory (GB): 4\n",
      "Epoch 2, Batch 134, Train Loss: 0.3145636022090912, Memory (GB): 4\n",
      "Epoch 2, Batch 135, Train Loss: 0.3531258702278137, Memory (GB): 4\n",
      "Epoch 2, Batch 136, Train Loss: 0.264157235622406, Memory (GB): 4\n",
      "Epoch 2, Batch 137, Train Loss: 0.2838975489139557, Memory (GB): 4\n",
      "Epoch 2, Batch 138, Train Loss: 0.29951244592666626, Memory (GB): 4\n",
      "Epoch 2, Batch 139, Train Loss: 0.29777076840400696, Memory (GB): 4\n",
      "Epoch 2, Batch 140, Train Loss: 0.2657662034034729, Memory (GB): 4\n",
      "Epoch 2, Batch 141, Train Loss: 0.3277404010295868, Memory (GB): 4\n",
      "Epoch 2, Batch 142, Train Loss: 0.30305686593055725, Memory (GB): 4\n",
      "Epoch 2, Batch 143, Train Loss: 0.2972281873226166, Memory (GB): 4\n",
      "Epoch 2, Batch 144, Train Loss: 0.2826376259326935, Memory (GB): 4\n",
      "Epoch 2, Batch 145, Train Loss: 0.24421760439872742, Memory (GB): 4\n",
      "Epoch 2, Batch 146, Train Loss: 0.2979764938354492, Memory (GB): 4\n",
      "Epoch 2, Batch 147, Train Loss: 0.2529926002025604, Memory (GB): 4\n",
      "Epoch 2, Batch 148, Train Loss: 0.3722551763057709, Memory (GB): 4\n",
      "Epoch 2, Batch 149, Train Loss: 0.28648287057876587, Memory (GB): 4\n",
      "Epoch 2, Batch 150, Train Loss: 0.3120215833187103, Memory (GB): 4\n",
      "Epoch 2, Batch 151, Train Loss: 0.25648370385169983, Memory (GB): 4\n",
      "Epoch 2, Batch 152, Train Loss: 0.2874796688556671, Memory (GB): 4\n",
      "Epoch 2, Batch 153, Train Loss: 0.30950090289115906, Memory (GB): 4\n",
      "Epoch 2, Batch 154, Train Loss: 0.31817156076431274, Memory (GB): 4\n",
      "Epoch 2, Batch 155, Train Loss: 0.25944286584854126, Memory (GB): 4\n",
      "Epoch 2, Batch 156, Train Loss: 0.2968677580356598, Memory (GB): 4\n",
      "Epoch 2, Batch 157, Train Loss: 0.2656135559082031, Memory (GB): 4\n",
      "Epoch 2, Batch 158, Train Loss: 0.28942441940307617, Memory (GB): 4\n",
      "Epoch 2, Batch 159, Train Loss: 0.3048785328865051, Memory (GB): 4\n",
      "Epoch 2, Batch 160, Train Loss: 0.2756347954273224, Memory (GB): 4\n",
      "Epoch 2, Batch 161, Train Loss: 0.3063858449459076, Memory (GB): 4\n",
      "Epoch 2, Batch 162, Train Loss: 0.2676597535610199, Memory (GB): 4\n",
      "Epoch 2, Batch 163, Train Loss: 0.30191558599472046, Memory (GB): 4\n",
      "Epoch 2, Batch 164, Train Loss: 0.3085409700870514, Memory (GB): 4\n",
      "Epoch 2, Batch 165, Train Loss: 0.27708521485328674, Memory (GB): 4\n",
      "Epoch 2, Batch 166, Train Loss: 0.29470571875572205, Memory (GB): 4\n",
      "Epoch 2, Batch 167, Train Loss: 0.3006064295768738, Memory (GB): 4\n",
      "Epoch 2, Batch 168, Train Loss: 0.28076857328414917, Memory (GB): 4\n",
      "Epoch 2, Batch 169, Train Loss: 0.2955932021141052, Memory (GB): 4\n",
      "Epoch 2, Batch 170, Train Loss: 0.2816307544708252, Memory (GB): 4\n",
      "Epoch 2, Batch 171, Train Loss: 0.30721426010131836, Memory (GB): 4\n",
      "Epoch 2, Batch 172, Train Loss: 0.29671546816825867, Memory (GB): 4\n",
      "Epoch 2, Batch 173, Train Loss: 0.2912069857120514, Memory (GB): 4\n",
      "Epoch 2, Batch 174, Train Loss: 0.3084365427494049, Memory (GB): 4\n",
      "Epoch 2, Batch 175, Train Loss: 0.2929956018924713, Memory (GB): 4\n",
      "Epoch 2, Batch 176, Train Loss: 0.31388577818870544, Memory (GB): 4\n",
      "Epoch 2, Batch 177, Train Loss: 0.3323807120323181, Memory (GB): 4\n",
      "Epoch 2, Batch 178, Train Loss: 0.3250720202922821, Memory (GB): 4\n",
      "Epoch 2, Batch 179, Train Loss: 0.30773019790649414, Memory (GB): 4\n",
      "Epoch 2, Batch 180, Train Loss: 0.308886855840683, Memory (GB): 4\n",
      "Epoch 2, Batch 181, Train Loss: 0.33764514327049255, Memory (GB): 4\n",
      "Epoch 2, Batch 182, Train Loss: 0.25823336839675903, Memory (GB): 4\n",
      "Epoch 2, Batch 183, Train Loss: 0.26366814970970154, Memory (GB): 4\n",
      "Epoch 2, Batch 184, Train Loss: 0.30713218450546265, Memory (GB): 4\n",
      "Epoch 2, Batch 185, Train Loss: 0.26719537377357483, Memory (GB): 4\n",
      "Epoch 2, Batch 186, Train Loss: 0.30833715200424194, Memory (GB): 4\n",
      "Epoch 2, Batch 187, Train Loss: 0.29758480191230774, Memory (GB): 4\n",
      "Epoch 2, Batch 188, Train Loss: 0.3015810549259186, Memory (GB): 4\n",
      "Epoch 2, Batch 189, Train Loss: 0.28339871764183044, Memory (GB): 4\n",
      "Epoch 2, Batch 190, Train Loss: 0.25913503766059875, Memory (GB): 4\n",
      "Epoch 2, Batch 191, Train Loss: 0.3495851159095764, Memory (GB): 4\n",
      "Epoch 2, Batch 192, Train Loss: 0.26905760169029236, Memory (GB): 4\n",
      "Epoch 2, Batch 193, Train Loss: 0.3333699107170105, Memory (GB): 4\n",
      "Epoch 2, Batch 194, Train Loss: 0.2553127110004425, Memory (GB): 4\n",
      "Epoch 2, Batch 195, Train Loss: 0.24251942336559296, Memory (GB): 4\n",
      "Epoch 2, Batch 196, Train Loss: 0.24883495271205902, Memory (GB): 4\n",
      "Epoch 2, Batch 197, Train Loss: 0.2921566069126129, Memory (GB): 4\n",
      "Epoch 2, Batch 198, Train Loss: 0.30096951127052307, Memory (GB): 4\n",
      "Epoch 2, Batch 199, Train Loss: 0.2835783362388611, Memory (GB): 4\n",
      "Epoch 2, Batch 200, Train Loss: 0.269253671169281, Memory (GB): 4\n",
      "Epoch 2, Batch 201, Train Loss: 0.3012697398662567, Memory (GB): 4\n",
      "Epoch 2, Batch 202, Train Loss: 0.2894534766674042, Memory (GB): 4\n",
      "Epoch 2, Batch 203, Train Loss: 0.3059328496456146, Memory (GB): 4\n",
      "Epoch 2, Batch 204, Train Loss: 0.2853056490421295, Memory (GB): 4\n",
      "Epoch 2, Batch 205, Train Loss: 0.3259168267250061, Memory (GB): 4\n",
      "Epoch 2, Batch 206, Train Loss: 0.3048117756843567, Memory (GB): 4\n",
      "Epoch 2, Batch 207, Train Loss: 0.2562865614891052, Memory (GB): 4\n",
      "Epoch 2, Batch 208, Train Loss: 0.2884121537208557, Memory (GB): 4\n",
      "Epoch 2, Batch 209, Train Loss: 0.328206330537796, Memory (GB): 4\n",
      "Epoch 2, Batch 210, Train Loss: 0.3122083842754364, Memory (GB): 4\n",
      "Epoch 2, Batch 211, Train Loss: 0.2610791027545929, Memory (GB): 4\n",
      "Epoch 2, Batch 212, Train Loss: 0.27536118030548096, Memory (GB): 4\n",
      "Epoch 2, Batch 213, Train Loss: 0.24950361251831055, Memory (GB): 4\n",
      "Epoch 2, Batch 214, Train Loss: 0.30886712670326233, Memory (GB): 4\n",
      "Epoch 2, Batch 215, Train Loss: 0.2770039141178131, Memory (GB): 4\n",
      "Epoch 2, Batch 216, Train Loss: 0.3081422448158264, Memory (GB): 4\n",
      "Epoch 2, Batch 217, Train Loss: 0.28243154287338257, Memory (GB): 4\n",
      "Epoch 2, Batch 218, Train Loss: 0.36422109603881836, Memory (GB): 4\n",
      "Epoch 2, Batch 219, Train Loss: 0.2947814166545868, Memory (GB): 4\n",
      "Epoch 2, Batch 220, Train Loss: 0.27355262637138367, Memory (GB): 4\n",
      "Epoch 2, Batch 221, Train Loss: 0.32575660943984985, Memory (GB): 4\n",
      "Epoch 2, Batch 222, Train Loss: 0.26249581575393677, Memory (GB): 4\n",
      "Epoch 2, Batch 223, Train Loss: 0.2791401445865631, Memory (GB): 4\n",
      "Epoch 2, Batch 224, Train Loss: 0.2773086726665497, Memory (GB): 4\n",
      "Epoch 2, Batch 225, Train Loss: 0.30813851952552795, Memory (GB): 4\n",
      "Epoch 2, Batch 226, Train Loss: 0.26590773463249207, Memory (GB): 4\n",
      "Epoch 2, Batch 227, Train Loss: 0.2788802981376648, Memory (GB): 4\n",
      "Epoch 2, Batch 228, Train Loss: 0.2900991439819336, Memory (GB): 4\n",
      "Epoch 2, Batch 229, Train Loss: 0.3253595232963562, Memory (GB): 4\n",
      "Epoch 2, Batch 230, Train Loss: 0.2792704403400421, Memory (GB): 4\n",
      "Epoch 2, Batch 231, Train Loss: 0.28725576400756836, Memory (GB): 4\n",
      "Epoch 2, Batch 232, Train Loss: 0.2970026433467865, Memory (GB): 4\n",
      "Epoch 2, Batch 233, Train Loss: 0.29659366607666016, Memory (GB): 4\n",
      "Epoch 2, Batch 234, Train Loss: 0.32904019951820374, Memory (GB): 4\n",
      "Epoch 2, Batch 235, Train Loss: 0.287534236907959, Memory (GB): 4\n",
      "Epoch 2, Batch 236, Train Loss: 0.2990981638431549, Memory (GB): 4\n",
      "Epoch 2, Batch 237, Train Loss: 0.28001922369003296, Memory (GB): 4\n",
      "Epoch 2, Batch 238, Train Loss: 0.28186818957328796, Memory (GB): 4\n",
      "Epoch 2, Batch 239, Train Loss: 0.29166343808174133, Memory (GB): 4\n",
      "Epoch 2, Batch 240, Train Loss: 0.260135680437088, Memory (GB): 4\n",
      "Epoch 2, Batch 241, Train Loss: 0.28952282667160034, Memory (GB): 4\n",
      "Epoch 2, Batch 242, Train Loss: 0.2448621541261673, Memory (GB): 4\n",
      "Epoch 2, Batch 243, Train Loss: 0.24207070469856262, Memory (GB): 4\n",
      "Epoch 2, Batch 244, Train Loss: 0.27149340510368347, Memory (GB): 4\n",
      "Epoch 2, Batch 245, Train Loss: 0.29065483808517456, Memory (GB): 4\n",
      "Epoch 2, Batch 246, Train Loss: 0.30049481987953186, Memory (GB): 4\n",
      "Epoch 2, Batch 247, Train Loss: 0.23464004695415497, Memory (GB): 4\n",
      "Epoch 2, Batch 248, Train Loss: 0.2668951451778412, Memory (GB): 4\n",
      "Epoch 2, Batch 249, Train Loss: 0.2720721662044525, Memory (GB): 4\n",
      "Epoch 2, Batch 250, Train Loss: 0.2728261649608612, Memory (GB): 4\n",
      "Epoch 2, Batch 251, Train Loss: 0.2639146149158478, Memory (GB): 4\n",
      "Epoch 2, Batch 252, Train Loss: 0.292863130569458, Memory (GB): 4\n",
      "Epoch 2, Batch 253, Train Loss: 0.2778197228908539, Memory (GB): 4\n",
      "Epoch 2, Batch 254, Train Loss: 0.25372013449668884, Memory (GB): 4\n",
      "Epoch 2, Batch 255, Train Loss: 0.2665615677833557, Memory (GB): 4\n",
      "Epoch 2, Batch 256, Train Loss: 0.32717862725257874, Memory (GB): 4\n",
      "Epoch 2, Batch 257, Train Loss: 0.30263569951057434, Memory (GB): 4\n",
      "Epoch 2, Batch 258, Train Loss: 0.2664194405078888, Memory (GB): 4\n",
      "Epoch 2, Batch 259, Train Loss: 0.26372721791267395, Memory (GB): 4\n",
      "Epoch 2, Batch 260, Train Loss: 0.2719624638557434, Memory (GB): 4\n",
      "Epoch 2, Batch 261, Train Loss: 0.27087894082069397, Memory (GB): 4\n",
      "Epoch 2, Batch 262, Train Loss: 0.3327966332435608, Memory (GB): 4\n",
      "Epoch 2, Batch 263, Train Loss: 0.28046372532844543, Memory (GB): 4\n",
      "Epoch 2, Batch 264, Train Loss: 0.2957085967063904, Memory (GB): 4\n",
      "Epoch 2, Batch 265, Train Loss: 0.329833984375, Memory (GB): 4\n",
      "Epoch 2, Batch 266, Train Loss: 0.31452158093452454, Memory (GB): 4\n",
      "Epoch 2, Batch 267, Train Loss: 0.2775689959526062, Memory (GB): 4\n",
      "Epoch 2, Batch 268, Train Loss: 0.30753618478775024, Memory (GB): 4\n",
      "Epoch 2, Batch 269, Train Loss: 0.29578667879104614, Memory (GB): 4\n",
      "[9.384786694707438e-05, 8.915547359972065e-05, 8.469769991973463e-05, 8.04628149237479e-05, 7.643967417756049e-05, 7.261769046868245e-05, 6.898680594524836e-05, 6.553746564798587e-05, 6.22605923655866e-05, 5.914756274730729e-05, 5.619018460994191e-05, 5.338067537944478e-05, 5.071164161047257e-05, 4.8176059529948924e-05, 4.576725655345148e-05, 4.347889372577892e-05, 4.130494903948995e-05, 3.923970158751546e-05, 3.72777165081397e-05, 3.5413830682732704e-05, 3.3643139148596064e-05, 3.196098219116625e-05, 3.0362933081607935e-05, 2.8844786427527534e-05, 2.740254710615116e-05, 2.60324197508436e-05, 2.4730798763301428e-05, 2.3494258825136337e-05, 2.2319545883879523e-05, 2.120356858968554e-05, 2.014339016020127e-05, 1.9136220652191208e-05, 1.817940961958165e-05, 1.7270439138602573e-05, 1.6406917181672443e-05, 1.5586571322588806e-05, 1.4807242756459373e-05, 1.40668806186364e-05, 1.3363536587704583e-05, 1.2695359758319348e-05, 1.2060591770403377e-05, 1.145756218188321e-05, 1.0884684072789048e-05, 1.0340449869149592e-05, 9.823427375692116e-06, 9.332256006907511e-06, 8.865643206562131e-06, 8.422361046234027e-06, 8.001242993922322e-06, 7.601180844226208e-06, 7.221121802014899e-06, 6.860065711914152e-06, 6.517062426318443e-06, 6.191209305002522e-06, 5.881648839752395e-06, 5.5875663977647755e-06, 5.308188077876535e-06, 5.042778673982709e-06, 4.790639740283576e-06, 4.5511077532693954e-06, 4.323552365605925e-06, 4.107374747325628e-06, 3.902006009959347e-06, 3.7069057094613787e-06, 3.52156042398831e-06, 3.345482402788894e-06, 3.1782082826494503e-06, 3.0192978685169763e-06, 2.8683329750911268e-06, 2.724916326336569e-06, 2.5886705100197416e-06, 2.4592369845187556e-06, 2.3362751352928173e-06, 2.2194613785281767e-06, 2.1084883096017674e-06, 2.0030638941216784e-06, 1.902910699415595e-06, 1.8077651644448157e-06, 1.717376906222575e-06, 1.6315080609114453e-06, 1.5499326578658737e-06, 1.4724360249725805e-06, 1.3988142237239509e-06, 1.3288735125377521e-06, 1.2624298369108653e-06, 1.1993083450653221e-06, 1.139342927812056e-06, 1.0823757814214528e-06, 1.02825699235038e-06, 9.768441427328615e-07, 9.280019355962184e-07, 8.816018388164074e-07, 8.375217468755865e-07, 7.956456595318075e-07, 7.558633765552172e-07, 7.180702077274563e-07, 6.821666973410837e-07, 6.480583624740293e-07, 6.156554443503275e-07, 5.848726721328111e-07, 5.556290385261702e-07, 5.278475865998619e-07, 5.014552072698692e-07, 4.763824469063754e-07, 4.5256332456105674e-07, 4.299351583330039e-07, 4.0843840041635385e-07, 3.88016480395536e-07, 3.6861565637575916e-07, 3.501848735569712e-07, 3.3267562987912264e-07, 3.1604184838516637e-07, 3.0023975596590826e-07, 2.8522776816761276e-07, 2.7096637975923204e-07, 2.574180607712705e-07, 2.4454715773270707e-07, 2.3231979984607148e-07, 2.207038098537679e-07, 2.0966861936107952e-07, 1.991851883930256e-07, 1.892259289733744e-07, 1.7976463252470553e-07, 1.707764008984702e-07, 1.622375808535468e-07, 1.5412570181086942e-07, 1.4641941672032583e-07, 1.3909844588430962e-07, 1.3214352359009417e-07, 1.2553634741058938e-07, 1.1925953004005996e-07, 1.1329655353805696e-07, 1.076317258611541e-07, 1.0225013956809638e-07, 9.713763258969159e-08, 9.228075096020702e-08, 8.766671341219665e-08, 8.328337774158682e-08, 7.911920885450743e-08, 7.516324841178206e-08, 7.140508599119298e-08, 6.783483169163335e-08, 6.444309010705165e-08, 6.12209356016991e-08, 5.81598888216141e-08, 5.525189438053339e-08, 5.2489299661506726e-08, 4.986483467843139e-08, 4.737159294450982e-08, 4.500301329728432e-08, 4.2752862632420085e-08, 4.061521950079911e-08, 3.858445852575916e-08, 3.66552355994712e-08, 3.4822473819497634e-08, 3.308135012852274e-08, 3.14272826220966e-08, 2.985591849099178e-08, 2.8363122566442186e-08, 2.694496643812007e-08, 2.5597718116214073e-08, 2.4317832210403347e-08, 2.31019405998832e-08, 2.1946843569889042e-08, 2.084950139139457e-08, 1.9807026321824843e-08, 1.88166750057336e-08, 1.7875841255446912e-08, 1.6982049192674563e-08, 1.6132946733040844e-08, 1.532629939638879e-08, 1.4559984426569362e-08, 1.3831985205240895e-08, 1.3140385944978852e-08, 1.2483366647729904e-08, 1.1859198315343407e-08, 1.1266238399576238e-08, 1.0702926479597423e-08, 1.0167780155617555e-08, 9.659391147836672e-09, 9.176421590444842e-09, 8.717600510922606e-09, 8.281720485376465e-09, 7.86763446110765e-09, 7.474252738052264e-09, 7.100540101149646e-09, 6.745513096092165e-09, 6.408237441287558e-09, 6.08782556922318e-09, 5.783434290762021e-09, 5.494262576223917e-09, 5.2195494474127206e-09, 4.958571975042086e-09, 4.710643376289982e-09, 4.4751112074754804e-09, 4.251355647101706e-09, 4.038787864746622e-09, 3.836848471509288e-09, 3.645006047933826e-09]\n",
      "Epoch 3, Batch 0, Train Loss: 0.3034580647945404, Memory (GB): 4\n",
      "Epoch 3, Batch 1, Train Loss: 0.29782432317733765, Memory (GB): 4\n",
      "Epoch 3, Batch 2, Train Loss: 0.34053468704223633, Memory (GB): 4\n",
      "Epoch 3, Batch 3, Train Loss: 0.21835386753082275, Memory (GB): 4\n",
      "Epoch 3, Batch 4, Train Loss: 0.3161048889160156, Memory (GB): 4\n",
      "Epoch 3, Batch 5, Train Loss: 0.2857297360897064, Memory (GB): 4\n",
      "Epoch 3, Batch 6, Train Loss: 0.27016162872314453, Memory (GB): 4\n",
      "Epoch 3, Batch 7, Train Loss: 0.24889616668224335, Memory (GB): 4\n",
      "Epoch 3, Batch 8, Train Loss: 0.2804425060749054, Memory (GB): 4\n",
      "Epoch 3, Batch 9, Train Loss: 0.3366383910179138, Memory (GB): 4\n",
      "Epoch 3, Batch 10, Train Loss: 0.27717605233192444, Memory (GB): 4\n",
      "Epoch 3, Batch 11, Train Loss: 0.276373028755188, Memory (GB): 4\n",
      "Epoch 3, Batch 12, Train Loss: 0.30097347497940063, Memory (GB): 4\n",
      "Epoch 3, Batch 13, Train Loss: 0.262927383184433, Memory (GB): 4\n",
      "Epoch 3, Batch 14, Train Loss: 0.293577641248703, Memory (GB): 4\n",
      "Epoch 3, Batch 15, Train Loss: 0.2571719288825989, Memory (GB): 4\n",
      "Epoch 3, Batch 16, Train Loss: 0.2625972628593445, Memory (GB): 4\n",
      "Epoch 3, Batch 17, Train Loss: 0.30992215871810913, Memory (GB): 4\n",
      "Epoch 3, Batch 18, Train Loss: 0.30363836884498596, Memory (GB): 4\n",
      "Epoch 3, Batch 19, Train Loss: 0.31123849749565125, Memory (GB): 4\n",
      "Epoch 3, Batch 20, Train Loss: 0.28816908597946167, Memory (GB): 4\n",
      "Epoch 3, Batch 21, Train Loss: 0.24996423721313477, Memory (GB): 4\n",
      "Epoch 3, Batch 22, Train Loss: 0.29688769578933716, Memory (GB): 4\n",
      "Epoch 3, Batch 23, Train Loss: 0.3322269320487976, Memory (GB): 4\n",
      "Epoch 3, Batch 24, Train Loss: 0.27481430768966675, Memory (GB): 4\n",
      "Epoch 3, Batch 25, Train Loss: 0.31661170721054077, Memory (GB): 4\n",
      "Epoch 3, Batch 26, Train Loss: 0.279523104429245, Memory (GB): 4\n",
      "Epoch 3, Batch 27, Train Loss: 0.28427281975746155, Memory (GB): 4\n",
      "Epoch 3, Batch 28, Train Loss: 0.26237863302230835, Memory (GB): 4\n",
      "Epoch 3, Batch 29, Train Loss: 0.28836727142333984, Memory (GB): 4\n",
      "Epoch 3, Batch 30, Train Loss: 0.30142197012901306, Memory (GB): 4\n",
      "Epoch 3, Batch 31, Train Loss: 0.26117682456970215, Memory (GB): 4\n",
      "Epoch 3, Batch 32, Train Loss: 0.28932175040245056, Memory (GB): 4\n",
      "Epoch 3, Batch 33, Train Loss: 0.2933385670185089, Memory (GB): 4\n",
      "Epoch 3, Batch 34, Train Loss: 0.26540571451187134, Memory (GB): 4\n",
      "Epoch 3, Batch 35, Train Loss: 0.2832406163215637, Memory (GB): 4\n",
      "Epoch 3, Batch 36, Train Loss: 0.262225478887558, Memory (GB): 4\n",
      "Epoch 3, Batch 37, Train Loss: 0.2784039080142975, Memory (GB): 4\n",
      "Epoch 3, Batch 38, Train Loss: 0.26769784092903137, Memory (GB): 4\n",
      "Epoch 3, Batch 39, Train Loss: 0.2894830107688904, Memory (GB): 4\n",
      "Epoch 3, Batch 40, Train Loss: 0.2863372266292572, Memory (GB): 4\n",
      "Epoch 3, Batch 41, Train Loss: 0.3047044575214386, Memory (GB): 4\n",
      "Epoch 3, Batch 42, Train Loss: 0.2613205015659332, Memory (GB): 4\n",
      "Epoch 3, Batch 43, Train Loss: 0.2782383859157562, Memory (GB): 4\n",
      "Epoch 3, Batch 44, Train Loss: 0.27168720960617065, Memory (GB): 4\n",
      "Epoch 3, Batch 45, Train Loss: 0.27723100781440735, Memory (GB): 4\n",
      "Epoch 3, Batch 46, Train Loss: 0.31290706992149353, Memory (GB): 4\n",
      "Epoch 3, Batch 47, Train Loss: 0.2991139590740204, Memory (GB): 4\n",
      "Epoch 3, Batch 48, Train Loss: 0.3422999083995819, Memory (GB): 4\n",
      "Epoch 3, Batch 49, Train Loss: 0.2849033772945404, Memory (GB): 4\n",
      "Epoch 3, Batch 50, Train Loss: 0.2463018000125885, Memory (GB): 4\n",
      "Epoch 3, Batch 51, Train Loss: 0.25526872277259827, Memory (GB): 4\n",
      "Epoch 3, Batch 52, Train Loss: 0.2849881649017334, Memory (GB): 4\n",
      "Epoch 3, Batch 53, Train Loss: 0.2901933193206787, Memory (GB): 4\n",
      "Epoch 3, Batch 54, Train Loss: 0.33908146619796753, Memory (GB): 4\n",
      "Epoch 3, Batch 55, Train Loss: 0.29659369587898254, Memory (GB): 4\n",
      "Epoch 3, Batch 56, Train Loss: 0.2734321355819702, Memory (GB): 4\n",
      "Epoch 3, Batch 57, Train Loss: 0.26900503039360046, Memory (GB): 4\n",
      "Epoch 3, Batch 58, Train Loss: 0.2685365378856659, Memory (GB): 4\n",
      "Epoch 3, Batch 59, Train Loss: 0.32351502776145935, Memory (GB): 4\n",
      "Epoch 3, Batch 60, Train Loss: 0.29101985692977905, Memory (GB): 4\n",
      "Epoch 3, Batch 61, Train Loss: 0.27296289801597595, Memory (GB): 4\n",
      "Epoch 3, Batch 62, Train Loss: 0.2583079934120178, Memory (GB): 4\n",
      "Epoch 3, Batch 63, Train Loss: 0.2601765990257263, Memory (GB): 4\n",
      "Epoch 3, Batch 64, Train Loss: 0.3154965043067932, Memory (GB): 4\n",
      "Epoch 3, Batch 65, Train Loss: 0.28074342012405396, Memory (GB): 4\n",
      "Epoch 3, Batch 66, Train Loss: 0.2813318371772766, Memory (GB): 4\n",
      "Epoch 3, Batch 67, Train Loss: 0.27476397156715393, Memory (GB): 4\n",
      "Epoch 3, Batch 68, Train Loss: 0.2538736164569855, Memory (GB): 4\n",
      "Epoch 3, Batch 69, Train Loss: 0.26257187128067017, Memory (GB): 4\n",
      "Epoch 3, Batch 70, Train Loss: 0.25113406777381897, Memory (GB): 4\n",
      "Epoch 3, Batch 71, Train Loss: 0.2823192775249481, Memory (GB): 4\n",
      "Epoch 3, Batch 72, Train Loss: 0.29012542963027954, Memory (GB): 4\n",
      "Epoch 3, Batch 73, Train Loss: 0.2961865961551666, Memory (GB): 4\n",
      "Epoch 3, Batch 74, Train Loss: 0.310122013092041, Memory (GB): 4\n",
      "Epoch 3, Batch 75, Train Loss: 0.27125245332717896, Memory (GB): 4\n",
      "Epoch 3, Batch 76, Train Loss: 0.34859031438827515, Memory (GB): 4\n",
      "Epoch 3, Batch 77, Train Loss: 0.29287078976631165, Memory (GB): 4\n",
      "Epoch 3, Batch 78, Train Loss: 0.2675482928752899, Memory (GB): 4\n",
      "Epoch 3, Batch 79, Train Loss: 0.27460065484046936, Memory (GB): 4\n",
      "Epoch 3, Batch 80, Train Loss: 0.28017860651016235, Memory (GB): 4\n",
      "Epoch 3, Batch 81, Train Loss: 0.2876451015472412, Memory (GB): 4\n",
      "Epoch 3, Batch 82, Train Loss: 0.276362806558609, Memory (GB): 4\n",
      "Epoch 3, Batch 83, Train Loss: 0.27098920941352844, Memory (GB): 4\n",
      "Epoch 3, Batch 84, Train Loss: 0.30387523770332336, Memory (GB): 4\n",
      "Epoch 3, Batch 85, Train Loss: 0.3005695939064026, Memory (GB): 4\n",
      "Epoch 3, Batch 86, Train Loss: 0.2697257697582245, Memory (GB): 4\n",
      "Epoch 3, Batch 87, Train Loss: 0.28010648488998413, Memory (GB): 4\n",
      "Epoch 3, Batch 88, Train Loss: 0.2898704409599304, Memory (GB): 4\n",
      "Epoch 3, Batch 89, Train Loss: 0.28427067399024963, Memory (GB): 4\n",
      "Epoch 3, Batch 90, Train Loss: 0.2640790343284607, Memory (GB): 4\n",
      "Epoch 3, Batch 91, Train Loss: 0.24676071107387543, Memory (GB): 4\n",
      "Epoch 3, Batch 92, Train Loss: 0.27575045824050903, Memory (GB): 4\n",
      "Epoch 3, Batch 93, Train Loss: 0.27168628573417664, Memory (GB): 4\n",
      "Epoch 3, Batch 94, Train Loss: 0.2710011303424835, Memory (GB): 4\n",
      "Epoch 3, Batch 95, Train Loss: 0.2637830674648285, Memory (GB): 4\n",
      "Epoch 3, Batch 96, Train Loss: 0.2858421504497528, Memory (GB): 4\n",
      "Epoch 3, Batch 97, Train Loss: 0.294456422328949, Memory (GB): 4\n",
      "Epoch 3, Batch 98, Train Loss: 0.3029962182044983, Memory (GB): 4\n",
      "Epoch 3, Batch 99, Train Loss: 0.2766394019126892, Memory (GB): 4\n",
      "Epoch 3, Batch 100, Train Loss: 0.2669687569141388, Memory (GB): 4\n",
      "Epoch 3, Batch 101, Train Loss: 0.2876533567905426, Memory (GB): 4\n",
      "Epoch 3, Batch 102, Train Loss: 0.2441624253988266, Memory (GB): 4\n",
      "Epoch 3, Batch 103, Train Loss: 0.28315600752830505, Memory (GB): 4\n",
      "Epoch 3, Batch 104, Train Loss: 0.27053847908973694, Memory (GB): 4\n",
      "Epoch 3, Batch 105, Train Loss: 0.2505084276199341, Memory (GB): 4\n",
      "Epoch 3, Batch 106, Train Loss: 0.28967681527137756, Memory (GB): 4\n",
      "Epoch 3, Batch 107, Train Loss: 0.25674915313720703, Memory (GB): 4\n",
      "Epoch 3, Batch 108, Train Loss: 0.2915334403514862, Memory (GB): 4\n",
      "Epoch 3, Batch 109, Train Loss: 0.3106814920902252, Memory (GB): 4\n",
      "Epoch 3, Batch 110, Train Loss: 0.2689445912837982, Memory (GB): 4\n",
      "Epoch 3, Batch 111, Train Loss: 0.3088921010494232, Memory (GB): 4\n",
      "Epoch 3, Batch 112, Train Loss: 0.2716827392578125, Memory (GB): 4\n",
      "Epoch 3, Batch 113, Train Loss: 0.28693464398384094, Memory (GB): 4\n",
      "Epoch 3, Batch 114, Train Loss: 0.27834466099739075, Memory (GB): 4\n",
      "Epoch 3, Batch 115, Train Loss: 0.3290221691131592, Memory (GB): 4\n",
      "Epoch 3, Batch 116, Train Loss: 0.25512051582336426, Memory (GB): 4\n",
      "Epoch 3, Batch 117, Train Loss: 0.3147048354148865, Memory (GB): 4\n",
      "Epoch 3, Batch 118, Train Loss: 0.3007276654243469, Memory (GB): 4\n",
      "Epoch 3, Batch 119, Train Loss: 0.29084253311157227, Memory (GB): 4\n",
      "Epoch 3, Batch 120, Train Loss: 0.3125093877315521, Memory (GB): 4\n",
      "Epoch 3, Batch 121, Train Loss: 0.2862088978290558, Memory (GB): 4\n",
      "Epoch 3, Batch 122, Train Loss: 0.27437400817871094, Memory (GB): 4\n",
      "Epoch 3, Batch 123, Train Loss: 0.23886756598949432, Memory (GB): 4\n",
      "Epoch 3, Batch 124, Train Loss: 0.28773775696754456, Memory (GB): 4\n",
      "Epoch 3, Batch 125, Train Loss: 0.27851060032844543, Memory (GB): 4\n",
      "Epoch 3, Batch 126, Train Loss: 0.2868371605873108, Memory (GB): 4\n",
      "Epoch 3, Batch 127, Train Loss: 0.2738710939884186, Memory (GB): 4\n",
      "Epoch 3, Batch 128, Train Loss: 0.23943071067333221, Memory (GB): 4\n",
      "Epoch 3, Batch 129, Train Loss: 0.2929857075214386, Memory (GB): 4\n",
      "Epoch 3, Batch 130, Train Loss: 0.2936317026615143, Memory (GB): 4\n",
      "Epoch 3, Batch 131, Train Loss: 0.2772773206233978, Memory (GB): 4\n",
      "Epoch 3, Batch 132, Train Loss: 0.26218274235725403, Memory (GB): 4\n",
      "Epoch 3, Batch 133, Train Loss: 0.24772100150585175, Memory (GB): 4\n",
      "Epoch 3, Batch 134, Train Loss: 0.32568150758743286, Memory (GB): 4\n",
      "Epoch 3, Batch 135, Train Loss: 0.3154143691062927, Memory (GB): 4\n",
      "Epoch 3, Batch 136, Train Loss: 0.307013601064682, Memory (GB): 4\n",
      "Epoch 3, Batch 137, Train Loss: 0.27854204177856445, Memory (GB): 4\n",
      "Epoch 3, Batch 138, Train Loss: 0.2581384479999542, Memory (GB): 4\n",
      "Epoch 3, Batch 139, Train Loss: 0.2924725115299225, Memory (GB): 4\n",
      "Epoch 3, Batch 140, Train Loss: 0.2562650144100189, Memory (GB): 4\n",
      "Epoch 3, Batch 141, Train Loss: 0.3088795840740204, Memory (GB): 4\n",
      "Epoch 3, Batch 142, Train Loss: 0.27025938034057617, Memory (GB): 4\n",
      "Epoch 3, Batch 143, Train Loss: 0.25247037410736084, Memory (GB): 4\n",
      "Epoch 3, Batch 144, Train Loss: 0.2819249927997589, Memory (GB): 4\n",
      "Epoch 3, Batch 145, Train Loss: 0.28288060426712036, Memory (GB): 4\n",
      "Epoch 3, Batch 146, Train Loss: 0.2955629527568817, Memory (GB): 4\n",
      "Epoch 3, Batch 147, Train Loss: 0.28758054971694946, Memory (GB): 4\n",
      "Epoch 3, Batch 148, Train Loss: 0.27646979689598083, Memory (GB): 4\n",
      "Epoch 3, Batch 149, Train Loss: 0.3218831419944763, Memory (GB): 4\n",
      "Epoch 3, Batch 150, Train Loss: 0.2742871642112732, Memory (GB): 4\n",
      "Epoch 3, Batch 151, Train Loss: 0.31743261218070984, Memory (GB): 4\n",
      "Epoch 3, Batch 152, Train Loss: 0.28623661398887634, Memory (GB): 4\n",
      "Epoch 3, Batch 153, Train Loss: 0.329048216342926, Memory (GB): 4\n",
      "Epoch 3, Batch 154, Train Loss: 0.28532448410987854, Memory (GB): 4\n",
      "Epoch 3, Batch 155, Train Loss: 0.2545742094516754, Memory (GB): 4\n",
      "Epoch 3, Batch 156, Train Loss: 0.2713814675807953, Memory (GB): 4\n",
      "Epoch 3, Batch 157, Train Loss: 0.2587231993675232, Memory (GB): 4\n",
      "Epoch 3, Batch 158, Train Loss: 0.2706396281719208, Memory (GB): 4\n",
      "Epoch 3, Batch 159, Train Loss: 0.2737235426902771, Memory (GB): 4\n",
      "Epoch 3, Batch 160, Train Loss: 0.24575766921043396, Memory (GB): 4\n",
      "Epoch 3, Batch 161, Train Loss: 0.2815646827220917, Memory (GB): 4\n",
      "Epoch 3, Batch 162, Train Loss: 0.2842387855052948, Memory (GB): 4\n",
      "Epoch 3, Batch 163, Train Loss: 0.2561527192592621, Memory (GB): 4\n",
      "Epoch 3, Batch 164, Train Loss: 0.298723429441452, Memory (GB): 4\n",
      "Epoch 3, Batch 165, Train Loss: 0.2816574275493622, Memory (GB): 4\n",
      "Epoch 3, Batch 166, Train Loss: 0.2908463180065155, Memory (GB): 4\n",
      "Epoch 3, Batch 167, Train Loss: 0.2521111071109772, Memory (GB): 4\n",
      "Epoch 3, Batch 168, Train Loss: 0.22062189877033234, Memory (GB): 4\n",
      "Epoch 3, Batch 169, Train Loss: 0.2823706269264221, Memory (GB): 4\n",
      "Epoch 3, Batch 170, Train Loss: 0.3153474032878876, Memory (GB): 4\n",
      "Epoch 3, Batch 171, Train Loss: 0.269146203994751, Memory (GB): 4\n",
      "Epoch 3, Batch 172, Train Loss: 0.2750580906867981, Memory (GB): 4\n",
      "Epoch 3, Batch 173, Train Loss: 0.3247660994529724, Memory (GB): 4\n",
      "Epoch 3, Batch 174, Train Loss: 0.26498404145240784, Memory (GB): 4\n",
      "Epoch 3, Batch 175, Train Loss: 0.31967952847480774, Memory (GB): 4\n",
      "Epoch 3, Batch 176, Train Loss: 0.2934359014034271, Memory (GB): 4\n",
      "Epoch 3, Batch 177, Train Loss: 0.29452991485595703, Memory (GB): 4\n",
      "Epoch 3, Batch 178, Train Loss: 0.2795829474925995, Memory (GB): 4\n",
      "Epoch 3, Batch 179, Train Loss: 0.28166383504867554, Memory (GB): 4\n",
      "Epoch 3, Batch 180, Train Loss: 0.2939566969871521, Memory (GB): 4\n",
      "Epoch 3, Batch 181, Train Loss: 0.28143310546875, Memory (GB): 4\n",
      "Epoch 3, Batch 182, Train Loss: 0.23784688115119934, Memory (GB): 4\n",
      "Epoch 3, Batch 183, Train Loss: 0.30051279067993164, Memory (GB): 4\n",
      "Epoch 3, Batch 184, Train Loss: 0.2721906304359436, Memory (GB): 4\n",
      "Epoch 3, Batch 185, Train Loss: 0.27593544125556946, Memory (GB): 4\n",
      "Epoch 3, Batch 186, Train Loss: 0.28906914591789246, Memory (GB): 4\n",
      "Epoch 3, Batch 187, Train Loss: 0.255023330450058, Memory (GB): 4\n",
      "Epoch 3, Batch 188, Train Loss: 0.297905296087265, Memory (GB): 4\n",
      "Epoch 3, Batch 189, Train Loss: 0.26263827085494995, Memory (GB): 4\n",
      "Epoch 3, Batch 190, Train Loss: 0.26802125573158264, Memory (GB): 4\n",
      "Epoch 3, Batch 191, Train Loss: 0.23232300579547882, Memory (GB): 4\n",
      "Epoch 3, Batch 192, Train Loss: 0.2917487323284149, Memory (GB): 4\n",
      "Epoch 3, Batch 193, Train Loss: 0.238120898604393, Memory (GB): 4\n",
      "Epoch 3, Batch 194, Train Loss: 0.3172819912433624, Memory (GB): 4\n",
      "Epoch 3, Batch 195, Train Loss: 0.2859131693840027, Memory (GB): 4\n",
      "Epoch 3, Batch 196, Train Loss: 0.25391727685928345, Memory (GB): 4\n",
      "Epoch 3, Batch 197, Train Loss: 0.26995882391929626, Memory (GB): 4\n",
      "Epoch 3, Batch 198, Train Loss: 0.2453155815601349, Memory (GB): 4\n",
      "Epoch 3, Batch 199, Train Loss: 0.27829352021217346, Memory (GB): 4\n",
      "Epoch 3, Batch 200, Train Loss: 0.2967095971107483, Memory (GB): 4\n",
      "Epoch 3, Batch 201, Train Loss: 0.2866857349872589, Memory (GB): 4\n",
      "Epoch 3, Batch 202, Train Loss: 0.2998732924461365, Memory (GB): 4\n",
      "Epoch 3, Batch 203, Train Loss: 0.2723957598209381, Memory (GB): 4\n",
      "Epoch 3, Batch 204, Train Loss: 0.2623405158519745, Memory (GB): 4\n",
      "Epoch 3, Batch 205, Train Loss: 0.2535540759563446, Memory (GB): 4\n",
      "Epoch 3, Batch 206, Train Loss: 0.2560352683067322, Memory (GB): 4\n",
      "Epoch 3, Batch 207, Train Loss: 0.30843400955200195, Memory (GB): 4\n",
      "Epoch 3, Batch 208, Train Loss: 0.33679765462875366, Memory (GB): 4\n",
      "Epoch 3, Batch 209, Train Loss: 0.2684907019138336, Memory (GB): 4\n",
      "Epoch 3, Batch 210, Train Loss: 0.25697362422943115, Memory (GB): 4\n",
      "Epoch 3, Batch 211, Train Loss: 0.26695889234542847, Memory (GB): 4\n",
      "Epoch 3, Batch 212, Train Loss: 0.32442617416381836, Memory (GB): 4\n",
      "Epoch 3, Batch 213, Train Loss: 0.28422215580940247, Memory (GB): 4\n",
      "Epoch 3, Batch 214, Train Loss: 0.24483391642570496, Memory (GB): 4\n",
      "Epoch 3, Batch 215, Train Loss: 0.25882354378700256, Memory (GB): 4\n",
      "Epoch 3, Batch 216, Train Loss: 0.2792297899723053, Memory (GB): 4\n",
      "Epoch 3, Batch 217, Train Loss: 0.3260062038898468, Memory (GB): 4\n",
      "Epoch 3, Batch 218, Train Loss: 0.30111607909202576, Memory (GB): 4\n",
      "Epoch 3, Batch 219, Train Loss: 0.29636070132255554, Memory (GB): 4\n",
      "Epoch 3, Batch 220, Train Loss: 0.29090213775634766, Memory (GB): 4\n",
      "Epoch 3, Batch 221, Train Loss: 0.3049755096435547, Memory (GB): 4\n",
      "Epoch 3, Batch 222, Train Loss: 0.2600990831851959, Memory (GB): 4\n",
      "Epoch 3, Batch 223, Train Loss: 0.26570114493370056, Memory (GB): 4\n",
      "Epoch 3, Batch 224, Train Loss: 0.3022780418395996, Memory (GB): 4\n",
      "Epoch 3, Batch 225, Train Loss: 0.30161434412002563, Memory (GB): 4\n",
      "Epoch 3, Batch 226, Train Loss: 0.27675825357437134, Memory (GB): 4\n",
      "Epoch 3, Batch 227, Train Loss: 0.29083722829818726, Memory (GB): 4\n",
      "Epoch 3, Batch 228, Train Loss: 0.26893407106399536, Memory (GB): 4\n",
      "Epoch 3, Batch 229, Train Loss: 0.2687247693538666, Memory (GB): 4\n",
      "Epoch 3, Batch 230, Train Loss: 0.3038933575153351, Memory (GB): 4\n",
      "Epoch 3, Batch 231, Train Loss: 0.27010878920555115, Memory (GB): 4\n",
      "Epoch 3, Batch 232, Train Loss: 0.2775564193725586, Memory (GB): 4\n",
      "Epoch 3, Batch 233, Train Loss: 0.28367558121681213, Memory (GB): 4\n",
      "Epoch 3, Batch 234, Train Loss: 0.2659620940685272, Memory (GB): 4\n",
      "Epoch 3, Batch 235, Train Loss: 0.33893728256225586, Memory (GB): 4\n",
      "Epoch 3, Batch 236, Train Loss: 0.2954677939414978, Memory (GB): 4\n",
      "Epoch 3, Batch 237, Train Loss: 0.2659469544887543, Memory (GB): 4\n",
      "Epoch 3, Batch 238, Train Loss: 0.23352575302124023, Memory (GB): 4\n",
      "Epoch 3, Batch 239, Train Loss: 0.31191807985305786, Memory (GB): 4\n",
      "Epoch 3, Batch 240, Train Loss: 0.27868351340293884, Memory (GB): 4\n",
      "Epoch 3, Batch 241, Train Loss: 0.2926677167415619, Memory (GB): 4\n",
      "Epoch 3, Batch 242, Train Loss: 0.2850387990474701, Memory (GB): 4\n",
      "Epoch 3, Batch 243, Train Loss: 0.29279083013534546, Memory (GB): 4\n",
      "Epoch 3, Batch 244, Train Loss: 0.282457172870636, Memory (GB): 4\n",
      "Epoch 3, Batch 245, Train Loss: 0.2805423140525818, Memory (GB): 4\n",
      "Epoch 3, Batch 246, Train Loss: 0.24928423762321472, Memory (GB): 4\n",
      "Epoch 3, Batch 247, Train Loss: 0.2623552680015564, Memory (GB): 4\n",
      "Epoch 3, Batch 248, Train Loss: 0.3263265788555145, Memory (GB): 4\n",
      "Epoch 3, Batch 249, Train Loss: 0.3102172017097473, Memory (GB): 4\n",
      "Epoch 3, Batch 250, Train Loss: 0.2936442494392395, Memory (GB): 4\n",
      "Epoch 3, Batch 251, Train Loss: 0.2589073181152344, Memory (GB): 4\n",
      "Epoch 3, Batch 252, Train Loss: 0.26451224088668823, Memory (GB): 4\n",
      "Epoch 3, Batch 253, Train Loss: 0.27586501836776733, Memory (GB): 4\n",
      "Epoch 3, Batch 254, Train Loss: 0.3186866343021393, Memory (GB): 4\n",
      "Epoch 3, Batch 255, Train Loss: 0.33242204785346985, Memory (GB): 4\n",
      "Epoch 3, Batch 256, Train Loss: 0.25977930426597595, Memory (GB): 4\n",
      "Epoch 3, Batch 257, Train Loss: 0.3171358108520508, Memory (GB): 4\n",
      "Epoch 3, Batch 258, Train Loss: 0.2674359381198883, Memory (GB): 4\n",
      "Epoch 3, Batch 259, Train Loss: 0.2875705063343048, Memory (GB): 4\n",
      "Epoch 3, Batch 260, Train Loss: 0.21409828960895538, Memory (GB): 4\n",
      "Epoch 3, Batch 261, Train Loss: 0.3172951638698578, Memory (GB): 4\n",
      "Epoch 3, Batch 262, Train Loss: 0.2592402398586273, Memory (GB): 4\n",
      "Epoch 3, Batch 263, Train Loss: 0.331158310174942, Memory (GB): 4\n",
      "Epoch 3, Batch 264, Train Loss: 0.2571567893028259, Memory (GB): 4\n",
      "Epoch 3, Batch 265, Train Loss: 0.248106449842453, Memory (GB): 4\n",
      "Epoch 3, Batch 266, Train Loss: 0.2659759223461151, Memory (GB): 4\n",
      "Epoch 3, Batch 267, Train Loss: 0.2554677426815033, Memory (GB): 4\n",
      "Epoch 3, Batch 268, Train Loss: 0.23994968831539154, Memory (GB): 4\n",
      "Epoch 3, Batch 269, Train Loss: 0.2898021638393402, Memory (GB): 4\n",
      "[0.0003967506122430222, 0.0003769130816308713, 0.0003580674275493273, 0.000340164056171861, 0.00032315585336326785, 0.0003069980606951044, 0.00029164815766034944, 0.00027706574977733175, 0.00026321246228846535, 0.00025005183917404183, 0.0002375492472153401, 0.00022567178485457295, 0.00021438819561184425, 0.00020366878583125226, 0.00019348534653968927, 0.00018381107921270506, 0.00017462052525206963, 0.0001658894989894661, 0.00015759502403999296, 0.00014971527283799316, 0.00014222950919609348, 0.00013511803373628884, 0.00012836213204947438, 0.00012194402544700061, 0.00011584682417465057, 0.00011005448296591814, 0.00010455175881762222, 9.932417087674109e-05, 9.435796233290403e-05, 8.964006421625882e-05, 8.51580610054459e-05, 8.090015795517361e-05, 7.685515005741487e-05, 7.301239255454413e-05, 6.936177292681691e-05, 6.589368428047603e-05, 6.259900006645228e-05, 5.946905006312966e-05, 5.6495597559973156e-05, 5.3670817681974464e-05, 5.098727679787575e-05, 4.843791295798196e-05, 4.601601731008285e-05, 4.371521644457873e-05, 4.1529455622349794e-05, 3.945298284123232e-05, 3.748033369917066e-05, 3.5606317014212154e-05, 3.382600116350153e-05, 3.213470110532646e-05, 3.052796605006014e-05, 2.9001567747557117e-05, 2.755148936017928e-05, 2.6173914892170298e-05, 2.4865219147561785e-05, 2.362195819018369e-05, 2.2440860280674522e-05, 2.1318817266640778e-05, 2.025287640330876e-05, 1.9240232583143312e-05, 1.8278220953986156e-05, 1.736430990628684e-05, 1.6496094410972485e-05, 1.567128969042386e-05, 1.4887725205902676e-05, 1.4143338945607526e-05, 1.3436171998327158e-05, 1.2764363398410795e-05, 1.2126145228490245e-05, 1.1519837967065741e-05, 1.0943846068712455e-05, 1.0396653765276829e-05, 9.876821077012999e-06, 9.382980023162335e-06, 8.913831022004216e-06, 8.468139470904004e-06, 8.044732497358807e-06, 7.642495872490868e-06, 7.2603710788663295e-06, 6.897352524923002e-06, 6.552484898676859e-06, 6.2248606537430145e-06, 5.913617621055864e-06, 5.617936740003068e-06, 5.337039903002914e-06, 5.070187907852778e-06, 4.816678512460131e-06, 4.5758445868371295e-06, 4.347052357495268e-06, 4.1296997396205105e-06, 3.923214752639483e-06, 3.7270540150075043e-06, 3.5407013142571297e-06, 3.3636662485442738e-06, 3.195482936117063e-06, 3.0357087893112075e-06, 2.883923349845645e-06, 2.7397271823533634e-06, 2.6027408232356967e-06, 2.4726037820739115e-06, 2.3489735929702163e-06, 2.2315249133217063e-06, 2.1199486676556194e-06, 2.0139512342728382e-06, 1.913253672559198e-06, 1.8175909889312368e-06, 1.726711439484674e-06, 1.6403758675104388e-06, 1.5583570741349191e-06, 1.4804392204281716e-06, 1.4064172594067639e-06, 1.3360963964364255e-06, 1.2692915766146041e-06, 1.205826997783874e-06, 1.1455356478946795e-06, 1.0882588654999466e-06, 1.0338459222249488e-06, 9.821536261137017e-07, 9.330459448080167e-07, 8.863936475676152e-07, 8.420739651892354e-07, 7.999702669297725e-07, 7.599717535832835e-07, 7.219731659041201e-07, 6.858745076089138e-07, 6.515807822284684e-07, 6.190017431170447e-07, 5.880516559611928e-07, 5.586490731631331e-07, 5.307166195049765e-07, 5.041807885297279e-07, 4.789717491032408e-07, 4.5502316164807887e-07, 4.3227200356567476e-07, 4.1065840338739166e-07, 3.9012548321802134e-07, 3.706192090571207e-07, 3.5208824860426433e-07, 3.3448383617405135e-07, 3.177596443653483e-07, 3.0187166214708095e-07, 2.867780790397272e-07, 2.7243917508774086e-07, 2.588172163333535e-07, 2.4587635551668596e-07, 2.335825377408519e-07, 2.2190341085380895e-07, 2.1080824031111858e-07, 2.0026782829556274e-07, 1.9025443688078461e-07, 1.807417150367453e-07, 1.71704629284908e-07, 1.631193978206626e-07, 1.549634279296294e-07, 1.4721525653314807e-07, 1.398544937064905e-07, 1.3286176902116605e-07, 1.262186805701078e-07, 1.199077465416025e-07, 1.1391235921452222e-07, 1.0821674125379617e-07, 1.0280590419110631e-07, 9.766560898155097e-08, 9.278232853247338e-08, 8.814321210584979e-08, 8.373605150055729e-08, 7.954924892552936e-08, 7.557178647925287e-08, 7.179319715529032e-08, 6.820353729752574e-08, 6.479336043264947e-08, 6.155369241101702e-08, 5.8476007790466094e-08, 5.555220740094282e-08, 5.277459703089566e-08, 5.0135867179350896e-08, 4.762907382038338e-08, 4.524762012936418e-08, 4.298523912289598e-08, 4.0835977166751166e-08, 3.8794178308413604e-08, 3.685446939299291e-08, 3.501174592334327e-08, 3.326115862717611e-08, 3.15981006958173e-08, 3.0018195661026435e-08, 2.8517285877975132e-08, 2.7091421584076346e-08, 2.573685050487253e-08, 2.4450007979628894e-08, 2.322750758064744e-08, 2.206613220161508e-08, 2.0962825591534337e-08, 1.99146843119576e-08, 1.8918950096359735e-08, 1.797300259154173e-08, 1.7074352461964653e-08, 1.6220634838866427e-08, 1.540960309692309e-08]\n",
      "Epoch 4, Batch 0, Train Loss: 0.29989486932754517, Memory (GB): 4\n",
      "Epoch 4, Batch 1, Train Loss: 0.29239481687545776, Memory (GB): 4\n",
      "Epoch 4, Batch 2, Train Loss: 0.29934802651405334, Memory (GB): 4\n",
      "Epoch 4, Batch 3, Train Loss: 0.2686334252357483, Memory (GB): 4\n",
      "Epoch 4, Batch 4, Train Loss: 0.27963846921920776, Memory (GB): 4\n",
      "Epoch 4, Batch 5, Train Loss: 0.2935655415058136, Memory (GB): 4\n",
      "Epoch 4, Batch 6, Train Loss: 0.25050246715545654, Memory (GB): 4\n",
      "Epoch 4, Batch 7, Train Loss: 0.29892855882644653, Memory (GB): 4\n",
      "Epoch 4, Batch 8, Train Loss: 0.2509564459323883, Memory (GB): 4\n",
      "Epoch 4, Batch 9, Train Loss: 0.31156492233276367, Memory (GB): 4\n",
      "Epoch 4, Batch 10, Train Loss: 0.2649787664413452, Memory (GB): 4\n",
      "Epoch 4, Batch 11, Train Loss: 0.27558499574661255, Memory (GB): 4\n",
      "Epoch 4, Batch 12, Train Loss: 0.2609885334968567, Memory (GB): 4\n",
      "Epoch 4, Batch 13, Train Loss: 0.2773471474647522, Memory (GB): 4\n",
      "Epoch 4, Batch 14, Train Loss: 0.29984813928604126, Memory (GB): 4\n",
      "Epoch 4, Batch 15, Train Loss: 0.2699379622936249, Memory (GB): 4\n",
      "Epoch 4, Batch 16, Train Loss: 0.29154258966445923, Memory (GB): 4\n",
      "Epoch 4, Batch 17, Train Loss: 0.27201852202415466, Memory (GB): 4\n",
      "Epoch 4, Batch 18, Train Loss: 0.20741140842437744, Memory (GB): 4\n",
      "Epoch 4, Batch 19, Train Loss: 0.21818263828754425, Memory (GB): 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#gc.collect()\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 71\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(model, train, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#scaler.scale(loss).backward()\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#scaler.step(optimizer)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#scaler.update()\u001b[39;00m\n\u001b[0;32m     70\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 71\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warmup_scheduler\u001b[38;5;241m.\u001b[39mdampening():\n\u001b[0;32m     74\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py:531\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    528\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_params, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[0;32m    534\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_addcmul_(\n\u001b[0;32m    535\u001b[0m     device_exp_avg_sqs, device_grads, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2\n\u001b[0;32m    536\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "#torch.cuda.empty_cache()\n",
    "#gc.collect()\n",
    "fine_tune(model, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c7613e-a346-4eca-b0e7-adf66f470ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.save(model.state_dict(), \"temp_model_parameters3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c93fe62-c545-4deb-b306-4ff2df7fbc88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5007503751875938e-06, 1.4257128564282141e-06, 1.3544272136068034e-06, 1.2867058529264632e-06, 1.22237056028014e-06, 1.1612520322661327e-06, 1.103189430652826e-06, 1.0480299591201848e-06, 9.956284611641754e-07, 9.458470381059664e-07, 8.98554686200668e-07, 8.536269518906346e-07, 8.109456042961028e-07, 7.703983240812976e-07, 7.318784078772328e-07, 6.952844874833711e-07, 6.605202631092025e-07, 6.274942499537424e-07, 5.961195374560553e-07, 5.663135605832524e-07, 5.379978825540898e-07, 5.110979884263852e-07, 4.85543089005066e-07, 4.612659345548126e-07, 4.382026378270719e-07, 4.162925059357183e-07, 3.9547788063893234e-07, 3.757039866069857e-07, 3.569187872766364e-07, 3.3907284791280456e-07, 3.2211920551716433e-07, 3.060132452413061e-07, 2.907125829792408e-07, 2.7617695383027873e-07, 2.6236810613876475e-07, 2.492497008318265e-07, 2.3678721579023518e-07, 2.249478550007234e-07, 2.137004622506872e-07, 2.0301543913815282e-07, 1.9286466718124518e-07, 1.832214338221829e-07, 1.7406036213107374e-07, 1.6535734402452004e-07, 1.5708947682329405e-07, 1.4923500298212934e-07, 1.4177325283302286e-07, 1.346845901913717e-07, 1.2795036068180312e-07, 1.2155284264771295e-07, 1.154752005153273e-07, 1.0970144048956095e-07, 1.0421636846508289e-07, 9.900555004182874e-08, 9.405527253973731e-08, 8.935250891275043e-08, 8.488488346711291e-08, 8.064063929375725e-08, 7.660860732906939e-08, 7.277817696261591e-08, 6.91392681144851e-08, 6.568230470876085e-08, 6.23981894733228e-08, 5.9278279999656654e-08, 5.631436599967382e-08, 5.349864769969013e-08, 5.082371531470561e-08, 4.828252954897033e-08, 4.586840307152181e-08, 4.357498291794572e-08, 4.139623377204843e-08, 3.9326422083446006e-08, 3.736010097927371e-08, 3.549209593031002e-08, 3.3717491133794516e-08, 3.2031616577104784e-08, 3.0430035748249546e-08, 2.8908533960837066e-08, 2.7463107262795215e-08, 2.6089951899655453e-08, 2.4785454304672685e-08, 2.3546181589439047e-08, 2.2368872509967094e-08, 2.125042888446874e-08, 2.01879074402453e-08, 1.9178512068233036e-08, 1.8219586464821384e-08, 1.7308607141580313e-08, 1.64431767845013e-08, 1.5621017945276233e-08, 1.4839967048012418e-08, 1.4097968695611797e-08, 1.3393070260831207e-08, 1.2723416747789647e-08, 1.2087245910400165e-08, 1.1482883614880156e-08, 1.0908739434136149e-08, 1.0363302462429341e-08, 9.845137339307873e-09, 9.35288047234248e-09, 8.885236448725354e-09, 8.440974626289086e-09, 8.018925894974633e-09, 7.6179796002259e-09, 7.237080620214606e-09, 6.875226589203875e-09, 6.531465259743681e-09, 6.204891996756496e-09, 5.894647396918671e-09, 5.599915027072737e-09, 5.3199192757191e-09, 5.053923311933145e-09, 4.801227146336487e-09, 4.561165789019663e-09, 4.333107499568679e-09, 4.116452124590245e-09, 3.910629518360733e-09, 3.7150980424426963e-09, 3.529343140320561e-09, 3.3528759833045328e-09, 3.185232184139306e-09, 3.02597057493234e-09, 2.8746720461857236e-09, 2.730938443876437e-09, 2.5943915216826147e-09, 2.464671945598484e-09, 2.34143834831856e-09, 2.2243664309026317e-09, 2.1131481093575e-09, 2.0074907038896252e-09, 1.9071161686951437e-09, 1.8117603602603864e-09, 1.7211723422473673e-09, 1.6351137251349989e-09, 1.5533580388782488e-09, 1.4756901369343362e-09, 1.4019056300876193e-09, 1.3318103485832382e-09, 1.2652198311540764e-09, 1.2019588395963725e-09, 1.1418608976165538e-09, 1.084767852735726e-09, 1.0305294600989397e-09, 9.790029870939926e-10, 9.30052837739293e-10, 8.835501958523283e-10, 8.393726860597119e-10, 7.974040517567262e-10, 7.575338491688897e-10, 7.196571567104452e-10, 6.836742988749229e-10, 6.494905839311768e-10, 6.170160547346179e-10, 5.86165251997887e-10, 5.568569893979926e-10, 5.290141399280929e-10, 5.025634329316883e-10, 4.774352612851038e-10, 4.535634982208486e-10, 4.3088532330980614e-10, 4.093410571443158e-10, 3.8887400428709995e-10, 3.6943030407274496e-10, 3.5095878886910765e-10, 3.3341084942565226e-10, 3.1674030695436965e-10, 3.009032916066511e-10, 2.8585812702631855e-10, 2.715652206750026e-10, 2.5798695964125245e-10, 2.450876116591898e-10, 2.3283323107623035e-10, 2.2119156952241885e-10, 2.101319910462979e-10, 1.9962539149398298e-10, 1.8964412191928384e-10, 1.8016191582331965e-10, 1.7115382003215365e-10, 1.6259612903054596e-10, 1.5446632257901867e-10, 1.4674300645006772e-10, 1.3940585612756433e-10, 1.324355633211861e-10, 1.258137851551268e-10, 1.1952309589737046e-10, 1.135469411025019e-10, 1.078695940473768e-10, 1.0247611434500796e-10, 9.735230862775756e-11, 9.248469319636967e-11, 8.786045853655117e-11, 8.346743560972361e-11, 7.929406382923743e-11, 7.532936063777555e-11, 7.156289260588677e-11, 6.798474797559243e-11, 6.458551057681282e-11, 6.135623504797217e-11, 5.828842329557355e-11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebei\\AppData\\Local\\Temp\\ipykernel_29764\\855433317.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 0, Train Loss: 0.2641275227069855, Memory (GB): 7\n",
      "Epoch 5, Batch 1, Train Loss: 0.2763296961784363, Memory (GB): 7\n",
      "Epoch 5, Batch 2, Train Loss: 0.3260619640350342, Memory (GB): 7\n",
      "Epoch 5, Batch 3, Train Loss: 0.29032576084136963, Memory (GB): 7\n",
      "Epoch 5, Batch 4, Train Loss: 0.2693054676055908, Memory (GB): 7\n",
      "Epoch 5, Batch 5, Train Loss: 0.27935194969177246, Memory (GB): 7\n",
      "Epoch 5, Batch 6, Train Loss: 0.23872263729572296, Memory (GB): 7\n",
      "Epoch 5, Batch 7, Train Loss: 0.2677673101425171, Memory (GB): 7\n",
      "Epoch 5, Batch 8, Train Loss: 0.2736033499240875, Memory (GB): 7\n",
      "Epoch 5, Batch 9, Train Loss: 0.2807002365589142, Memory (GB): 7\n",
      "Epoch 5, Batch 10, Train Loss: 0.29847589135169983, Memory (GB): 7\n",
      "Epoch 5, Batch 11, Train Loss: 0.25968363881111145, Memory (GB): 7\n",
      "Epoch 5, Batch 12, Train Loss: 0.274177610874176, Memory (GB): 7\n",
      "Epoch 5, Batch 13, Train Loss: 0.31476983428001404, Memory (GB): 7\n",
      "Epoch 5, Batch 14, Train Loss: 0.2956920266151428, Memory (GB): 7\n",
      "Epoch 5, Batch 15, Train Loss: 0.26089397072792053, Memory (GB): 7\n",
      "Epoch 5, Batch 16, Train Loss: 0.280227929353714, Memory (GB): 7\n",
      "Epoch 5, Batch 17, Train Loss: 0.23723676800727844, Memory (GB): 7\n",
      "Epoch 5, Batch 18, Train Loss: 0.25978994369506836, Memory (GB): 7\n",
      "Epoch 5, Batch 19, Train Loss: 0.2749997079372406, Memory (GB): 7\n",
      "Epoch 5, Batch 20, Train Loss: 0.2870221734046936, Memory (GB): 7\n",
      "Epoch 5, Batch 21, Train Loss: 0.2929404079914093, Memory (GB): 7\n",
      "Epoch 5, Batch 22, Train Loss: 0.24518099427223206, Memory (GB): 7\n",
      "Epoch 5, Batch 23, Train Loss: 0.2601800560951233, Memory (GB): 7\n",
      "Epoch 5, Batch 24, Train Loss: 0.27560660243034363, Memory (GB): 7\n",
      "Epoch 5, Batch 25, Train Loss: 0.28915169835090637, Memory (GB): 7\n",
      "Epoch 5, Batch 26, Train Loss: 0.263739675283432, Memory (GB): 7\n",
      "Epoch 5, Batch 27, Train Loss: 0.2582414150238037, Memory (GB): 7\n",
      "Epoch 5, Batch 28, Train Loss: 0.27557554841041565, Memory (GB): 7\n",
      "Epoch 5, Batch 29, Train Loss: 0.2723088264465332, Memory (GB): 7\n",
      "Epoch 5, Batch 30, Train Loss: 0.2710456848144531, Memory (GB): 7\n",
      "Epoch 5, Batch 31, Train Loss: 0.2846198081970215, Memory (GB): 7\n",
      "Epoch 5, Batch 32, Train Loss: 0.2501458525657654, Memory (GB): 7\n",
      "Epoch 5, Batch 33, Train Loss: 0.26888176798820496, Memory (GB): 7\n",
      "Epoch 5, Batch 34, Train Loss: 0.2826524078845978, Memory (GB): 7\n",
      "Epoch 5, Batch 35, Train Loss: 0.23763029277324677, Memory (GB): 7\n",
      "Epoch 5, Batch 36, Train Loss: 0.2919071614742279, Memory (GB): 7\n",
      "Epoch 5, Batch 37, Train Loss: 0.2739337682723999, Memory (GB): 7\n",
      "Epoch 5, Batch 38, Train Loss: 0.2579851448535919, Memory (GB): 7\n",
      "Epoch 5, Batch 39, Train Loss: 0.2925335466861725, Memory (GB): 7\n",
      "Epoch 5, Batch 40, Train Loss: 0.24193759262561798, Memory (GB): 7\n",
      "Epoch 5, Batch 41, Train Loss: 0.2758764326572418, Memory (GB): 7\n",
      "Epoch 5, Batch 42, Train Loss: 0.2913024425506592, Memory (GB): 7\n",
      "Epoch 5, Batch 43, Train Loss: 0.2900519073009491, Memory (GB): 7\n",
      "Epoch 5, Batch 44, Train Loss: 0.27694961428642273, Memory (GB): 7\n",
      "Epoch 5, Batch 45, Train Loss: 0.29089999198913574, Memory (GB): 7\n",
      "Epoch 5, Batch 46, Train Loss: 0.2694022059440613, Memory (GB): 7\n",
      "Epoch 5, Batch 47, Train Loss: 0.2787138521671295, Memory (GB): 7\n",
      "Epoch 5, Batch 48, Train Loss: 0.2545837163925171, Memory (GB): 7\n",
      "Epoch 5, Batch 49, Train Loss: 0.25189995765686035, Memory (GB): 7\n",
      "Epoch 5, Batch 50, Train Loss: 0.2687366008758545, Memory (GB): 7\n",
      "Epoch 5, Batch 51, Train Loss: 0.274116188287735, Memory (GB): 7\n",
      "Epoch 5, Batch 52, Train Loss: 0.2621377408504486, Memory (GB): 7\n",
      "Epoch 5, Batch 53, Train Loss: 0.2598177194595337, Memory (GB): 7\n",
      "Epoch 5, Batch 54, Train Loss: 0.23105672001838684, Memory (GB): 7\n",
      "Epoch 5, Batch 55, Train Loss: 0.25359225273132324, Memory (GB): 7\n",
      "Epoch 5, Batch 56, Train Loss: 0.2539657950401306, Memory (GB): 7\n",
      "Epoch 5, Batch 57, Train Loss: 0.2510879933834076, Memory (GB): 7\n",
      "Epoch 5, Batch 58, Train Loss: 0.24381282925605774, Memory (GB): 7\n",
      "Epoch 5, Batch 59, Train Loss: 0.29665401577949524, Memory (GB): 7\n",
      "Epoch 5, Batch 60, Train Loss: 0.24774578213691711, Memory (GB): 7\n",
      "Epoch 5, Batch 61, Train Loss: 0.28698402643203735, Memory (GB): 7\n",
      "Epoch 5, Batch 62, Train Loss: 0.27708688378334045, Memory (GB): 7\n",
      "Epoch 5, Batch 63, Train Loss: 0.2755134701728821, Memory (GB): 7\n",
      "Epoch 5, Batch 64, Train Loss: 0.2520120143890381, Memory (GB): 7\n",
      "Epoch 5, Batch 65, Train Loss: 0.2965034246444702, Memory (GB): 7\n",
      "Epoch 5, Batch 66, Train Loss: 0.29989320039749146, Memory (GB): 7\n",
      "Epoch 5, Batch 67, Train Loss: 0.27197006344795227, Memory (GB): 7\n",
      "Epoch 5, Batch 68, Train Loss: 0.28232342004776, Memory (GB): 7\n",
      "Epoch 5, Batch 69, Train Loss: 0.30683499574661255, Memory (GB): 7\n",
      "Epoch 5, Batch 70, Train Loss: 0.28162723779678345, Memory (GB): 7\n",
      "Epoch 5, Batch 71, Train Loss: 0.27990975975990295, Memory (GB): 7\n",
      "Epoch 5, Batch 72, Train Loss: 0.24043019115924835, Memory (GB): 7\n",
      "Epoch 5, Batch 73, Train Loss: 0.24851910769939423, Memory (GB): 7\n",
      "Epoch 5, Batch 74, Train Loss: 0.298000305891037, Memory (GB): 7\n",
      "Epoch 5, Batch 75, Train Loss: 0.2580493986606598, Memory (GB): 7\n",
      "Epoch 5, Batch 76, Train Loss: 0.27295729517936707, Memory (GB): 7\n",
      "Epoch 5, Batch 77, Train Loss: 0.26097097992897034, Memory (GB): 7\n",
      "Epoch 5, Batch 78, Train Loss: 0.2965422570705414, Memory (GB): 7\n",
      "Epoch 5, Batch 79, Train Loss: 0.2853568494319916, Memory (GB): 7\n",
      "Epoch 5, Batch 80, Train Loss: 0.25939351320266724, Memory (GB): 7\n",
      "Epoch 5, Batch 81, Train Loss: 0.25995296239852905, Memory (GB): 7\n",
      "Epoch 5, Batch 82, Train Loss: 0.31135615706443787, Memory (GB): 7\n",
      "Epoch 5, Batch 83, Train Loss: 0.2509938180446625, Memory (GB): 7\n",
      "Epoch 5, Batch 84, Train Loss: 0.2545218765735626, Memory (GB): 7\n",
      "Epoch 5, Batch 85, Train Loss: 0.2394266575574875, Memory (GB): 7\n",
      "Epoch 5, Batch 86, Train Loss: 0.2720484733581543, Memory (GB): 7\n",
      "Epoch 5, Batch 87, Train Loss: 0.27309685945510864, Memory (GB): 7\n",
      "Epoch 5, Batch 88, Train Loss: 0.28940773010253906, Memory (GB): 7\n",
      "Epoch 5, Batch 89, Train Loss: 0.2453298270702362, Memory (GB): 7\n",
      "Epoch 5, Batch 90, Train Loss: 0.2756146788597107, Memory (GB): 7\n",
      "Epoch 5, Batch 91, Train Loss: 0.27660250663757324, Memory (GB): 7\n",
      "Epoch 5, Batch 92, Train Loss: 0.26841720938682556, Memory (GB): 7\n",
      "Epoch 5, Batch 93, Train Loss: 0.24753344058990479, Memory (GB): 7\n",
      "Epoch 5, Batch 94, Train Loss: 0.2717720866203308, Memory (GB): 7\n",
      "Epoch 5, Batch 95, Train Loss: 0.2790939211845398, Memory (GB): 7\n",
      "Epoch 5, Batch 96, Train Loss: 0.2581349313259125, Memory (GB): 7\n",
      "Epoch 5, Batch 97, Train Loss: 0.25096234679222107, Memory (GB): 7\n",
      "Epoch 5, Batch 98, Train Loss: 0.2839754819869995, Memory (GB): 7\n",
      "Epoch 5, Batch 99, Train Loss: 0.2697448134422302, Memory (GB): 7\n",
      "Epoch 5, Batch 100, Train Loss: 0.2920587360858917, Memory (GB): 7\n",
      "Epoch 5, Batch 101, Train Loss: 0.2934398651123047, Memory (GB): 7\n",
      "Epoch 5, Batch 102, Train Loss: 0.2477700412273407, Memory (GB): 7\n",
      "Epoch 5, Batch 103, Train Loss: 0.2579026222229004, Memory (GB): 7\n",
      "Epoch 5, Batch 104, Train Loss: 0.2765752375125885, Memory (GB): 7\n",
      "Epoch 5, Batch 105, Train Loss: 0.26311370730400085, Memory (GB): 7\n",
      "Epoch 5, Batch 106, Train Loss: 0.2960275113582611, Memory (GB): 7\n",
      "Epoch 5, Batch 107, Train Loss: 0.2657392919063568, Memory (GB): 7\n",
      "Epoch 5, Batch 108, Train Loss: 0.2650819718837738, Memory (GB): 7\n",
      "Epoch 5, Batch 109, Train Loss: 0.2686249017715454, Memory (GB): 7\n",
      "Epoch 5, Batch 110, Train Loss: 0.2748037874698639, Memory (GB): 7\n",
      "Epoch 5, Batch 111, Train Loss: 0.2531238794326782, Memory (GB): 7\n",
      "Epoch 5, Batch 112, Train Loss: 0.2846141755580902, Memory (GB): 7\n",
      "Epoch 5, Batch 113, Train Loss: 0.224808007478714, Memory (GB): 7\n",
      "Epoch 5, Batch 114, Train Loss: 0.27623677253723145, Memory (GB): 7\n",
      "Epoch 5, Batch 115, Train Loss: 0.2582909166812897, Memory (GB): 7\n",
      "Epoch 5, Batch 116, Train Loss: 0.22761990129947662, Memory (GB): 7\n",
      "Epoch 5, Batch 117, Train Loss: 0.2909376621246338, Memory (GB): 7\n",
      "Epoch 5, Batch 118, Train Loss: 0.2838287055492401, Memory (GB): 7\n",
      "Epoch 5, Batch 119, Train Loss: 0.2699025571346283, Memory (GB): 7\n",
      "Epoch 5, Batch 120, Train Loss: 0.2888409197330475, Memory (GB): 7\n",
      "Epoch 5, Batch 121, Train Loss: 0.23229803144931793, Memory (GB): 7\n",
      "Epoch 5, Batch 122, Train Loss: 0.23437421023845673, Memory (GB): 7\n",
      "Epoch 5, Batch 123, Train Loss: 0.2820967137813568, Memory (GB): 7\n",
      "Epoch 5, Batch 124, Train Loss: 0.27123063802719116, Memory (GB): 7\n",
      "Epoch 5, Batch 125, Train Loss: 0.2820555865764618, Memory (GB): 7\n",
      "Epoch 5, Batch 126, Train Loss: 0.27452605962753296, Memory (GB): 7\n",
      "Epoch 5, Batch 127, Train Loss: 0.27891361713409424, Memory (GB): 7\n",
      "Epoch 5, Batch 128, Train Loss: 0.2588503658771515, Memory (GB): 7\n",
      "Epoch 5, Batch 129, Train Loss: 0.26123282313346863, Memory (GB): 7\n",
      "Epoch 5, Batch 130, Train Loss: 0.268370658159256, Memory (GB): 7\n",
      "Epoch 5, Batch 131, Train Loss: 0.25000885128974915, Memory (GB): 7\n",
      "Epoch 5, Batch 132, Train Loss: 0.2983585000038147, Memory (GB): 7\n",
      "Epoch 5, Batch 133, Train Loss: 0.2519876956939697, Memory (GB): 7\n",
      "Epoch 5, Batch 134, Train Loss: 0.2708255350589752, Memory (GB): 7\n",
      "Epoch 5, Batch 135, Train Loss: 0.2892796993255615, Memory (GB): 7\n",
      "Epoch 5, Batch 136, Train Loss: 0.2508959174156189, Memory (GB): 7\n",
      "Epoch 5, Batch 137, Train Loss: 0.25118809938430786, Memory (GB): 7\n",
      "Epoch 5, Batch 138, Train Loss: 0.2839697301387787, Memory (GB): 7\n",
      "Epoch 5, Batch 139, Train Loss: 0.29643014073371887, Memory (GB): 7\n",
      "Epoch 5, Batch 140, Train Loss: 0.25949588418006897, Memory (GB): 7\n",
      "Epoch 5, Batch 141, Train Loss: 0.2540753185749054, Memory (GB): 7\n",
      "Epoch 5, Batch 142, Train Loss: 0.28759124875068665, Memory (GB): 7\n",
      "Epoch 5, Batch 143, Train Loss: 0.2649996280670166, Memory (GB): 7\n",
      "Epoch 5, Batch 144, Train Loss: 0.27397632598876953, Memory (GB): 7\n",
      "Epoch 5, Batch 145, Train Loss: 0.2399371862411499, Memory (GB): 7\n",
      "Epoch 5, Batch 146, Train Loss: 0.24529188871383667, Memory (GB): 7\n",
      "Epoch 5, Batch 147, Train Loss: 0.24786758422851562, Memory (GB): 7\n",
      "Epoch 5, Batch 148, Train Loss: 0.2629706859588623, Memory (GB): 7\n",
      "Epoch 5, Batch 149, Train Loss: 0.27363911271095276, Memory (GB): 7\n",
      "Epoch 5, Batch 150, Train Loss: 0.27122437953948975, Memory (GB): 7\n",
      "Epoch 5, Batch 151, Train Loss: 0.2703600525856018, Memory (GB): 7\n",
      "Epoch 5, Batch 152, Train Loss: 0.28260934352874756, Memory (GB): 7\n",
      "Epoch 5, Batch 153, Train Loss: 0.24101407825946808, Memory (GB): 7\n",
      "Epoch 5, Batch 154, Train Loss: 0.2673376202583313, Memory (GB): 7\n",
      "Epoch 5, Batch 155, Train Loss: 0.2693309187889099, Memory (GB): 7\n",
      "Epoch 5, Batch 156, Train Loss: 0.25292277336120605, Memory (GB): 7\n",
      "Epoch 5, Batch 157, Train Loss: 0.288618803024292, Memory (GB): 7\n",
      "Epoch 5, Batch 158, Train Loss: 0.2589922845363617, Memory (GB): 7\n",
      "Epoch 5, Batch 159, Train Loss: 0.2675912082195282, Memory (GB): 7\n",
      "Epoch 5, Batch 160, Train Loss: 0.2481854110956192, Memory (GB): 7\n",
      "Epoch 5, Batch 161, Train Loss: 0.2836996614933014, Memory (GB): 7\n",
      "Epoch 5, Batch 162, Train Loss: 0.277641624212265, Memory (GB): 7\n",
      "Epoch 5, Batch 163, Train Loss: 0.2707560658454895, Memory (GB): 7\n",
      "Epoch 5, Batch 164, Train Loss: 0.230111762881279, Memory (GB): 7\n",
      "Epoch 5, Batch 165, Train Loss: 0.26042816042900085, Memory (GB): 7\n",
      "Epoch 5, Batch 166, Train Loss: 0.2754878103733063, Memory (GB): 7\n",
      "Epoch 5, Batch 167, Train Loss: 0.27850809693336487, Memory (GB): 7\n",
      "Epoch 5, Batch 168, Train Loss: 0.286271870136261, Memory (GB): 7\n",
      "Epoch 5, Batch 169, Train Loss: 0.2809663414955139, Memory (GB): 7\n",
      "Epoch 5, Batch 170, Train Loss: 0.26613178849220276, Memory (GB): 7\n",
      "Epoch 5, Batch 171, Train Loss: 0.2374652475118637, Memory (GB): 7\n",
      "Epoch 5, Batch 172, Train Loss: 0.27559635043144226, Memory (GB): 7\n",
      "Epoch 5, Batch 173, Train Loss: 0.24803967773914337, Memory (GB): 7\n",
      "Epoch 5, Batch 174, Train Loss: 0.2470427006483078, Memory (GB): 7\n",
      "Epoch 5, Batch 175, Train Loss: 0.3271198272705078, Memory (GB): 7\n",
      "Epoch 5, Batch 176, Train Loss: 0.25427356362342834, Memory (GB): 7\n",
      "Epoch 5, Batch 177, Train Loss: 0.2500913143157959, Memory (GB): 7\n",
      "Epoch 5, Batch 178, Train Loss: 0.265298455953598, Memory (GB): 7\n",
      "Epoch 5, Batch 179, Train Loss: 0.23384341597557068, Memory (GB): 7\n",
      "[5.991748623577172e-05, 5.69216119239831e-05, 5.407553132778397e-05, 5.1371754761394775e-05, 4.880316702332503e-05, 4.636300867215876e-05, 4.404485823855084e-05, 4.184261532662328e-05, 3.9750484560292136e-05, 3.776296033227752e-05, 3.587481231566362e-05, 3.4081071699880456e-05, 3.237701811488643e-05, 3.07581672091421e-05, 2.9220258848685008e-05, 2.7759245906250745e-05, 2.637128361093821e-05, 2.50527194303913e-05, 2.380008345887174e-05, 2.261007928592814e-05, 2.1479575321631734e-05, 2.040559655555015e-05, 1.9385316727772642e-05, 1.8416050891384e-05, 1.7495248346814804e-05, 1.662048592947405e-05, 1.5789461633000358e-05, 1.4999988551350335e-05, 1.4249989123782814e-05, 1.3537489667593678e-05, 1.2860615184213994e-05, 1.221758442500329e-05, 1.1606705203753127e-05, 1.102636994356547e-05, 1.0475051446387193e-05, 9.951298874067839e-06, 9.453733930364448e-06, 8.981047233846223e-06, 8.531994872153909e-06, 8.105395128546213e-06, 7.700125372118902e-06, 7.3151191035129555e-06, 6.949363148337309e-06, 6.60189499092044e-06, 6.2718002413744175e-06, 5.958210229305696e-06, 5.660299717840415e-06, 5.3772847319483905e-06, 5.108420495350974e-06, 4.852999470583424e-06, 4.610349497054253e-06, 4.379832022201539e-06, 4.1608404210914635e-06, 3.952798400036886e-06, 3.755158480035045e-06, 3.567400556033292e-06, 3.3890305282316265e-06, 3.2195790018200456e-06, 3.0586000517290443e-06, 2.905670049142592e-06, 2.7603865466854605e-06, 2.6223672193511886e-06, 2.4912488583836284e-06, 2.3666864154644465e-06, 2.2483520946912246e-06, 2.1359344899566625e-06, 2.0291377654588304e-06, 1.9276808771858888e-06, 1.831296833326593e-06, 1.7397319916602636e-06, 1.6527453920772507e-06, 1.5701081224733881e-06, 1.4916027163497178e-06, 1.4170225805322328e-06, 1.3461714515056209e-06, 1.2788628789303394e-06, 1.2149197349838224e-06, 1.1541737482346315e-06, 1.0964650608228992e-06, 1.0416418077817547e-06, 9.89559717392667e-07, 9.400817315230337e-07, 8.930776449468821e-07, 8.484237626995374e-07, 8.060025745645609e-07, 7.657024458363331e-07, 7.274173235445162e-07, 6.910464573672905e-07, 6.564941344989259e-07, 6.236694277739796e-07, 5.924859563852804e-07, 5.628616585660166e-07, 5.347185756377154e-07, 5.079826468558298e-07, 4.825835145130385e-07, 4.584543387873865e-07, 4.3553162184801724e-07, 4.137550407556164e-07, 3.930672887178354e-07, 3.734139242819436e-07, 3.5474322806784614e-07, 3.3700606666445404e-07, 3.201557633312313e-07, 3.041479751646699e-07, 2.889405764064363e-07, 2.744935475861145e-07, 2.6076887020680876e-07, 2.477304266964681e-07, 2.3534390536164473e-07, 2.2357671009356257e-07, 2.1239787458888442e-07, 2.0177798085944026e-07, 1.9168908181646818e-07, 1.8210462772564473e-07, 1.729993963393625e-07, 1.6434942652239438e-07, 1.5613195519627477e-07, 1.4832535743646088e-07, 1.4090908956463787e-07, 1.3386363508640595e-07, 1.2717045333208564e-07, 1.2081193066548136e-07, 1.1477133413220735e-07, 1.090327674255969e-07, 1.0358112905431704e-07, 9.840207260160123e-08, 9.348196897152119e-08, 8.880787052294508e-08, 8.436747699679786e-08, 8.014910314695795e-08, 7.614164798961004e-08, 7.233456559012956e-08, 6.871783731062306e-08, 6.528194544509193e-08, 6.20178481728373e-08, 5.891695576419547e-08, 5.597110797598564e-08, 5.3172552577186394e-08, 5.051392494832705e-08, 4.798822870091071e-08, 4.558881726586518e-08, 4.3309376402571905e-08, 4.114390758244331e-08, 3.908671220332115e-08, 3.713237659315508e-08, 3.5275757763497324e-08, 3.3511969875322454e-08, 3.183637138155634e-08, 3.024455281247851e-08, 2.8732325171854574e-08, 2.7295708913261852e-08, 2.5930923467598756e-08, 2.463437729421882e-08, 2.3402658429507876e-08, 2.223252550803249e-08, 2.112089923263087e-08, 2.0064854270999322e-08, 1.9061611557449337e-08, 1.8108530979576887e-08, 1.7203104430598035e-08, 1.6342949209068133e-08, 1.5525801748614726e-08, 1.4749511661183992e-08, 1.4012036078124787e-08, 1.3311434274218543e-08, 1.2645862560507617e-08, 1.2013569432482234e-08, 1.141289096085812e-08, 1.0842246412815218e-08, 1.0300134092174457e-08, 9.785127387565737e-09, 9.295871018187441e-09, 8.831077467278078e-09, 8.38952359391417e-09, 7.970047414218465e-09, 7.57154504350754e-09, 7.192967791332163e-09, 6.833319401765553e-09, 6.4916534316772735e-09, 6.1670707600934135e-09, 5.85871722208874e-09, 5.565781360984302e-09, 5.287492292935089e-09, 5.023117678288333e-09, 4.771961794373916e-09, 4.53336370465522e-09, 4.306695519422459e-09, 4.091360743451334e-09, 3.8867927062787695e-09, 3.6924530709648283e-09, 3.507830417416588e-09, 3.3324388965457576e-09, 3.16581695171847e-09, 3.0075261041325462e-09, 2.857149798925919e-09, 2.714292308979622e-09, 2.578577693530641e-09, 2.449648808854109e-09, 2.3271663684114035e-09]\n",
      "Epoch 6, Batch 0, Train Loss: 0.2991732656955719, Memory (GB): 7\n",
      "Epoch 6, Batch 1, Train Loss: 0.24069707095623016, Memory (GB): 7\n",
      "Epoch 6, Batch 2, Train Loss: 0.258268803358078, Memory (GB): 7\n",
      "Epoch 6, Batch 3, Train Loss: 0.2675851583480835, Memory (GB): 7\n",
      "Epoch 6, Batch 4, Train Loss: 0.23738014698028564, Memory (GB): 7\n",
      "Epoch 6, Batch 5, Train Loss: 0.2731982171535492, Memory (GB): 7\n",
      "Epoch 6, Batch 6, Train Loss: 0.2803916931152344, Memory (GB): 7\n",
      "Epoch 6, Batch 7, Train Loss: 0.2874297499656677, Memory (GB): 7\n",
      "Epoch 6, Batch 8, Train Loss: 0.2351926565170288, Memory (GB): 7\n",
      "Epoch 6, Batch 9, Train Loss: 0.2758956551551819, Memory (GB): 7\n",
      "Epoch 6, Batch 10, Train Loss: 0.24901194870471954, Memory (GB): 7\n",
      "Epoch 6, Batch 11, Train Loss: 0.2771345376968384, Memory (GB): 7\n",
      "Epoch 6, Batch 12, Train Loss: 0.25166404247283936, Memory (GB): 7\n",
      "Epoch 6, Batch 13, Train Loss: 0.25191551446914673, Memory (GB): 7\n",
      "Epoch 6, Batch 14, Train Loss: 0.2309253066778183, Memory (GB): 7\n",
      "Epoch 6, Batch 15, Train Loss: 0.2913155257701874, Memory (GB): 7\n",
      "Epoch 6, Batch 16, Train Loss: 0.2502562999725342, Memory (GB): 7\n",
      "Epoch 6, Batch 17, Train Loss: 0.2451520413160324, Memory (GB): 7\n",
      "Epoch 6, Batch 18, Train Loss: 0.2310742884874344, Memory (GB): 7\n",
      "Epoch 6, Batch 19, Train Loss: 0.2581792175769806, Memory (GB): 7\n",
      "Epoch 6, Batch 20, Train Loss: 0.3015945255756378, Memory (GB): 7\n",
      "Epoch 6, Batch 21, Train Loss: 0.2540057301521301, Memory (GB): 7\n",
      "Epoch 6, Batch 22, Train Loss: 0.25483155250549316, Memory (GB): 7\n",
      "Epoch 6, Batch 23, Train Loss: 0.24571868777275085, Memory (GB): 7\n",
      "Epoch 6, Batch 24, Train Loss: 0.28519320487976074, Memory (GB): 7\n",
      "Epoch 6, Batch 25, Train Loss: 0.2498520463705063, Memory (GB): 7\n",
      "Epoch 6, Batch 26, Train Loss: 0.2729990780353546, Memory (GB): 7\n",
      "Epoch 6, Batch 27, Train Loss: 0.255349338054657, Memory (GB): 7\n",
      "Epoch 6, Batch 28, Train Loss: 0.279168039560318, Memory (GB): 7\n",
      "Epoch 6, Batch 29, Train Loss: 0.2627212107181549, Memory (GB): 7\n",
      "Epoch 6, Batch 30, Train Loss: 0.2911089360713959, Memory (GB): 7\n",
      "Epoch 6, Batch 31, Train Loss: 0.24723947048187256, Memory (GB): 7\n",
      "Epoch 6, Batch 32, Train Loss: 0.2768923342227936, Memory (GB): 7\n",
      "Epoch 6, Batch 33, Train Loss: 0.2920638620853424, Memory (GB): 7\n",
      "Epoch 6, Batch 34, Train Loss: 0.25416430830955505, Memory (GB): 7\n",
      "Epoch 6, Batch 35, Train Loss: 0.262319952249527, Memory (GB): 7\n",
      "Epoch 6, Batch 36, Train Loss: 0.2803502678871155, Memory (GB): 7\n",
      "Epoch 6, Batch 37, Train Loss: 0.24154341220855713, Memory (GB): 7\n",
      "Epoch 6, Batch 38, Train Loss: 0.2602405548095703, Memory (GB): 7\n",
      "Epoch 6, Batch 39, Train Loss: 0.2610132098197937, Memory (GB): 7\n",
      "Epoch 6, Batch 40, Train Loss: 0.2330283671617508, Memory (GB): 7\n",
      "Epoch 6, Batch 41, Train Loss: 0.24775409698486328, Memory (GB): 7\n",
      "Epoch 6, Batch 42, Train Loss: 0.27070388197898865, Memory (GB): 7\n",
      "Epoch 6, Batch 43, Train Loss: 0.2783198654651642, Memory (GB): 7\n",
      "Epoch 6, Batch 44, Train Loss: 0.2746029496192932, Memory (GB): 7\n",
      "Epoch 6, Batch 45, Train Loss: 0.2971472442150116, Memory (GB): 7\n",
      "Epoch 6, Batch 46, Train Loss: 0.2553345263004303, Memory (GB): 7\n",
      "Epoch 6, Batch 47, Train Loss: 0.25497251749038696, Memory (GB): 7\n",
      "Epoch 6, Batch 48, Train Loss: 0.2566176950931549, Memory (GB): 7\n",
      "Epoch 6, Batch 49, Train Loss: 0.2598690688610077, Memory (GB): 7\n",
      "Epoch 6, Batch 50, Train Loss: 0.26950791478157043, Memory (GB): 7\n",
      "Epoch 6, Batch 51, Train Loss: 0.2635561227798462, Memory (GB): 7\n",
      "Epoch 6, Batch 52, Train Loss: 0.2855125069618225, Memory (GB): 7\n",
      "Epoch 6, Batch 53, Train Loss: 0.27571624517440796, Memory (GB): 7\n",
      "Epoch 6, Batch 54, Train Loss: 0.25582098960876465, Memory (GB): 7\n",
      "Epoch 6, Batch 55, Train Loss: 0.26501959562301636, Memory (GB): 7\n",
      "Epoch 6, Batch 56, Train Loss: 0.23394444584846497, Memory (GB): 7\n",
      "Epoch 6, Batch 57, Train Loss: 0.264980673789978, Memory (GB): 7\n",
      "Epoch 6, Batch 58, Train Loss: 0.2518119215965271, Memory (GB): 7\n",
      "Epoch 6, Batch 59, Train Loss: 0.298203706741333, Memory (GB): 7\n",
      "Epoch 6, Batch 60, Train Loss: 0.2918298840522766, Memory (GB): 7\n",
      "Epoch 6, Batch 61, Train Loss: 0.24697217345237732, Memory (GB): 7\n",
      "Epoch 6, Batch 62, Train Loss: 0.29957374930381775, Memory (GB): 7\n",
      "Epoch 6, Batch 63, Train Loss: 0.2536203861236572, Memory (GB): 7\n",
      "Epoch 6, Batch 64, Train Loss: 0.25616690516471863, Memory (GB): 7\n",
      "Epoch 6, Batch 65, Train Loss: 0.2256060093641281, Memory (GB): 7\n",
      "Epoch 6, Batch 66, Train Loss: 0.2833308279514313, Memory (GB): 7\n",
      "Epoch 6, Batch 67, Train Loss: 0.2699466645717621, Memory (GB): 7\n",
      "Epoch 6, Batch 68, Train Loss: 0.25958362221717834, Memory (GB): 7\n",
      "Epoch 6, Batch 69, Train Loss: 0.2763206958770752, Memory (GB): 7\n",
      "Epoch 6, Batch 70, Train Loss: 0.2467893809080124, Memory (GB): 7\n",
      "Epoch 6, Batch 71, Train Loss: 0.26894065737724304, Memory (GB): 7\n",
      "Epoch 6, Batch 72, Train Loss: 0.2632661461830139, Memory (GB): 7\n",
      "Epoch 6, Batch 73, Train Loss: 0.2713291645050049, Memory (GB): 7\n",
      "Epoch 6, Batch 74, Train Loss: 0.28680697083473206, Memory (GB): 7\n",
      "Epoch 6, Batch 75, Train Loss: 0.2392852008342743, Memory (GB): 7\n",
      "Epoch 6, Batch 76, Train Loss: 0.2515864968299866, Memory (GB): 7\n",
      "Epoch 6, Batch 77, Train Loss: 0.2891876995563507, Memory (GB): 7\n",
      "Epoch 6, Batch 78, Train Loss: 0.2478497326374054, Memory (GB): 7\n",
      "Epoch 6, Batch 79, Train Loss: 0.254243940114975, Memory (GB): 7\n",
      "Epoch 6, Batch 80, Train Loss: 0.24976488947868347, Memory (GB): 7\n",
      "Epoch 6, Batch 81, Train Loss: 0.30307936668395996, Memory (GB): 7\n",
      "Epoch 6, Batch 82, Train Loss: 0.2828983962535858, Memory (GB): 7\n",
      "Epoch 6, Batch 83, Train Loss: 0.30129948258399963, Memory (GB): 7\n",
      "Epoch 6, Batch 84, Train Loss: 0.267097145318985, Memory (GB): 7\n",
      "Epoch 6, Batch 85, Train Loss: 0.28949499130249023, Memory (GB): 7\n",
      "Epoch 6, Batch 86, Train Loss: 0.2616460621356964, Memory (GB): 7\n",
      "Epoch 6, Batch 87, Train Loss: 0.2750595808029175, Memory (GB): 7\n",
      "Epoch 6, Batch 88, Train Loss: 0.25577807426452637, Memory (GB): 7\n",
      "Epoch 6, Batch 89, Train Loss: 0.23687905073165894, Memory (GB): 7\n",
      "Epoch 6, Batch 90, Train Loss: 0.25958243012428284, Memory (GB): 7\n",
      "Epoch 6, Batch 91, Train Loss: 0.2445819228887558, Memory (GB): 7\n",
      "Epoch 6, Batch 92, Train Loss: 0.26157665252685547, Memory (GB): 7\n",
      "Epoch 6, Batch 93, Train Loss: 0.2810756266117096, Memory (GB): 7\n",
      "Epoch 6, Batch 94, Train Loss: 0.2476070523262024, Memory (GB): 7\n",
      "Epoch 6, Batch 95, Train Loss: 0.2834084630012512, Memory (GB): 7\n",
      "Epoch 6, Batch 96, Train Loss: 0.27115944027900696, Memory (GB): 7\n",
      "Epoch 6, Batch 97, Train Loss: 0.28865551948547363, Memory (GB): 7\n",
      "Epoch 6, Batch 98, Train Loss: 0.2781834602355957, Memory (GB): 7\n",
      "Epoch 6, Batch 99, Train Loss: 0.2601063549518585, Memory (GB): 7\n",
      "Epoch 6, Batch 100, Train Loss: 0.28581973910331726, Memory (GB): 7\n",
      "Epoch 6, Batch 101, Train Loss: 0.2595636546611786, Memory (GB): 7\n",
      "Epoch 6, Batch 102, Train Loss: 0.251808226108551, Memory (GB): 7\n",
      "Epoch 6, Batch 103, Train Loss: 0.2633628249168396, Memory (GB): 7\n",
      "Epoch 6, Batch 104, Train Loss: 0.2427305281162262, Memory (GB): 7\n",
      "Epoch 6, Batch 105, Train Loss: 0.27132052183151245, Memory (GB): 7\n",
      "Epoch 6, Batch 106, Train Loss: 0.282200425863266, Memory (GB): 7\n",
      "Epoch 6, Batch 107, Train Loss: 0.27223214507102966, Memory (GB): 7\n",
      "Epoch 6, Batch 108, Train Loss: 0.2691476345062256, Memory (GB): 7\n",
      "Epoch 6, Batch 109, Train Loss: 0.2512376606464386, Memory (GB): 7\n",
      "Epoch 6, Batch 110, Train Loss: 0.23923760652542114, Memory (GB): 7\n",
      "Epoch 6, Batch 111, Train Loss: 0.2573774456977844, Memory (GB): 7\n",
      "Epoch 6, Batch 112, Train Loss: 0.24788843095302582, Memory (GB): 7\n",
      "Epoch 6, Batch 113, Train Loss: 0.2638329565525055, Memory (GB): 7\n",
      "Epoch 6, Batch 114, Train Loss: 0.30584901571273804, Memory (GB): 7\n",
      "Epoch 6, Batch 115, Train Loss: 0.273897647857666, Memory (GB): 7\n",
      "Epoch 6, Batch 116, Train Loss: 0.2781689465045929, Memory (GB): 7\n",
      "Epoch 6, Batch 117, Train Loss: 0.28379422426223755, Memory (GB): 7\n",
      "Epoch 6, Batch 118, Train Loss: 0.27587369084358215, Memory (GB): 7\n",
      "Epoch 6, Batch 119, Train Loss: 0.2804243564605713, Memory (GB): 7\n",
      "Epoch 6, Batch 120, Train Loss: 0.2900945544242859, Memory (GB): 7\n",
      "Epoch 6, Batch 121, Train Loss: 0.288531631231308, Memory (GB): 7\n",
      "Epoch 6, Batch 122, Train Loss: 0.2854855954647064, Memory (GB): 7\n",
      "Epoch 6, Batch 123, Train Loss: 0.23617398738861084, Memory (GB): 7\n",
      "Epoch 6, Batch 124, Train Loss: 0.3189012408256531, Memory (GB): 7\n",
      "Epoch 6, Batch 125, Train Loss: 0.28042423725128174, Memory (GB): 7\n",
      "Epoch 6, Batch 126, Train Loss: 0.26433396339416504, Memory (GB): 7\n",
      "Epoch 6, Batch 127, Train Loss: 0.24281756579875946, Memory (GB): 7\n",
      "Epoch 6, Batch 128, Train Loss: 0.2983362078666687, Memory (GB): 7\n",
      "Epoch 6, Batch 129, Train Loss: 0.2663410007953644, Memory (GB): 7\n",
      "Epoch 6, Batch 130, Train Loss: 0.2680663466453552, Memory (GB): 7\n",
      "Epoch 6, Batch 131, Train Loss: 0.2515767812728882, Memory (GB): 7\n",
      "Epoch 6, Batch 132, Train Loss: 0.25785374641418457, Memory (GB): 7\n",
      "Epoch 6, Batch 133, Train Loss: 0.24467463791370392, Memory (GB): 7\n",
      "Epoch 6, Batch 134, Train Loss: 0.2561306953430176, Memory (GB): 7\n",
      "Epoch 6, Batch 135, Train Loss: 0.31110066175460815, Memory (GB): 7\n",
      "Epoch 6, Batch 136, Train Loss: 0.2671518921852112, Memory (GB): 7\n",
      "Epoch 6, Batch 137, Train Loss: 0.2608467638492584, Memory (GB): 7\n",
      "Epoch 6, Batch 138, Train Loss: 0.26790592074394226, Memory (GB): 7\n",
      "Epoch 6, Batch 139, Train Loss: 0.2698257565498352, Memory (GB): 7\n",
      "Epoch 6, Batch 140, Train Loss: 0.27542194724082947, Memory (GB): 7\n",
      "Epoch 6, Batch 141, Train Loss: 0.2618318796157837, Memory (GB): 7\n",
      "Epoch 6, Batch 142, Train Loss: 0.24148333072662354, Memory (GB): 7\n",
      "Epoch 6, Batch 143, Train Loss: 0.2548436224460602, Memory (GB): 7\n",
      "Epoch 6, Batch 144, Train Loss: 0.2714506983757019, Memory (GB): 7\n",
      "Epoch 6, Batch 145, Train Loss: 0.2792368531227112, Memory (GB): 7\n",
      "Epoch 6, Batch 146, Train Loss: 0.31721267104148865, Memory (GB): 7\n",
      "Epoch 6, Batch 147, Train Loss: 0.2701270878314972, Memory (GB): 7\n",
      "Epoch 6, Batch 148, Train Loss: 0.29541611671447754, Memory (GB): 7\n",
      "Epoch 6, Batch 149, Train Loss: 0.2979993224143982, Memory (GB): 7\n",
      "Epoch 6, Batch 150, Train Loss: 0.2653214931488037, Memory (GB): 7\n",
      "Epoch 6, Batch 151, Train Loss: 0.2556540369987488, Memory (GB): 7\n",
      "Epoch 6, Batch 152, Train Loss: 0.28766146302223206, Memory (GB): 7\n",
      "Epoch 6, Batch 153, Train Loss: 0.2476627677679062, Memory (GB): 7\n",
      "Epoch 6, Batch 154, Train Loss: 0.2667914927005768, Memory (GB): 7\n",
      "Epoch 6, Batch 155, Train Loss: 0.26546791195869446, Memory (GB): 7\n",
      "Epoch 6, Batch 156, Train Loss: 0.25466209650039673, Memory (GB): 7\n",
      "Epoch 6, Batch 157, Train Loss: 0.2865658700466156, Memory (GB): 7\n",
      "Epoch 6, Batch 158, Train Loss: 0.2541216313838959, Memory (GB): 7\n",
      "Epoch 6, Batch 159, Train Loss: 0.24813219904899597, Memory (GB): 7\n",
      "Epoch 6, Batch 160, Train Loss: 0.2517651915550232, Memory (GB): 7\n",
      "Epoch 6, Batch 161, Train Loss: 0.26008519530296326, Memory (GB): 7\n",
      "Epoch 6, Batch 162, Train Loss: 0.2731034457683563, Memory (GB): 7\n",
      "Epoch 6, Batch 163, Train Loss: 0.25194767117500305, Memory (GB): 7\n",
      "Epoch 6, Batch 164, Train Loss: 0.2668861150741577, Memory (GB): 7\n",
      "Epoch 6, Batch 165, Train Loss: 0.2877006530761719, Memory (GB): 7\n",
      "Epoch 6, Batch 166, Train Loss: 0.23529581725597382, Memory (GB): 7\n",
      "Epoch 6, Batch 167, Train Loss: 0.2734792232513428, Memory (GB): 7\n",
      "Epoch 6, Batch 168, Train Loss: 0.2649896442890167, Memory (GB): 7\n",
      "Epoch 6, Batch 169, Train Loss: 0.27755025029182434, Memory (GB): 7\n",
      "Epoch 6, Batch 170, Train Loss: 0.25287672877311707, Memory (GB): 7\n",
      "Epoch 6, Batch 171, Train Loss: 0.2660616338253021, Memory (GB): 7\n",
      "Epoch 6, Batch 172, Train Loss: 0.2663448750972748, Memory (GB): 7\n",
      "Epoch 6, Batch 173, Train Loss: 0.23965151607990265, Memory (GB): 7\n",
      "Epoch 6, Batch 174, Train Loss: 0.2665327787399292, Memory (GB): 7\n",
      "Epoch 6, Batch 175, Train Loss: 0.28941336274147034, Memory (GB): 7\n",
      "Epoch 6, Batch 176, Train Loss: 0.2949138283729553, Memory (GB): 7\n",
      "Epoch 6, Batch 177, Train Loss: 0.2800482213497162, Memory (GB): 7\n",
      "Epoch 6, Batch 178, Train Loss: 0.26231685280799866, Memory (GB): 7\n",
      "Epoch 6, Batch 179, Train Loss: 0.26472845673561096, Memory (GB): 7\n",
      "[1.7340377949449186e-05, 1.6473359051976718e-05, 1.564969109937789e-05, 1.4867206544408987e-05, 1.4123846217188539e-05, 1.3417653906329111e-05, 1.2746771211012652e-05, 1.2109432650462022e-05, 1.150396101793892e-05, 1.092876296704197e-05, 1.038232481868987e-05, 9.863208577755377e-06, 9.37004814886761e-06, 8.901545741424228e-06, 8.456468454353017e-06, 8.03364503163537e-06, 7.631962780053595e-06, 7.250364641050917e-06, 6.88784640899837e-06, 6.543454088548452e-06, 6.2162813841210295e-06, 5.905467314914977e-06, 5.610193949169226e-06, 5.329684251710765e-06, 5.063200039125225e-06, 4.810040037168964e-06, 4.569538035310516e-06, 4.34106113354499e-06, 4.124008076867739e-06, 3.917807673024353e-06, 3.7219172893731348e-06, 3.535821424904478e-06, 3.3590303536592553e-06, 3.191078835976291e-06, 3.0315248941774767e-06, 2.8799486494686027e-06, 2.7359512169951724e-06, 2.599153656145413e-06, 2.4691959733381424e-06, 2.3457361746712355e-06, 2.2284493659376735e-06, 2.117026897640791e-06, 2.0111755527587494e-06, 1.9106167751208122e-06, 1.8150859363647718e-06, 1.7243316395465328e-06, 1.6381150575692063e-06, 1.5562093046907455e-06, 1.4783988394562086e-06, 1.4044788974833977e-06, 1.334254952609228e-06, 1.2675422049787664e-06, 1.2041650947298284e-06, 1.1439568399933367e-06, 1.0867589979936697e-06, 1.0324210480939863e-06, 9.807999956892871e-07, 9.317599959048227e-07, 8.851719961095815e-07, 8.409133963041024e-07, 7.988677264888973e-07, 7.58924340164452e-07, 7.209781231562294e-07, 6.849292169984178e-07, 6.506827561484971e-07, 6.181486183410722e-07, 5.872411874240183e-07, 5.578791280528175e-07, 5.299851716501766e-07, 5.034859130676678e-07, 4.783116174142844e-07, 4.5439603654357007e-07, 4.3167623471639157e-07, 4.10092422980572e-07, 3.895878018315434e-07, 3.701084117399661e-07, 3.5160299115296774e-07, 3.3402284159531943e-07, 3.173216995155535e-07, 3.014556145397758e-07, 2.8638283381278704e-07, 2.720636921221476e-07, 2.5846050751604025e-07, 2.4553748214023825e-07, 2.3326060803322632e-07, 2.2159757763156505e-07, 2.1051769874998674e-07, 1.9999181381248744e-07, 1.8999222312186307e-07, 1.8049261196576983e-07, 1.7146798136748137e-07, 1.6289458229910726e-07, 1.5474985318415187e-07, 1.4701236052494436e-07, 1.3966174249869712e-07, 1.3267865537376225e-07, 1.2604472260507416e-07, 1.197424864748204e-07, 1.1375536215107937e-07, 1.0806759404352544e-07, 1.0266421434134915e-07, 9.753100362428169e-08, 9.265445344306759e-08, 8.802173077091423e-08, 8.362064423236852e-08, 7.943961202075008e-08, 7.546763141971258e-08, 7.169424984872693e-08, 6.81095373562906e-08, 6.470406048847607e-08, 6.146885746405224e-08, 5.839541459084962e-08, 5.547564386130715e-08, 5.2701861668241785e-08, 5.00667685848297e-08, 4.756343015558821e-08, 4.5185258647808806e-08, 4.2925995715418366e-08, 4.077969592964743e-08, 3.8740711133165074e-08, 3.680367557650681e-08, 3.4963491797681454e-08, 3.32153172077974e-08, 3.155455134740752e-08, 2.997682378003714e-08, 2.8477982591035274e-08, 2.705408346148351e-08, 2.570137928840934e-08, 2.4416310323988866e-08, 2.319549480778942e-08, 2.2035720067399953e-08, 2.0933934064029953e-08, 1.9887237360828466e-08, 1.8892875492787036e-08, 1.7948231718147687e-08, 1.7050820132240296e-08, 1.619827912562828e-08, 1.5388365169346873e-08, 1.4618946910879522e-08, 1.3887999565335545e-08, 1.3193599587068769e-08, 1.253391960771533e-08, 1.190722362732956e-08, 1.1311862445963085e-08, 1.074626932366493e-08, 1.0208955857481682e-08, 9.698508064607599e-09, 9.213582661377218e-09, 8.752903528308355e-09, 8.315258351892936e-09, 7.89949543429829e-09, 7.504520662583374e-09, 7.129294629454205e-09, 6.772829897981496e-09, 6.43418840308242e-09, 6.112478982928299e-09, 5.806855033781883e-09, 5.51651228209279e-09, 5.240686667988149e-09, 4.978652334588742e-09, 4.729719717859304e-09, 4.493233731966339e-09, 4.268572045368022e-09, 4.05514344309962e-09, 3.852386270944639e-09, 3.6597669573974062e-09, 3.4767786095275354e-09, 3.302939679051159e-09, 3.137792695098601e-09, 2.9809030603436707e-09, 2.8318579073264867e-09, 2.6902650119601624e-09, 2.5557517613621547e-09, 2.4279641732940467e-09, 2.306565964629344e-09, 2.1912376663978767e-09, 2.0816757830779833e-09, 1.9775919939240844e-09, 1.8787123942278803e-09, 1.7847767745164856e-09, 1.6955379357906613e-09, 1.6107610390011281e-09, 1.5302229870510717e-09, 1.453711837698518e-09, 1.3810262458135918e-09, 1.3119749335229124e-09, 1.2463761868467664e-09, 1.184057377504428e-09, 1.1248545086292063e-09, 1.068611783197746e-09, 1.0151811940378587e-09, 9.644221343359662e-10, 9.162010276191674e-10, 8.703909762382092e-10, 8.268714274262986e-10, 7.855278560549836e-10, 7.462514632522343e-10, 7.089388900896227e-10, 6.734919455851415e-10]\n",
      "Epoch 7, Batch 0, Train Loss: 0.26273268461227417, Memory (GB): 7\n",
      "Epoch 7, Batch 1, Train Loss: 0.30302366614341736, Memory (GB): 7\n",
      "Epoch 7, Batch 2, Train Loss: 0.2640559673309326, Memory (GB): 7\n",
      "Epoch 7, Batch 3, Train Loss: 0.24466180801391602, Memory (GB): 7\n",
      "Epoch 7, Batch 4, Train Loss: 0.27708184719085693, Memory (GB): 7\n",
      "Epoch 7, Batch 5, Train Loss: 0.27087050676345825, Memory (GB): 7\n",
      "Epoch 7, Batch 6, Train Loss: 0.24608443677425385, Memory (GB): 7\n",
      "Epoch 7, Batch 7, Train Loss: 0.2647126615047455, Memory (GB): 7\n",
      "Epoch 7, Batch 8, Train Loss: 0.2481050193309784, Memory (GB): 7\n",
      "Epoch 7, Batch 9, Train Loss: 0.2610880434513092, Memory (GB): 7\n",
      "Epoch 7, Batch 10, Train Loss: 0.26603156328201294, Memory (GB): 7\n",
      "Epoch 7, Batch 11, Train Loss: 0.2381235808134079, Memory (GB): 7\n",
      "Epoch 7, Batch 12, Train Loss: 0.25375834107398987, Memory (GB): 7\n",
      "Epoch 7, Batch 13, Train Loss: 0.264636367559433, Memory (GB): 7\n",
      "Epoch 7, Batch 14, Train Loss: 0.23451514542102814, Memory (GB): 7\n",
      "Epoch 7, Batch 15, Train Loss: 0.25048670172691345, Memory (GB): 7\n",
      "Epoch 7, Batch 16, Train Loss: 0.27346527576446533, Memory (GB): 7\n",
      "Epoch 7, Batch 17, Train Loss: 0.274828165769577, Memory (GB): 7\n",
      "Epoch 7, Batch 18, Train Loss: 0.2561885416507721, Memory (GB): 7\n",
      "Epoch 7, Batch 19, Train Loss: 0.28689923882484436, Memory (GB): 7\n",
      "Epoch 7, Batch 20, Train Loss: 0.2976124584674835, Memory (GB): 7\n",
      "Epoch 7, Batch 21, Train Loss: 0.2718897759914398, Memory (GB): 7\n",
      "Epoch 7, Batch 22, Train Loss: 0.23684574663639069, Memory (GB): 7\n",
      "Epoch 7, Batch 23, Train Loss: 0.2589326500892639, Memory (GB): 7\n",
      "Epoch 7, Batch 24, Train Loss: 0.2637808322906494, Memory (GB): 7\n",
      "Epoch 7, Batch 25, Train Loss: 0.24051176011562347, Memory (GB): 7\n",
      "Epoch 7, Batch 26, Train Loss: 0.2807885408401489, Memory (GB): 7\n",
      "Epoch 7, Batch 27, Train Loss: 0.24163870513439178, Memory (GB): 7\n",
      "Epoch 7, Batch 28, Train Loss: 0.24160905182361603, Memory (GB): 7\n",
      "Epoch 7, Batch 29, Train Loss: 0.27741456031799316, Memory (GB): 7\n",
      "Epoch 7, Batch 30, Train Loss: 0.23026011884212494, Memory (GB): 7\n",
      "Epoch 7, Batch 31, Train Loss: 0.22582313418388367, Memory (GB): 7\n",
      "Epoch 7, Batch 32, Train Loss: 0.2570490539073944, Memory (GB): 7\n",
      "Epoch 7, Batch 33, Train Loss: 0.2825159728527069, Memory (GB): 7\n",
      "Epoch 7, Batch 34, Train Loss: 0.2789655029773712, Memory (GB): 7\n",
      "Epoch 7, Batch 35, Train Loss: 0.2618429660797119, Memory (GB): 7\n",
      "Epoch 7, Batch 36, Train Loss: 0.2764557898044586, Memory (GB): 7\n",
      "Epoch 7, Batch 37, Train Loss: 0.2527919411659241, Memory (GB): 7\n",
      "Epoch 7, Batch 38, Train Loss: 0.2812455892562866, Memory (GB): 7\n",
      "Epoch 7, Batch 39, Train Loss: 0.28457826375961304, Memory (GB): 7\n",
      "Epoch 7, Batch 40, Train Loss: 0.25129395723342896, Memory (GB): 7\n",
      "Epoch 7, Batch 41, Train Loss: 0.26445716619491577, Memory (GB): 7\n",
      "Epoch 7, Batch 42, Train Loss: 0.2669675946235657, Memory (GB): 7\n",
      "Epoch 7, Batch 43, Train Loss: 0.2426287829875946, Memory (GB): 7\n",
      "Epoch 7, Batch 44, Train Loss: 0.27544528245925903, Memory (GB): 7\n",
      "Epoch 7, Batch 45, Train Loss: 0.26855045557022095, Memory (GB): 7\n",
      "Epoch 7, Batch 46, Train Loss: 0.26052388548851013, Memory (GB): 7\n",
      "Epoch 7, Batch 47, Train Loss: 0.2860392928123474, Memory (GB): 7\n",
      "Epoch 7, Batch 48, Train Loss: 0.277099072933197, Memory (GB): 7\n",
      "Epoch 7, Batch 49, Train Loss: 0.3010595142841339, Memory (GB): 7\n",
      "Epoch 7, Batch 50, Train Loss: 0.2859870493412018, Memory (GB): 7\n",
      "Epoch 7, Batch 51, Train Loss: 0.28522494435310364, Memory (GB): 7\n",
      "Epoch 7, Batch 52, Train Loss: 0.2655145525932312, Memory (GB): 7\n",
      "Epoch 7, Batch 53, Train Loss: 0.26145321130752563, Memory (GB): 7\n",
      "Epoch 7, Batch 54, Train Loss: 0.24731171131134033, Memory (GB): 7\n",
      "Epoch 7, Batch 55, Train Loss: 0.30604812502861023, Memory (GB): 7\n",
      "Epoch 7, Batch 56, Train Loss: 0.26558631658554077, Memory (GB): 7\n",
      "Epoch 7, Batch 57, Train Loss: 0.2622220516204834, Memory (GB): 7\n",
      "Epoch 7, Batch 58, Train Loss: 0.2775917053222656, Memory (GB): 7\n",
      "Epoch 7, Batch 59, Train Loss: 0.2615630626678467, Memory (GB): 7\n",
      "Epoch 7, Batch 60, Train Loss: 0.27143973112106323, Memory (GB): 7\n",
      "Epoch 7, Batch 61, Train Loss: 0.26737022399902344, Memory (GB): 7\n",
      "Epoch 7, Batch 62, Train Loss: 0.30225831270217896, Memory (GB): 7\n",
      "Epoch 7, Batch 63, Train Loss: 0.29068830609321594, Memory (GB): 7\n",
      "Epoch 7, Batch 64, Train Loss: 0.26884329319000244, Memory (GB): 7\n",
      "Epoch 7, Batch 65, Train Loss: 0.25317272543907166, Memory (GB): 7\n",
      "Epoch 7, Batch 66, Train Loss: 0.24368727207183838, Memory (GB): 7\n",
      "Epoch 7, Batch 67, Train Loss: 0.26654648780822754, Memory (GB): 7\n",
      "Epoch 7, Batch 68, Train Loss: 0.2757619619369507, Memory (GB): 7\n",
      "Epoch 7, Batch 69, Train Loss: 0.24524979293346405, Memory (GB): 7\n",
      "Epoch 7, Batch 70, Train Loss: 0.2516411244869232, Memory (GB): 7\n",
      "Epoch 7, Batch 71, Train Loss: 0.24784061312675476, Memory (GB): 7\n",
      "Epoch 7, Batch 72, Train Loss: 0.25679752230644226, Memory (GB): 7\n",
      "Epoch 7, Batch 73, Train Loss: 0.2969110310077667, Memory (GB): 7\n",
      "Epoch 7, Batch 74, Train Loss: 0.25177451968193054, Memory (GB): 7\n",
      "Epoch 7, Batch 75, Train Loss: 0.26403093338012695, Memory (GB): 7\n",
      "Epoch 7, Batch 76, Train Loss: 0.2835404872894287, Memory (GB): 7\n",
      "Epoch 7, Batch 77, Train Loss: 0.2854578495025635, Memory (GB): 7\n",
      "Epoch 7, Batch 78, Train Loss: 0.2592724561691284, Memory (GB): 7\n",
      "Epoch 7, Batch 79, Train Loss: 0.27048325538635254, Memory (GB): 7\n",
      "Epoch 7, Batch 80, Train Loss: 0.2383001446723938, Memory (GB): 7\n",
      "Epoch 7, Batch 81, Train Loss: 0.23310665786266327, Memory (GB): 7\n",
      "Epoch 7, Batch 82, Train Loss: 0.2579154372215271, Memory (GB): 7\n",
      "Epoch 7, Batch 83, Train Loss: 0.26048585772514343, Memory (GB): 7\n",
      "Epoch 7, Batch 84, Train Loss: 0.2362384796142578, Memory (GB): 7\n",
      "Epoch 7, Batch 85, Train Loss: 0.26043015718460083, Memory (GB): 7\n",
      "Epoch 7, Batch 86, Train Loss: 0.26143717765808105, Memory (GB): 7\n",
      "Epoch 7, Batch 87, Train Loss: 0.2548758089542389, Memory (GB): 7\n",
      "Epoch 7, Batch 88, Train Loss: 0.25060269236564636, Memory (GB): 7\n",
      "Epoch 7, Batch 89, Train Loss: 0.24693147838115692, Memory (GB): 7\n",
      "Epoch 7, Batch 90, Train Loss: 0.23433710634708405, Memory (GB): 7\n",
      "Epoch 7, Batch 91, Train Loss: 0.2705213129520416, Memory (GB): 7\n",
      "Epoch 7, Batch 92, Train Loss: 0.2368665188550949, Memory (GB): 7\n",
      "Epoch 7, Batch 93, Train Loss: 0.282024621963501, Memory (GB): 7\n",
      "Epoch 7, Batch 94, Train Loss: 0.24735300242900848, Memory (GB): 7\n",
      "Epoch 7, Batch 95, Train Loss: 0.25182878971099854, Memory (GB): 7\n",
      "Epoch 7, Batch 96, Train Loss: 0.27781930565834045, Memory (GB): 7\n",
      "Epoch 7, Batch 97, Train Loss: 0.24602940678596497, Memory (GB): 7\n",
      "Epoch 7, Batch 98, Train Loss: 0.25702813267707825, Memory (GB): 7\n",
      "Epoch 7, Batch 99, Train Loss: 0.2453891634941101, Memory (GB): 7\n",
      "Epoch 7, Batch 100, Train Loss: 0.2629753649234772, Memory (GB): 7\n",
      "Epoch 7, Batch 101, Train Loss: 0.24773338437080383, Memory (GB): 7\n",
      "Epoch 7, Batch 102, Train Loss: 0.2842174172401428, Memory (GB): 7\n",
      "Epoch 7, Batch 103, Train Loss: 0.30220839381217957, Memory (GB): 7\n",
      "Epoch 7, Batch 104, Train Loss: 0.2407550811767578, Memory (GB): 7\n",
      "Epoch 7, Batch 105, Train Loss: 0.28055840730667114, Memory (GB): 7\n",
      "Epoch 7, Batch 106, Train Loss: 0.26009273529052734, Memory (GB): 7\n",
      "Epoch 7, Batch 107, Train Loss: 0.25259530544281006, Memory (GB): 7\n",
      "Epoch 7, Batch 108, Train Loss: 0.2564528286457062, Memory (GB): 7\n",
      "Epoch 7, Batch 109, Train Loss: 0.3055034577846527, Memory (GB): 7\n",
      "Epoch 7, Batch 110, Train Loss: 0.27899134159088135, Memory (GB): 7\n",
      "Epoch 7, Batch 111, Train Loss: 0.2577666640281677, Memory (GB): 7\n",
      "Epoch 7, Batch 112, Train Loss: 0.266568124294281, Memory (GB): 7\n",
      "Epoch 7, Batch 113, Train Loss: 0.2631758153438568, Memory (GB): 7\n",
      "Epoch 7, Batch 114, Train Loss: 0.20908242464065552, Memory (GB): 7\n",
      "Epoch 7, Batch 115, Train Loss: 0.3053395748138428, Memory (GB): 7\n",
      "Epoch 7, Batch 116, Train Loss: 0.23462490737438202, Memory (GB): 7\n",
      "Epoch 7, Batch 117, Train Loss: 0.2386600822210312, Memory (GB): 7\n",
      "Epoch 7, Batch 118, Train Loss: 0.2950662672519684, Memory (GB): 7\n",
      "Epoch 7, Batch 119, Train Loss: 0.2396600991487503, Memory (GB): 7\n",
      "Epoch 7, Batch 120, Train Loss: 0.24942220747470856, Memory (GB): 7\n",
      "Epoch 7, Batch 121, Train Loss: 0.2702862322330475, Memory (GB): 7\n",
      "Epoch 7, Batch 122, Train Loss: 0.2106844186782837, Memory (GB): 7\n",
      "Epoch 7, Batch 123, Train Loss: 0.2308272421360016, Memory (GB): 7\n",
      "Epoch 7, Batch 124, Train Loss: 0.29393553733825684, Memory (GB): 7\n",
      "Epoch 7, Batch 125, Train Loss: 0.2643761932849884, Memory (GB): 7\n",
      "Epoch 7, Batch 126, Train Loss: 0.26920875906944275, Memory (GB): 7\n",
      "Epoch 7, Batch 127, Train Loss: 0.2536366879940033, Memory (GB): 7\n",
      "Epoch 7, Batch 128, Train Loss: 0.2715069353580475, Memory (GB): 7\n",
      "Epoch 7, Batch 129, Train Loss: 0.27542710304260254, Memory (GB): 7\n",
      "Epoch 7, Batch 130, Train Loss: 0.24806708097457886, Memory (GB): 7\n",
      "Epoch 7, Batch 131, Train Loss: 0.2710287570953369, Memory (GB): 7\n",
      "Epoch 7, Batch 132, Train Loss: 0.2748153507709503, Memory (GB): 7\n",
      "Epoch 7, Batch 133, Train Loss: 0.2742505967617035, Memory (GB): 7\n",
      "Epoch 7, Batch 134, Train Loss: 0.25840067863464355, Memory (GB): 7\n",
      "Epoch 7, Batch 135, Train Loss: 0.28671687841415405, Memory (GB): 7\n",
      "Epoch 7, Batch 136, Train Loss: 0.2630443871021271, Memory (GB): 7\n",
      "Epoch 7, Batch 137, Train Loss: 0.25562915205955505, Memory (GB): 7\n",
      "Epoch 7, Batch 138, Train Loss: 0.27250200510025024, Memory (GB): 7\n",
      "Epoch 7, Batch 139, Train Loss: 0.2523399293422699, Memory (GB): 7\n",
      "Epoch 7, Batch 140, Train Loss: 0.275385320186615, Memory (GB): 7\n",
      "Epoch 7, Batch 141, Train Loss: 0.24961526691913605, Memory (GB): 7\n",
      "Epoch 7, Batch 142, Train Loss: 0.2554017901420593, Memory (GB): 7\n",
      "Epoch 7, Batch 143, Train Loss: 0.26141661405563354, Memory (GB): 7\n",
      "Epoch 7, Batch 144, Train Loss: 0.26466402411460876, Memory (GB): 7\n",
      "Epoch 7, Batch 145, Train Loss: 0.271272748708725, Memory (GB): 7\n",
      "Epoch 7, Batch 146, Train Loss: 0.24618647992610931, Memory (GB): 7\n",
      "Epoch 7, Batch 147, Train Loss: 0.2449105978012085, Memory (GB): 7\n",
      "Epoch 7, Batch 148, Train Loss: 0.2977038025856018, Memory (GB): 7\n",
      "Epoch 7, Batch 149, Train Loss: 0.2448192834854126, Memory (GB): 7\n",
      "Epoch 7, Batch 150, Train Loss: 0.28996872901916504, Memory (GB): 7\n",
      "Epoch 7, Batch 151, Train Loss: 0.24988864362239838, Memory (GB): 7\n",
      "Epoch 7, Batch 152, Train Loss: 0.2507888376712799, Memory (GB): 7\n",
      "Epoch 7, Batch 153, Train Loss: 0.26423218846321106, Memory (GB): 7\n",
      "Epoch 7, Batch 154, Train Loss: 0.2293529063463211, Memory (GB): 7\n",
      "Epoch 7, Batch 155, Train Loss: 0.3294246196746826, Memory (GB): 7\n",
      "Epoch 7, Batch 156, Train Loss: 0.26137575507164, Memory (GB): 7\n",
      "Epoch 7, Batch 157, Train Loss: 0.2658819556236267, Memory (GB): 7\n",
      "Epoch 7, Batch 158, Train Loss: 0.23648551106452942, Memory (GB): 7\n",
      "Epoch 7, Batch 159, Train Loss: 0.23611505329608917, Memory (GB): 7\n",
      "Epoch 7, Batch 160, Train Loss: 0.2670404016971588, Memory (GB): 7\n",
      "Epoch 7, Batch 161, Train Loss: 0.2882230281829834, Memory (GB): 7\n",
      "Epoch 7, Batch 162, Train Loss: 0.3045629858970642, Memory (GB): 7\n",
      "Epoch 7, Batch 163, Train Loss: 0.2553914487361908, Memory (GB): 7\n",
      "Epoch 7, Batch 164, Train Loss: 0.23984895646572113, Memory (GB): 7\n",
      "Epoch 7, Batch 165, Train Loss: 0.26975077390670776, Memory (GB): 7\n",
      "Epoch 7, Batch 166, Train Loss: 0.24409687519073486, Memory (GB): 7\n",
      "Epoch 7, Batch 167, Train Loss: 0.2532733380794525, Memory (GB): 7\n",
      "Epoch 7, Batch 168, Train Loss: 0.3022330403327942, Memory (GB): 7\n",
      "Epoch 7, Batch 169, Train Loss: 0.28066983819007874, Memory (GB): 7\n",
      "Epoch 7, Batch 170, Train Loss: 0.2703858017921448, Memory (GB): 7\n",
      "Epoch 7, Batch 171, Train Loss: 0.2507483661174774, Memory (GB): 7\n",
      "Epoch 7, Batch 172, Train Loss: 0.2451886683702469, Memory (GB): 7\n",
      "Epoch 7, Batch 173, Train Loss: 0.25856709480285645, Memory (GB): 7\n",
      "Epoch 7, Batch 174, Train Loss: 0.2639102339744568, Memory (GB): 7\n",
      "Epoch 7, Batch 175, Train Loss: 0.2696400284767151, Memory (GB): 7\n",
      "Epoch 7, Batch 176, Train Loss: 0.2656140923500061, Memory (GB): 7\n",
      "Epoch 7, Batch 177, Train Loss: 0.2646338641643524, Memory (GB): 7\n",
      "Epoch 7, Batch 178, Train Loss: 0.27329936623573303, Memory (GB): 7\n",
      "Epoch 7, Batch 179, Train Loss: 0.28772687911987305, Memory (GB): 7\n",
      "[2.5938912469834457e-05, 2.464196684634272e-05, 2.3409868504025587e-05, 2.2239375078824318e-05, 2.11274063248831e-05, 2.0071036008638936e-05, 1.906748420820698e-05, 1.811410999779663e-05, 1.7208404497906814e-05, 1.634798427301145e-05, 1.5530585059360895e-05, 1.475405580639283e-05, 1.40163530160732e-05, 1.3315535365269535e-05, 1.2649758597006066e-05, 1.2017270667155756e-05, 1.1416407133797963e-05, 1.0845586777108077e-05, 1.0303307438252666e-05, 9.788142066340032e-06, 9.298734963023033e-06, 8.833798214871879e-06, 8.392108304128277e-06, 7.972502888921875e-06, 7.5738777444757735e-06, 7.195183857251986e-06, 6.835424664389389e-06, 6.493653431169912e-06, 6.168970759611414e-06, 5.860522221630847e-06, 5.567496110549305e-06, 5.289121305021837e-06, 5.024665239770745e-06, 4.773431977782209e-06, 4.534760378893098e-06, 4.308022359948445e-06, 4.092621241951022e-06, 3.887990179853469e-06, 3.693590670860798e-06, 3.508911137317756e-06, 3.333465580451868e-06, 3.166792301429272e-06, 3.008452686357812e-06, 2.8580300520399225e-06, 2.7151285494379226e-06, 2.5793721219660273e-06, 2.450403515867726e-06, 2.3278833400743397e-06, 2.2114891730706234e-06, 2.100914714417092e-06, 1.9958689786962354e-06, 1.8960755297614235e-06, 1.8012717532733546e-06, 1.7112081656096849e-06, 1.6256477573292018e-06, 1.544365369462742e-06, 1.4671471009896054e-06, 1.3937897459401234e-06, 1.3241002586431176e-06, 1.2578952457109614e-06, 1.1950004834254136e-06, 1.1352504592541426e-06, 1.0784879362914354e-06, 1.0245635394768631e-06, 9.733353625030198e-07, 9.246685943778694e-07, 8.784351646589755e-07, 8.345134064260264e-07, 7.927877361047252e-07, 7.531483492994896e-07, 7.154909318345148e-07, 6.797163852427889e-07, 6.457305659806495e-07, 6.134440376816169e-07, 5.827718357975363e-07, 5.536332440076587e-07, 5.259515818072763e-07, 4.996540027169122e-07, 4.7467130258106673e-07, 4.509377374520137e-07, 4.283908505794128e-07, 4.069713080504421e-07, 3.8662274264792006e-07, 3.672916055155238e-07, 3.489270252397477e-07, 3.314806739777604e-07, 3.14906640278872e-07, 2.9916130826492884e-07, 2.842032428516824e-07, 2.6999308070909813e-07, 2.56493426673643e-07, 2.43668755339961e-07, 2.3148531757296305e-07, 2.1991105169431486e-07, 2.089154991095991e-07, 1.984697241541192e-07, 1.8854623794641306e-07, 1.7911892604909257e-07, 1.701629797466379e-07, 1.6165483075930603e-07, 1.5357208922134072e-07, 1.458934847602737e-07, 1.3859881052225982e-07, 1.3166886999614705e-07, 1.2508542649633962e-07, 1.1883115517152259e-07, 1.1288959741294652e-07, 1.0724511754229917e-07, 1.0188286166518414e-07, 9.678871858192494e-08, 9.19492826528287e-08, 8.735181852018725e-08, 8.298422759417793e-08, 7.883501621446894e-08, 7.489326540374554e-08, 7.114860213355826e-08, 6.759117202688035e-08, 6.421161342553633e-08, 6.100103275425956e-08, 5.7950981116546504e-08, 5.505343206071919e-08, 5.230076045768323e-08, 4.968572243479906e-08, 4.720143631305913e-08, 4.484136449740619e-08, 4.259929627253584e-08, 4.046933145890905e-08, 3.8445864885963636e-08, 3.6523571641665424e-08, 3.4697393059582156e-08, 3.2962523406603025e-08, 3.131439723627291e-08, 2.9748677374459234e-08, 2.8261243505736296e-08, 2.6848181330449474e-08, 2.5505772263926996e-08, 2.4230483650730647e-08, 2.3018959468194103e-08, 2.1868011494784424e-08, 2.0774610920045186e-08, 1.973588037404292e-08, 1.8749086355340786e-08, 1.7811632037573732e-08, 1.6921050435695053e-08, 1.607499791391028e-08, 1.5271248018214776e-08, 1.4507685617304038e-08, 1.3782301336438834e-08, 1.3093186269616895e-08, 1.2438526956136054e-08, 1.1816600608329233e-08, 1.1225770577912787e-08, 1.0664482049017137e-08, 1.0131257946566281e-08, 9.624695049237976e-09, 9.14346029677607e-09, 8.68628728193726e-09, 8.251972917840401e-09, 7.839374271948378e-09, 7.447405558350961e-09, 7.075035280433415e-09, 6.721283516411745e-09, 6.385219340591156e-09, 6.065958373561594e-09, 5.762660454883516e-09, 5.4745274321393435e-09, 5.200801060532372e-09, 4.940761007505753e-09, 4.693722957130467e-09, 4.459036809273941e-09, 4.236084968810247e-09, 4.024280720369735e-09, 3.82306668435125e-09, 3.631913350133685e-09, 3.4503176826269996e-09, 3.277801798495649e-09, 3.1139117085708648e-09, 2.9582161231423237e-09, 2.8103053169852077e-09, 2.6697900511359466e-09, 2.5363005485791504e-09, 2.409485521150193e-09, 2.2890112450926833e-09, 2.174560682838049e-09, 2.0658326486961463e-09, 1.9625410162613373e-09, 1.864413965448271e-09, 1.7711932671758578e-09, 1.6826336038170648e-09, 1.5985019236262102e-09, 1.5185768274449012e-09, 1.442647986072655e-09, 1.370515586769022e-09, 1.3019898074305722e-09, 1.2368903170590425e-09, 1.1750458012060904e-09, 1.1162935111457854e-09, 1.0604788355884963e-09, 1.007454893809072e-09]\n",
      "Epoch 8, Batch 0, Train Loss: 0.2370455116033554, Memory (GB): 7\n",
      "Epoch 8, Batch 1, Train Loss: 0.23661713302135468, Memory (GB): 7\n",
      "Epoch 8, Batch 2, Train Loss: 0.2576717138290405, Memory (GB): 7\n",
      "Epoch 8, Batch 3, Train Loss: 0.26059141755104065, Memory (GB): 7\n",
      "Epoch 8, Batch 4, Train Loss: 0.262493371963501, Memory (GB): 7\n",
      "Epoch 8, Batch 5, Train Loss: 0.22859780490398407, Memory (GB): 7\n",
      "Epoch 8, Batch 6, Train Loss: 0.2711739242076874, Memory (GB): 7\n",
      "Epoch 8, Batch 7, Train Loss: 0.24389231204986572, Memory (GB): 7\n",
      "Epoch 8, Batch 8, Train Loss: 0.2592487931251526, Memory (GB): 7\n",
      "Epoch 8, Batch 9, Train Loss: 0.2467249184846878, Memory (GB): 7\n",
      "Epoch 8, Batch 10, Train Loss: 0.27454090118408203, Memory (GB): 7\n",
      "Epoch 8, Batch 11, Train Loss: 0.27257266640663147, Memory (GB): 7\n",
      "Epoch 8, Batch 12, Train Loss: 0.24236756563186646, Memory (GB): 7\n",
      "Epoch 8, Batch 13, Train Loss: 0.27451497316360474, Memory (GB): 7\n",
      "Epoch 8, Batch 14, Train Loss: 0.278348833322525, Memory (GB): 7\n",
      "Epoch 8, Batch 15, Train Loss: 0.2277131825685501, Memory (GB): 7\n",
      "Epoch 8, Batch 16, Train Loss: 0.2657739818096161, Memory (GB): 7\n",
      "Epoch 8, Batch 17, Train Loss: 0.22671592235565186, Memory (GB): 7\n",
      "Epoch 8, Batch 18, Train Loss: 0.25691476464271545, Memory (GB): 7\n",
      "Epoch 8, Batch 19, Train Loss: 0.2527068555355072, Memory (GB): 7\n",
      "Epoch 8, Batch 20, Train Loss: 0.2633918523788452, Memory (GB): 7\n",
      "Epoch 8, Batch 21, Train Loss: 0.25293946266174316, Memory (GB): 7\n",
      "Epoch 8, Batch 22, Train Loss: 0.24081920087337494, Memory (GB): 7\n",
      "Epoch 8, Batch 23, Train Loss: 0.2312954068183899, Memory (GB): 7\n",
      "Epoch 8, Batch 24, Train Loss: 0.252765417098999, Memory (GB): 7\n",
      "Epoch 8, Batch 25, Train Loss: 0.26321330666542053, Memory (GB): 7\n",
      "Epoch 8, Batch 26, Train Loss: 0.2590492367744446, Memory (GB): 7\n",
      "Epoch 8, Batch 27, Train Loss: 0.2272413820028305, Memory (GB): 7\n",
      "Epoch 8, Batch 28, Train Loss: 0.2460649609565735, Memory (GB): 7\n",
      "Epoch 8, Batch 29, Train Loss: 0.25868484377861023, Memory (GB): 7\n",
      "Epoch 8, Batch 30, Train Loss: 0.24159660935401917, Memory (GB): 7\n",
      "Epoch 8, Batch 31, Train Loss: 0.22366085648536682, Memory (GB): 7\n",
      "Epoch 8, Batch 32, Train Loss: 0.2504388988018036, Memory (GB): 7\n",
      "Epoch 8, Batch 33, Train Loss: 0.22979854047298431, Memory (GB): 7\n",
      "Epoch 8, Batch 34, Train Loss: 0.2647668719291687, Memory (GB): 7\n",
      "Epoch 8, Batch 35, Train Loss: 0.2741393446922302, Memory (GB): 7\n",
      "Epoch 8, Batch 36, Train Loss: 0.2747851014137268, Memory (GB): 7\n",
      "Epoch 8, Batch 37, Train Loss: 0.2556244134902954, Memory (GB): 7\n",
      "Epoch 8, Batch 38, Train Loss: 0.2590673565864563, Memory (GB): 7\n",
      "Epoch 8, Batch 39, Train Loss: 0.2655341625213623, Memory (GB): 7\n",
      "Epoch 8, Batch 40, Train Loss: 0.24006131291389465, Memory (GB): 7\n",
      "Epoch 8, Batch 41, Train Loss: 0.29174357652664185, Memory (GB): 7\n",
      "Epoch 8, Batch 42, Train Loss: 0.24884280562400818, Memory (GB): 7\n",
      "Epoch 8, Batch 43, Train Loss: 0.2485646903514862, Memory (GB): 7\n",
      "Epoch 8, Batch 44, Train Loss: 0.2290734350681305, Memory (GB): 7\n",
      "Epoch 8, Batch 45, Train Loss: 0.26145780086517334, Memory (GB): 7\n",
      "Epoch 8, Batch 46, Train Loss: 0.2622818946838379, Memory (GB): 7\n",
      "Epoch 8, Batch 47, Train Loss: 0.2574397623538971, Memory (GB): 7\n",
      "Epoch 8, Batch 48, Train Loss: 0.2668944001197815, Memory (GB): 7\n",
      "Epoch 8, Batch 49, Train Loss: 0.28016871213912964, Memory (GB): 7\n",
      "Epoch 8, Batch 50, Train Loss: 0.250843346118927, Memory (GB): 7\n",
      "Epoch 8, Batch 51, Train Loss: 0.24671338498592377, Memory (GB): 7\n",
      "Epoch 8, Batch 52, Train Loss: 0.2489982396364212, Memory (GB): 7\n",
      "Epoch 8, Batch 53, Train Loss: 0.2481883317232132, Memory (GB): 7\n",
      "Epoch 8, Batch 54, Train Loss: 0.2572850286960602, Memory (GB): 7\n",
      "Epoch 8, Batch 55, Train Loss: 0.25590798258781433, Memory (GB): 7\n",
      "Epoch 8, Batch 56, Train Loss: 0.2606286108493805, Memory (GB): 7\n",
      "Epoch 8, Batch 57, Train Loss: 0.30125585198402405, Memory (GB): 7\n",
      "Epoch 8, Batch 58, Train Loss: 0.26302456855773926, Memory (GB): 7\n",
      "Epoch 8, Batch 59, Train Loss: 0.2319934219121933, Memory (GB): 7\n",
      "Epoch 8, Batch 60, Train Loss: 0.2410602569580078, Memory (GB): 7\n",
      "Epoch 8, Batch 61, Train Loss: 0.26339900493621826, Memory (GB): 7\n",
      "Epoch 8, Batch 62, Train Loss: 0.26760247349739075, Memory (GB): 7\n",
      "Epoch 8, Batch 63, Train Loss: 0.2879900336265564, Memory (GB): 7\n",
      "Epoch 8, Batch 64, Train Loss: 0.23482058942317963, Memory (GB): 7\n",
      "Epoch 8, Batch 65, Train Loss: 0.2500499188899994, Memory (GB): 7\n",
      "Epoch 8, Batch 66, Train Loss: 0.2639658749103546, Memory (GB): 7\n",
      "Epoch 8, Batch 67, Train Loss: 0.257582426071167, Memory (GB): 7\n",
      "Epoch 8, Batch 68, Train Loss: 0.29581743478775024, Memory (GB): 7\n",
      "Epoch 8, Batch 69, Train Loss: 0.2538110315799713, Memory (GB): 7\n",
      "Epoch 8, Batch 70, Train Loss: 0.26427483558654785, Memory (GB): 7\n",
      "Epoch 8, Batch 71, Train Loss: 0.2750875949859619, Memory (GB): 7\n",
      "Epoch 8, Batch 72, Train Loss: 0.28886380791664124, Memory (GB): 7\n",
      "Epoch 8, Batch 73, Train Loss: 0.2332872450351715, Memory (GB): 7\n",
      "Epoch 8, Batch 74, Train Loss: 0.25279659032821655, Memory (GB): 7\n",
      "Epoch 8, Batch 75, Train Loss: 0.26838189363479614, Memory (GB): 7\n",
      "Epoch 8, Batch 76, Train Loss: 0.24799324572086334, Memory (GB): 7\n",
      "Epoch 8, Batch 77, Train Loss: 0.27672186493873596, Memory (GB): 7\n",
      "Epoch 8, Batch 78, Train Loss: 0.2652573585510254, Memory (GB): 7\n",
      "Epoch 8, Batch 79, Train Loss: 0.2581884264945984, Memory (GB): 7\n",
      "Epoch 8, Batch 80, Train Loss: 0.28741246461868286, Memory (GB): 7\n",
      "Epoch 8, Batch 81, Train Loss: 0.24160735309123993, Memory (GB): 7\n",
      "Epoch 8, Batch 82, Train Loss: 0.2707786560058594, Memory (GB): 7\n",
      "Epoch 8, Batch 83, Train Loss: 0.27688074111938477, Memory (GB): 7\n",
      "Epoch 8, Batch 84, Train Loss: 0.26270154118537903, Memory (GB): 7\n",
      "Epoch 8, Batch 85, Train Loss: 0.24139468371868134, Memory (GB): 7\n",
      "Epoch 8, Batch 86, Train Loss: 0.2570405602455139, Memory (GB): 7\n",
      "Epoch 8, Batch 87, Train Loss: 0.2448033094406128, Memory (GB): 7\n",
      "Epoch 8, Batch 88, Train Loss: 0.23135258257389069, Memory (GB): 7\n",
      "Epoch 8, Batch 89, Train Loss: 0.25773298740386963, Memory (GB): 7\n",
      "Epoch 8, Batch 90, Train Loss: 0.27094197273254395, Memory (GB): 7\n",
      "Epoch 8, Batch 91, Train Loss: 0.22156086564064026, Memory (GB): 7\n",
      "Epoch 8, Batch 92, Train Loss: 0.2694289982318878, Memory (GB): 7\n",
      "Epoch 8, Batch 93, Train Loss: 0.2776820659637451, Memory (GB): 7\n",
      "Epoch 8, Batch 94, Train Loss: 0.2582997679710388, Memory (GB): 7\n",
      "Epoch 8, Batch 95, Train Loss: 0.2510824203491211, Memory (GB): 7\n",
      "Epoch 8, Batch 96, Train Loss: 0.22283971309661865, Memory (GB): 7\n",
      "Epoch 8, Batch 97, Train Loss: 0.22345079481601715, Memory (GB): 7\n",
      "Epoch 8, Batch 98, Train Loss: 0.2365933358669281, Memory (GB): 7\n",
      "Epoch 8, Batch 99, Train Loss: 0.2521555423736572, Memory (GB): 7\n",
      "Epoch 8, Batch 100, Train Loss: 0.2419661581516266, Memory (GB): 7\n",
      "Epoch 8, Batch 101, Train Loss: 0.24021346867084503, Memory (GB): 7\n",
      "Epoch 8, Batch 102, Train Loss: 0.24857845902442932, Memory (GB): 7\n",
      "Epoch 8, Batch 103, Train Loss: 0.260617196559906, Memory (GB): 7\n",
      "Epoch 8, Batch 104, Train Loss: 0.28102993965148926, Memory (GB): 7\n",
      "Epoch 8, Batch 105, Train Loss: 0.2699337899684906, Memory (GB): 7\n",
      "Epoch 8, Batch 106, Train Loss: 0.248666450381279, Memory (GB): 7\n",
      "Epoch 8, Batch 107, Train Loss: 0.24460424482822418, Memory (GB): 7\n",
      "Epoch 8, Batch 108, Train Loss: 0.25766199827194214, Memory (GB): 7\n",
      "Epoch 8, Batch 109, Train Loss: 0.24933463335037231, Memory (GB): 7\n",
      "Epoch 8, Batch 110, Train Loss: 0.23554715514183044, Memory (GB): 7\n",
      "Epoch 8, Batch 111, Train Loss: 0.2617705166339874, Memory (GB): 7\n",
      "Epoch 8, Batch 112, Train Loss: 0.26681259274482727, Memory (GB): 7\n",
      "Epoch 8, Batch 113, Train Loss: 0.26399359107017517, Memory (GB): 7\n",
      "Epoch 8, Batch 114, Train Loss: 0.24481941759586334, Memory (GB): 7\n",
      "Epoch 8, Batch 115, Train Loss: 0.2346528321504593, Memory (GB): 7\n",
      "Epoch 8, Batch 116, Train Loss: 0.23854950070381165, Memory (GB): 7\n",
      "Epoch 8, Batch 117, Train Loss: 0.2291237711906433, Memory (GB): 7\n",
      "Epoch 8, Batch 118, Train Loss: 0.23591023683547974, Memory (GB): 7\n",
      "Epoch 8, Batch 119, Train Loss: 0.2667125165462494, Memory (GB): 7\n",
      "Epoch 8, Batch 120, Train Loss: 0.25634700059890747, Memory (GB): 7\n",
      "Epoch 8, Batch 121, Train Loss: 0.24850229918956757, Memory (GB): 7\n",
      "Epoch 8, Batch 122, Train Loss: 0.25396430492401123, Memory (GB): 7\n",
      "Epoch 8, Batch 123, Train Loss: 0.24173516035079956, Memory (GB): 7\n",
      "Epoch 8, Batch 124, Train Loss: 0.29268887639045715, Memory (GB): 7\n",
      "Epoch 8, Batch 125, Train Loss: 0.2353360801935196, Memory (GB): 7\n",
      "Epoch 8, Batch 126, Train Loss: 0.25327831506729126, Memory (GB): 7\n",
      "Epoch 8, Batch 127, Train Loss: 0.27445143461227417, Memory (GB): 7\n",
      "Epoch 8, Batch 128, Train Loss: 0.2550606429576874, Memory (GB): 7\n",
      "Epoch 8, Batch 129, Train Loss: 0.24347373843193054, Memory (GB): 7\n",
      "Epoch 8, Batch 130, Train Loss: 0.27640384435653687, Memory (GB): 7\n",
      "Epoch 8, Batch 131, Train Loss: 0.26655498147010803, Memory (GB): 7\n",
      "Epoch 8, Batch 132, Train Loss: 0.22639378905296326, Memory (GB): 7\n",
      "Epoch 8, Batch 133, Train Loss: 0.25351738929748535, Memory (GB): 7\n",
      "Epoch 8, Batch 134, Train Loss: 0.275058388710022, Memory (GB): 7\n",
      "Epoch 8, Batch 135, Train Loss: 0.25049853324890137, Memory (GB): 7\n",
      "Epoch 8, Batch 136, Train Loss: 0.22712139785289764, Memory (GB): 7\n",
      "Epoch 8, Batch 137, Train Loss: 0.253573477268219, Memory (GB): 7\n",
      "Epoch 8, Batch 138, Train Loss: 0.23026014864444733, Memory (GB): 7\n",
      "Epoch 8, Batch 139, Train Loss: 0.2531411647796631, Memory (GB): 7\n",
      "Epoch 8, Batch 140, Train Loss: 0.26417556405067444, Memory (GB): 7\n",
      "Epoch 8, Batch 141, Train Loss: 0.23281079530715942, Memory (GB): 7\n",
      "Epoch 8, Batch 142, Train Loss: 0.20949891209602356, Memory (GB): 7\n",
      "Epoch 8, Batch 143, Train Loss: 0.24781087040901184, Memory (GB): 7\n",
      "Epoch 8, Batch 144, Train Loss: 0.2721948027610779, Memory (GB): 7\n",
      "Epoch 8, Batch 145, Train Loss: 0.2766803205013275, Memory (GB): 7\n",
      "Epoch 8, Batch 146, Train Loss: 0.25959697365760803, Memory (GB): 7\n",
      "Epoch 8, Batch 147, Train Loss: 0.29334044456481934, Memory (GB): 7\n",
      "Epoch 8, Batch 148, Train Loss: 0.2522262930870056, Memory (GB): 7\n",
      "Epoch 8, Batch 149, Train Loss: 0.24861812591552734, Memory (GB): 7\n",
      "Epoch 8, Batch 150, Train Loss: 0.2427549809217453, Memory (GB): 7\n",
      "Epoch 8, Batch 151, Train Loss: 0.2669200301170349, Memory (GB): 7\n",
      "Epoch 8, Batch 152, Train Loss: 0.2550155520439148, Memory (GB): 7\n",
      "Epoch 8, Batch 153, Train Loss: 0.2824772298336029, Memory (GB): 7\n",
      "Epoch 8, Batch 154, Train Loss: 0.2603183388710022, Memory (GB): 7\n",
      "Epoch 8, Batch 155, Train Loss: 0.2839621305465698, Memory (GB): 7\n",
      "Epoch 8, Batch 156, Train Loss: 0.2516028881072998, Memory (GB): 7\n",
      "Epoch 8, Batch 157, Train Loss: 0.2779881954193115, Memory (GB): 7\n",
      "Epoch 8, Batch 158, Train Loss: 0.2770560681819916, Memory (GB): 7\n",
      "Epoch 8, Batch 159, Train Loss: 0.2858305275440216, Memory (GB): 7\n",
      "Epoch 8, Batch 160, Train Loss: 0.27723821997642517, Memory (GB): 7\n",
      "Epoch 8, Batch 161, Train Loss: 0.24166570603847504, Memory (GB): 7\n",
      "Epoch 8, Batch 162, Train Loss: 0.2748870849609375, Memory (GB): 7\n",
      "Epoch 8, Batch 163, Train Loss: 0.24881821870803833, Memory (GB): 7\n",
      "Epoch 8, Batch 164, Train Loss: 0.25811687111854553, Memory (GB): 7\n",
      "Epoch 8, Batch 165, Train Loss: 0.25800973176956177, Memory (GB): 7\n",
      "Epoch 8, Batch 166, Train Loss: 0.2646629810333252, Memory (GB): 7\n",
      "Epoch 8, Batch 167, Train Loss: 0.23668427765369415, Memory (GB): 7\n",
      "Epoch 8, Batch 168, Train Loss: 0.27688294649124146, Memory (GB): 7\n",
      "Epoch 8, Batch 169, Train Loss: 0.261799693107605, Memory (GB): 7\n",
      "Epoch 8, Batch 170, Train Loss: 0.2749054729938507, Memory (GB): 7\n",
      "Epoch 8, Batch 171, Train Loss: 0.264769971370697, Memory (GB): 7\n",
      "Epoch 8, Batch 172, Train Loss: 0.29340070486068726, Memory (GB): 7\n",
      "Epoch 8, Batch 173, Train Loss: 0.25902965664863586, Memory (GB): 7\n",
      "Epoch 8, Batch 174, Train Loss: 0.261338472366333, Memory (GB): 7\n",
      "Epoch 8, Batch 175, Train Loss: 0.27431902289390564, Memory (GB): 7\n",
      "Epoch 8, Batch 176, Train Loss: 0.25637689232826233, Memory (GB): 7\n",
      "Epoch 8, Batch 177, Train Loss: 0.27660757303237915, Memory (GB): 7\n",
      "Epoch 8, Batch 178, Train Loss: 0.24079664051532745, Memory (GB): 7\n",
      "Epoch 8, Batch 179, Train Loss: 0.2820490002632141, Memory (GB): 7\n",
      "[0.00023672318332494807, 0.00022488702415870054, 0.00021364267295076564, 0.00020296053930322726, 0.00019281251233806585, 0.00018317188672116257, 0.00017401329238510434, 0.00016531262776584924, 0.00015704699637755667, 0.00014919464655867892, 0.00014173491423074495, 0.00013464816851920775, 0.00012791576009324725, 0.00012151997208858495, 0.00011544397348415569, 0.00010967177480994788, 0.00010418818606945054, 9.897877676597798e-05, 9.402983792767906e-05, 8.932834603129515e-05, 8.486192872973033e-05, 8.061883229324377e-05, 7.658789067858165e-05, 7.275849614465253e-05, 6.912057133741991e-05, 6.566454277054887e-05, 6.238131563202147e-05, 5.926224985042033e-05, 5.6299137357899324e-05, 5.348418049000436e-05, 5.080997146550417e-05, 4.8269472892228944e-05, 4.585599924761751e-05, 4.35631992852366e-05, 4.138503932097477e-05, 3.931578735492601e-05, 3.734999798717975e-05, 3.548249808782074e-05, 3.3708373183429684e-05, 3.202295452425822e-05, 3.0421806798045305e-05, 2.8900716458143048e-05, 2.745568063523589e-05, 2.6082896603474108e-05, 2.4778751773300393e-05, 2.3539814184635354e-05, 2.2362823475403586e-05, 2.124468230163341e-05, 2.0182448186551732e-05, 1.917332577722416e-05, 1.821465948836294e-05, 1.7303926513944793e-05, 1.6438730188247558e-05, 1.5616793678835182e-05, 1.4835953994893422e-05, 1.4094156295148752e-05, 1.3389448480391311e-05, 1.2719976056371745e-05, 1.2083977253553167e-05, 1.1479778390875494e-05, 1.0905789471331727e-05, 1.0360499997765133e-05, 9.842474997876872e-06, 9.350351247983034e-06, 8.882833685583883e-06, 8.43869200130468e-06, 8.016757401239453e-06, 7.615919531177475e-06, 7.235123554618608e-06, 6.873367376887671e-06, 6.529699008043286e-06, 6.203214057641124e-06, 5.893053354759064e-06, 5.598400687021112e-06, 5.318480652670059e-06, 5.052556620036555e-06, 4.7999287890347255e-06, 4.559932349582992e-06, 4.331935732103838e-06, 4.115338945498649e-06, 3.90957199822372e-06, 3.714093398312531e-06, 3.5283887283969067e-06, 3.3519692919770575e-06, 3.1843708273782075e-06, 3.0251522860092977e-06, 2.873894671708832e-06, 2.730199938123389e-06, 2.5936899412172196e-06, 2.4640054441563575e-06, 2.3408051719485387e-06, 2.223764913351113e-06, 2.1125766676835574e-06, 2.00694783429938e-06, 1.9066004425844106e-06, 1.8112704204551902e-06, 1.7207068994324317e-06, 1.6346715544608094e-06, 1.5529379767377688e-06, 1.4752910779008797e-06, 1.4015265240058351e-06, 1.3314501978055442e-06, 1.2648776879152676e-06, 1.2016338035195041e-06, 1.141552113343528e-06, 1.0844745076763515e-06, 1.0302507822925341e-06, 9.787382431779072e-07, 9.298013310190121e-07, 8.833112644680613e-07, 8.39145701244658e-07, 7.971884161824256e-07, 7.573289953733044e-07, 7.194625456046385e-07, 6.834894183244068e-07, 6.493149474081867e-07, 6.168492000377777e-07, 5.860067400358883e-07, 5.567064030340936e-07, 5.288710828823888e-07, 5.024275287382695e-07, 4.773061523013558e-07, 4.534408446862883e-07, 4.307688024519736e-07, 4.09230362329375e-07, 3.8876884421290654e-07, 3.693304020022608e-07, 3.508638819021478e-07, 3.3332068780704047e-07, 3.166546534166885e-07, 3.0082192074585406e-07, 2.8578082470856104e-07, 2.71491783473133e-07, 2.5791719429947655e-07, 2.450213345845027e-07, 2.3277026785527765e-07, 2.2113175446251374e-07, 2.1007516673938808e-07, 1.9957140840241882e-07, 1.8959283798229766e-07, 1.8011319608318278e-07, 1.7110753627902352e-07, 1.6255215946507245e-07, 1.5442455149181877e-07, 1.4670332391722786e-07, 1.393681577213666e-07, 1.323997498352981e-07, 1.2577976234353323e-07, 1.194907742263565e-07, 1.1351623551503865e-07, 1.0784042373928677e-07, 1.0244840255232241e-07, 9.732598242470631e-08, 9.245968330347092e-08, 8.783669913829739e-08, 8.34448641813825e-08, 7.927262097231343e-08, 7.530898992369776e-08, 7.154354042751286e-08, 6.796636340613718e-08, 6.456804523583035e-08, 6.13396429740388e-08, 5.827266082533691e-08, 5.535902778407001e-08, 5.259107639486649e-08, 4.996152257512317e-08, 4.7463446446367017e-08, 4.509027412404865e-08, 4.283576041784622e-08, 4.06939723969539e-08, 3.8659273777106215e-08, 3.672631008825092e-08, 3.488999458383838e-08, 3.3145494854646445e-08, 3.1488220111914137e-08, 2.991380910631842e-08, 2.8418118651002482e-08, 2.6997212718452375e-08, 2.5647352082529756e-08, 2.436498447840327e-08, 2.3146735254483102e-08, 2.1989398491758954e-08, 2.0889928567170992e-08, 1.9845432138812453e-08, 1.8853160531871835e-08, 1.7910502505278235e-08, 1.7014977380014307e-08, 1.6164228511013586e-08, 1.5356017085462912e-08, 1.4588216231189764e-08, 1.3858805419630276e-08, 1.3165865148648765e-08, 1.2507571891216326e-08, 1.1882193296655514e-08, 1.1288083631822732e-08, 1.0723679450231591e-08, 1.0187495477720007e-08, 9.678120703834015e-09, 9.194214668642306e-09]\n",
      "Epoch 9, Batch 0, Train Loss: 0.2463553547859192, Memory (GB): 7\n",
      "Epoch 9, Batch 1, Train Loss: 0.23432084918022156, Memory (GB): 7\n",
      "Epoch 9, Batch 2, Train Loss: 0.27258753776550293, Memory (GB): 7\n",
      "Epoch 9, Batch 3, Train Loss: 0.22600333392620087, Memory (GB): 7\n",
      "Epoch 9, Batch 4, Train Loss: 0.2677725851535797, Memory (GB): 7\n",
      "Epoch 9, Batch 5, Train Loss: 0.24648544192314148, Memory (GB): 7\n",
      "Epoch 9, Batch 6, Train Loss: 0.24348028004169464, Memory (GB): 7\n",
      "Epoch 9, Batch 7, Train Loss: 0.23931071162223816, Memory (GB): 7\n",
      "Epoch 9, Batch 8, Train Loss: 0.2515636086463928, Memory (GB): 7\n",
      "Epoch 9, Batch 9, Train Loss: 0.23619642853736877, Memory (GB): 7\n",
      "Epoch 9, Batch 10, Train Loss: 0.25073865056037903, Memory (GB): 7\n",
      "Epoch 9, Batch 11, Train Loss: 0.24121874570846558, Memory (GB): 7\n",
      "Epoch 9, Batch 12, Train Loss: 0.233988419175148, Memory (GB): 7\n",
      "Epoch 9, Batch 13, Train Loss: 0.25645360350608826, Memory (GB): 7\n",
      "Epoch 9, Batch 14, Train Loss: 0.25335997343063354, Memory (GB): 7\n",
      "Epoch 9, Batch 15, Train Loss: 0.24466973543167114, Memory (GB): 7\n",
      "Epoch 9, Batch 16, Train Loss: 0.2581947445869446, Memory (GB): 7\n",
      "Epoch 9, Batch 17, Train Loss: 0.2436273843050003, Memory (GB): 7\n",
      "Epoch 9, Batch 18, Train Loss: 0.22263695299625397, Memory (GB): 7\n",
      "Epoch 9, Batch 19, Train Loss: 0.2228376269340515, Memory (GB): 7\n",
      "Epoch 9, Batch 20, Train Loss: 0.2666642665863037, Memory (GB): 7\n",
      "Epoch 9, Batch 21, Train Loss: 0.23286797106266022, Memory (GB): 7\n",
      "Epoch 9, Batch 22, Train Loss: 0.23299725353717804, Memory (GB): 7\n",
      "Epoch 9, Batch 23, Train Loss: 0.23877684772014618, Memory (GB): 7\n",
      "Epoch 9, Batch 24, Train Loss: 0.24422180652618408, Memory (GB): 7\n",
      "Epoch 9, Batch 25, Train Loss: 0.26281851530075073, Memory (GB): 7\n",
      "Epoch 9, Batch 26, Train Loss: 0.26554352045059204, Memory (GB): 7\n",
      "Epoch 9, Batch 27, Train Loss: 0.2582828402519226, Memory (GB): 7\n",
      "Epoch 9, Batch 28, Train Loss: 0.22822588682174683, Memory (GB): 7\n",
      "Epoch 9, Batch 29, Train Loss: 0.2645445168018341, Memory (GB): 7\n",
      "Epoch 9, Batch 30, Train Loss: 0.23153036832809448, Memory (GB): 7\n",
      "Epoch 9, Batch 31, Train Loss: 0.2473769187927246, Memory (GB): 7\n",
      "Epoch 9, Batch 32, Train Loss: 0.24162787199020386, Memory (GB): 7\n",
      "Epoch 9, Batch 33, Train Loss: 0.22961117327213287, Memory (GB): 7\n",
      "Epoch 9, Batch 34, Train Loss: 0.24651432037353516, Memory (GB): 7\n",
      "Epoch 9, Batch 35, Train Loss: 0.24748633801937103, Memory (GB): 7\n",
      "Epoch 9, Batch 36, Train Loss: 0.2446555644273758, Memory (GB): 7\n",
      "Epoch 9, Batch 37, Train Loss: 0.2598305940628052, Memory (GB): 7\n",
      "Epoch 9, Batch 38, Train Loss: 0.22566349804401398, Memory (GB): 7\n",
      "Epoch 9, Batch 39, Train Loss: 0.22653788328170776, Memory (GB): 7\n",
      "Epoch 9, Batch 40, Train Loss: 0.20964254438877106, Memory (GB): 7\n",
      "Epoch 9, Batch 41, Train Loss: 0.2343670278787613, Memory (GB): 7\n",
      "Epoch 9, Batch 42, Train Loss: 0.2336300015449524, Memory (GB): 7\n",
      "Epoch 9, Batch 43, Train Loss: 0.2510770559310913, Memory (GB): 7\n",
      "Epoch 9, Batch 44, Train Loss: 0.23590871691703796, Memory (GB): 7\n",
      "Epoch 9, Batch 45, Train Loss: 0.25155559182167053, Memory (GB): 7\n",
      "Epoch 9, Batch 46, Train Loss: 0.2355731576681137, Memory (GB): 7\n",
      "Epoch 9, Batch 47, Train Loss: 0.26235270500183105, Memory (GB): 7\n",
      "Epoch 9, Batch 48, Train Loss: 0.251902312040329, Memory (GB): 7\n",
      "Epoch 9, Batch 49, Train Loss: 0.22314298152923584, Memory (GB): 7\n",
      "Epoch 9, Batch 50, Train Loss: 0.2506203353404999, Memory (GB): 7\n",
      "Epoch 9, Batch 51, Train Loss: 0.22770507633686066, Memory (GB): 7\n",
      "Epoch 9, Batch 52, Train Loss: 0.23579034209251404, Memory (GB): 7\n",
      "Epoch 9, Batch 53, Train Loss: 0.21708045899868011, Memory (GB): 7\n",
      "Epoch 9, Batch 54, Train Loss: 0.225803941488266, Memory (GB): 7\n",
      "Epoch 9, Batch 55, Train Loss: 0.284231036901474, Memory (GB): 7\n",
      "Epoch 9, Batch 56, Train Loss: 0.2171817570924759, Memory (GB): 7\n",
      "Epoch 9, Batch 57, Train Loss: 0.26277369260787964, Memory (GB): 7\n",
      "Epoch 9, Batch 58, Train Loss: 0.22894872725009918, Memory (GB): 7\n",
      "Epoch 9, Batch 59, Train Loss: 0.20638854801654816, Memory (GB): 7\n",
      "Epoch 9, Batch 60, Train Loss: 0.21871943771839142, Memory (GB): 7\n",
      "Epoch 9, Batch 61, Train Loss: 0.24392764270305634, Memory (GB): 7\n",
      "Epoch 9, Batch 62, Train Loss: 0.228130042552948, Memory (GB): 7\n",
      "Epoch 9, Batch 63, Train Loss: 0.23877933621406555, Memory (GB): 7\n",
      "Epoch 9, Batch 64, Train Loss: 0.27108100056648254, Memory (GB): 7\n",
      "Epoch 9, Batch 65, Train Loss: 0.2418866902589798, Memory (GB): 7\n",
      "Epoch 9, Batch 66, Train Loss: 0.2643362879753113, Memory (GB): 7\n",
      "Epoch 9, Batch 67, Train Loss: 0.23661349713802338, Memory (GB): 7\n",
      "Epoch 9, Batch 68, Train Loss: 0.24349230527877808, Memory (GB): 7\n",
      "Epoch 9, Batch 69, Train Loss: 0.24918511509895325, Memory (GB): 7\n",
      "Epoch 9, Batch 70, Train Loss: 0.2882360816001892, Memory (GB): 7\n",
      "Epoch 9, Batch 71, Train Loss: 0.2748921811580658, Memory (GB): 7\n",
      "Epoch 9, Batch 72, Train Loss: 0.27041390538215637, Memory (GB): 7\n",
      "Epoch 9, Batch 73, Train Loss: 0.2692701816558838, Memory (GB): 7\n",
      "Epoch 9, Batch 74, Train Loss: 0.24142292141914368, Memory (GB): 7\n",
      "Epoch 9, Batch 75, Train Loss: 0.24776476621627808, Memory (GB): 7\n",
      "Epoch 9, Batch 76, Train Loss: 0.23140880465507507, Memory (GB): 7\n",
      "Epoch 9, Batch 77, Train Loss: 0.2579396665096283, Memory (GB): 7\n",
      "Epoch 9, Batch 78, Train Loss: 0.2474420815706253, Memory (GB): 7\n",
      "Epoch 9, Batch 79, Train Loss: 0.27113014459609985, Memory (GB): 7\n",
      "Epoch 9, Batch 80, Train Loss: 0.24256375432014465, Memory (GB): 7\n",
      "Epoch 9, Batch 81, Train Loss: 0.24972465634346008, Memory (GB): 7\n",
      "Epoch 9, Batch 82, Train Loss: 0.23630011081695557, Memory (GB): 7\n",
      "Epoch 9, Batch 83, Train Loss: 0.22458688914775848, Memory (GB): 7\n",
      "Epoch 9, Batch 84, Train Loss: 0.24923188984394073, Memory (GB): 7\n",
      "Epoch 9, Batch 85, Train Loss: 0.27470263838768005, Memory (GB): 7\n",
      "Epoch 9, Batch 86, Train Loss: 0.2805069386959076, Memory (GB): 7\n",
      "Epoch 9, Batch 87, Train Loss: 0.26473841071128845, Memory (GB): 7\n",
      "Epoch 9, Batch 88, Train Loss: 0.2511301636695862, Memory (GB): 7\n",
      "Epoch 9, Batch 89, Train Loss: 0.2654989957809448, Memory (GB): 7\n",
      "Epoch 9, Batch 90, Train Loss: 0.2721588611602783, Memory (GB): 7\n",
      "Epoch 9, Batch 91, Train Loss: 0.2923716604709625, Memory (GB): 7\n",
      "Epoch 9, Batch 92, Train Loss: 0.25600165128707886, Memory (GB): 7\n",
      "Epoch 9, Batch 93, Train Loss: 0.23293821513652802, Memory (GB): 7\n",
      "Epoch 9, Batch 94, Train Loss: 0.2546550929546356, Memory (GB): 7\n",
      "Epoch 9, Batch 95, Train Loss: 0.23153890669345856, Memory (GB): 7\n",
      "Epoch 9, Batch 96, Train Loss: 0.25587114691734314, Memory (GB): 7\n",
      "Epoch 9, Batch 97, Train Loss: 0.24311888217926025, Memory (GB): 7\n",
      "Epoch 9, Batch 98, Train Loss: 0.2492590695619583, Memory (GB): 7\n",
      "Epoch 9, Batch 99, Train Loss: 0.24806901812553406, Memory (GB): 7\n",
      "Epoch 9, Batch 100, Train Loss: 0.26578521728515625, Memory (GB): 7\n",
      "Epoch 9, Batch 101, Train Loss: 0.2551896572113037, Memory (GB): 7\n",
      "Epoch 9, Batch 102, Train Loss: 0.26870182156562805, Memory (GB): 7\n",
      "Epoch 9, Batch 103, Train Loss: 0.2554377019405365, Memory (GB): 7\n",
      "Epoch 9, Batch 104, Train Loss: 0.25076591968536377, Memory (GB): 7\n",
      "Epoch 9, Batch 105, Train Loss: 0.2557466924190521, Memory (GB): 7\n",
      "Epoch 9, Batch 106, Train Loss: 0.30204102396965027, Memory (GB): 7\n",
      "Epoch 9, Batch 107, Train Loss: 0.25431856513023376, Memory (GB): 7\n",
      "Epoch 9, Batch 108, Train Loss: 0.24872234463691711, Memory (GB): 7\n",
      "Epoch 9, Batch 109, Train Loss: 0.24051718413829803, Memory (GB): 7\n",
      "Epoch 9, Batch 110, Train Loss: 0.25356605648994446, Memory (GB): 7\n",
      "Epoch 9, Batch 111, Train Loss: 0.27029862999916077, Memory (GB): 7\n",
      "Epoch 9, Batch 112, Train Loss: 0.2214077264070511, Memory (GB): 7\n",
      "Epoch 9, Batch 113, Train Loss: 0.2626744210720062, Memory (GB): 7\n",
      "Epoch 9, Batch 114, Train Loss: 0.24952229857444763, Memory (GB): 7\n",
      "Epoch 9, Batch 115, Train Loss: 0.2234037220478058, Memory (GB): 7\n",
      "Epoch 9, Batch 116, Train Loss: 0.2710530757904053, Memory (GB): 7\n",
      "Epoch 9, Batch 117, Train Loss: 0.22570623457431793, Memory (GB): 7\n",
      "Epoch 9, Batch 118, Train Loss: 0.22718684375286102, Memory (GB): 7\n",
      "Epoch 9, Batch 119, Train Loss: 0.24554561078548431, Memory (GB): 7\n",
      "Epoch 9, Batch 120, Train Loss: 0.23415490984916687, Memory (GB): 7\n",
      "Epoch 9, Batch 121, Train Loss: 0.22344398498535156, Memory (GB): 7\n",
      "Epoch 9, Batch 122, Train Loss: 0.2593505084514618, Memory (GB): 7\n",
      "Epoch 9, Batch 123, Train Loss: 0.23657144606113434, Memory (GB): 7\n",
      "Epoch 9, Batch 124, Train Loss: 0.24851353466510773, Memory (GB): 7\n",
      "Epoch 9, Batch 125, Train Loss: 0.2669440805912018, Memory (GB): 7\n",
      "Epoch 9, Batch 126, Train Loss: 0.2403729259967804, Memory (GB): 7\n",
      "Epoch 9, Batch 127, Train Loss: 0.23219476640224457, Memory (GB): 7\n",
      "Epoch 9, Batch 128, Train Loss: 0.2573447823524475, Memory (GB): 7\n",
      "Epoch 9, Batch 129, Train Loss: 0.2564444839954376, Memory (GB): 7\n",
      "Epoch 9, Batch 130, Train Loss: 0.2325810194015503, Memory (GB): 7\n",
      "Epoch 9, Batch 131, Train Loss: 0.2378932386636734, Memory (GB): 7\n",
      "Epoch 9, Batch 132, Train Loss: 0.2296598106622696, Memory (GB): 7\n",
      "Epoch 9, Batch 133, Train Loss: 0.2318294644355774, Memory (GB): 7\n",
      "Epoch 9, Batch 134, Train Loss: 0.24037787318229675, Memory (GB): 7\n",
      "Epoch 9, Batch 135, Train Loss: 0.2331300973892212, Memory (GB): 7\n",
      "Epoch 9, Batch 136, Train Loss: 0.22409287095069885, Memory (GB): 7\n",
      "Epoch 9, Batch 137, Train Loss: 0.23130151629447937, Memory (GB): 7\n",
      "Epoch 9, Batch 138, Train Loss: 0.23988983035087585, Memory (GB): 7\n",
      "Epoch 9, Batch 139, Train Loss: 0.2445131540298462, Memory (GB): 7\n",
      "Epoch 9, Batch 140, Train Loss: 0.25017479062080383, Memory (GB): 7\n",
      "Epoch 9, Batch 141, Train Loss: 0.2522392272949219, Memory (GB): 7\n",
      "Epoch 9, Batch 142, Train Loss: 0.25202181935310364, Memory (GB): 7\n",
      "Epoch 9, Batch 143, Train Loss: 0.2505672872066498, Memory (GB): 7\n",
      "Epoch 9, Batch 144, Train Loss: 0.2567894756793976, Memory (GB): 7\n",
      "Epoch 9, Batch 145, Train Loss: 0.2366437017917633, Memory (GB): 7\n",
      "Epoch 9, Batch 146, Train Loss: 0.2429477572441101, Memory (GB): 7\n",
      "Epoch 9, Batch 147, Train Loss: 0.22029520571231842, Memory (GB): 7\n",
      "Epoch 9, Batch 148, Train Loss: 0.2318693846464157, Memory (GB): 7\n",
      "Epoch 9, Batch 149, Train Loss: 0.23123736679553986, Memory (GB): 7\n",
      "Epoch 9, Batch 150, Train Loss: 0.28319278359413147, Memory (GB): 7\n",
      "Epoch 9, Batch 151, Train Loss: 0.20448191463947296, Memory (GB): 7\n",
      "Epoch 9, Batch 152, Train Loss: 0.2311168611049652, Memory (GB): 7\n",
      "Epoch 9, Batch 153, Train Loss: 0.2582453191280365, Memory (GB): 7\n",
      "Epoch 9, Batch 154, Train Loss: 0.24175938963890076, Memory (GB): 7\n",
      "Epoch 9, Batch 155, Train Loss: 0.23111321032047272, Memory (GB): 7\n",
      "Epoch 9, Batch 156, Train Loss: 0.2389901876449585, Memory (GB): 7\n",
      "Epoch 9, Batch 157, Train Loss: 0.2649897336959839, Memory (GB): 7\n",
      "Epoch 9, Batch 158, Train Loss: 0.2399841994047165, Memory (GB): 7\n",
      "Epoch 9, Batch 159, Train Loss: 0.24679088592529297, Memory (GB): 7\n",
      "Epoch 9, Batch 160, Train Loss: 0.2683730721473694, Memory (GB): 7\n",
      "Epoch 9, Batch 161, Train Loss: 0.21882584691047668, Memory (GB): 7\n",
      "Epoch 9, Batch 162, Train Loss: 0.2787322402000427, Memory (GB): 7\n",
      "Epoch 9, Batch 163, Train Loss: 0.23401901125907898, Memory (GB): 7\n",
      "Epoch 9, Batch 164, Train Loss: 0.23288333415985107, Memory (GB): 7\n",
      "Epoch 9, Batch 165, Train Loss: 0.2351570576429367, Memory (GB): 7\n",
      "Epoch 9, Batch 166, Train Loss: 0.2361496239900589, Memory (GB): 7\n",
      "Epoch 9, Batch 167, Train Loss: 0.2544204890727997, Memory (GB): 7\n",
      "Epoch 9, Batch 168, Train Loss: 0.25982657074928284, Memory (GB): 7\n",
      "Epoch 9, Batch 169, Train Loss: 0.22960445284843445, Memory (GB): 7\n",
      "Epoch 9, Batch 170, Train Loss: 0.24873292446136475, Memory (GB): 7\n",
      "Epoch 9, Batch 171, Train Loss: 0.29136255383491516, Memory (GB): 7\n",
      "Epoch 9, Batch 172, Train Loss: 0.22895662486553192, Memory (GB): 7\n",
      "Epoch 9, Batch 173, Train Loss: 0.2571689188480377, Memory (GB): 7\n",
      "Epoch 9, Batch 174, Train Loss: 0.24879589676856995, Memory (GB): 7\n",
      "Epoch 9, Batch 175, Train Loss: 0.2774536609649658, Memory (GB): 7\n",
      "Epoch 9, Batch 176, Train Loss: 0.28640684485435486, Memory (GB): 7\n",
      "Epoch 9, Batch 177, Train Loss: 0.24743981659412384, Memory (GB): 7\n",
      "Epoch 9, Batch 178, Train Loss: 0.23015113174915314, Memory (GB): 7\n",
      "Epoch 9, Batch 179, Train Loss: 0.2659112513065338, Memory (GB): 7\n",
      "[0.0004517258629314915, 0.00042913956978491645, 0.0004076825912956709, 0.00038729846173088747, 0.00036793353864434297, 0.0003495368617121256, 0.00033206001862651934, 0.00031545701769519345, 0.00029968416681043383, 0.00028469995846991213, 0.00027046496054641654, 0.0002569417125190955, 0.0002440946268931408, 0.00023188989554848367, 0.0002202954007710596, 0.0002092806307325065, 0.00019881659919588116, 0.00018887576923608712, 0.00017943198077428267, 0.00017046038173556863, 0.00016193736264879017, 0.0001538404945163508, 0.0001461484697905332, 0.0001388410463010065, 0.00013189899398595607, 0.00012530404428665826, 0.00011903884207232529, 0.00011308689996870907, 0.0001074325549702736, 0.0001020609272217599, 9.695788086067192e-05, 9.210998681763831e-05, 8.750448747675641e-05, 8.312926310291861e-05, 7.897279994777267e-05, 7.502415995038405e-05, 7.127295195286482e-05, 6.770930435522157e-05, 6.432383913746043e-05, 6.110764718058748e-05, 5.805226482155807e-05, 5.514965158048017e-05, 5.239216900145617e-05, 4.9772560551383336e-05, 4.728393252381416e-05, 4.491973589762348e-05, 4.267374910274231e-05, 4.0540061647605154e-05, 3.8513058565224903e-05, 3.6587405636963655e-05, 3.4758035355115464e-05, 3.302013358735971e-05, 3.136912690799173e-05, 2.980067056259214e-05, 2.8310637034462532e-05, 2.6895105182739426e-05, 2.555034992360243e-05, 2.4272832427422302e-05, 2.305919080605119e-05, 2.1906231265748637e-05, 2.081091970246119e-05, 1.9770373717338124e-05, 1.8781855031471222e-05, 1.7842762279897664e-05, 1.6950624165902786e-05, 1.6103092957607643e-05, 1.5297938309727242e-05, 1.4533041394240887e-05, 1.3806389324528836e-05, 1.3116069858302404e-05, 1.2460266365387283e-05, 1.1837253047117911e-05, 1.1245390394762021e-05, 1.068312087502392e-05, 1.0148964831272724e-05, 9.641516589709084e-06, 9.159440760223629e-06, 8.701468722212445e-06, 8.266395286101826e-06, 7.853075521796732e-06, 7.460421745706898e-06, 7.087400658421553e-06, 6.733030625500476e-06, 6.3963790942254506e-06, 6.0765601395141786e-06, 5.772732132538471e-06, 5.484095525911543e-06, 5.209890749615969e-06, 4.9493962121351736e-06, 4.701926401528407e-06, 4.466830081451987e-06, 4.243488577379392e-06, 4.0313141485104194e-06, 3.8297484410849004e-06, 3.638261019030654e-06, 3.456347968079123e-06, 3.283530569675166e-06, 3.119354041191408e-06, 2.963386339131835e-06, 2.8152170221752476e-06, 2.674456171066482e-06, 2.5407333625131586e-06, 2.4136966943875e-06, 2.293011859668126e-06, 2.178361266684719e-06, 2.0694432033504825e-06, 1.9659710431829587e-06, 1.867672491023812e-06, 1.7742888664726202e-06, 1.6855744231489883e-06, 1.6012957019915395e-06, 1.5212309168919628e-06, 1.4451693710473639e-06, 1.3729109024949959e-06, 1.3042653573702467e-06, 1.2390520895017333e-06, 1.1770994850266469e-06, 1.1182445107753153e-06, 1.062332285236548e-06, 1.0092156709747216e-06, 9.587548874259851e-07, 9.108171430546862e-07, 8.652762859019513e-07, 8.220124716068538e-07, 7.809118480265112e-07, 7.418662556251856e-07, 7.047729428439263e-07, 6.695342957017298e-07, 6.360575809166433e-07, 6.04254701870811e-07, 5.740419667772708e-07, 5.453398684384072e-07, 5.18072875016487e-07, 4.921692312656627e-07, 4.6756076970237917e-07, 4.4418273121726004e-07, 4.2197359465639704e-07, 4.0087491492357736e-07, 3.808311691773985e-07, 3.617896107185285e-07, 3.437001301826022e-07, 3.2651512367347196e-07, 3.1018936748979837e-07, 2.946798991153084e-07, 2.799459041595431e-07, 2.6594860895156587e-07, 2.526511785039877e-07, 2.4001861957878817e-07, 2.2801768859984857e-07, 2.1661680416985614e-07, 2.0578596396136338e-07, 1.9549666576329527e-07, 1.8572183247513048e-07, 1.76435740851374e-07, 1.676139538088052e-07, 1.5923325611836492e-07, 1.5127159331244667e-07, 1.4370801364682435e-07, 1.365226129644832e-07, 1.2969648231625905e-07, 1.2321165820044601e-07, 1.1705107529042364e-07, 1.1119852152590249e-07, 1.0563859544960741e-07, 1.0035666567712702e-07, 9.533883239327068e-08, 9.057189077360711e-08, 8.604329623492675e-08, 8.17411314231804e-08, 7.765407485202138e-08, 7.37713711094203e-08, 7.008280255394927e-08, 6.657866242625184e-08, 6.324972930493924e-08, 6.008724283969229e-08, 5.708288069770765e-08, 5.4228736662822284e-08, 5.1517299829681137e-08, 4.894143483819709e-08, 4.649436309628726e-08, 4.416964494147288e-08, 4.1961162694399244e-08, 3.986310455967928e-08, 3.786994933169532e-08, 3.597645186511052e-08, 3.4177629271855e-08, 3.246874780826226e-08, 3.0845310417849144e-08, 2.9303044896956674e-08, 2.7837892652108835e-08, 2.64459980195034e-08, 2.512369811852822e-08, 2.3867513212601815e-08, 2.267413755197172e-08, 2.1540430674373124e-08, 2.046340914065448e-08, 1.9440238683621757e-08, 1.8468226749440666e-08, 1.7544815411968622e-08]\n",
      "Epoch 10, Batch 0, Train Loss: 0.2617030441761017, Memory (GB): 7\n",
      "Epoch 10, Batch 1, Train Loss: 0.2422419935464859, Memory (GB): 7\n",
      "Epoch 10, Batch 2, Train Loss: 0.2486911565065384, Memory (GB): 7\n",
      "Epoch 10, Batch 3, Train Loss: 0.25051960349082947, Memory (GB): 7\n",
      "Epoch 10, Batch 4, Train Loss: 0.30725082755088806, Memory (GB): 7\n",
      "Epoch 10, Batch 5, Train Loss: 0.2158510386943817, Memory (GB): 7\n",
      "Epoch 10, Batch 6, Train Loss: 0.25718870759010315, Memory (GB): 7\n",
      "Epoch 10, Batch 7, Train Loss: 0.2364715188741684, Memory (GB): 7\n",
      "Epoch 10, Batch 8, Train Loss: 0.2488681524991989, Memory (GB): 7\n",
      "Epoch 10, Batch 9, Train Loss: 0.26933836936950684, Memory (GB): 7\n",
      "Epoch 10, Batch 10, Train Loss: 0.2542326748371124, Memory (GB): 7\n",
      "Epoch 10, Batch 11, Train Loss: 0.24452799558639526, Memory (GB): 7\n",
      "Epoch 10, Batch 12, Train Loss: 0.2432384043931961, Memory (GB): 7\n",
      "Epoch 10, Batch 13, Train Loss: 0.2543756365776062, Memory (GB): 7\n",
      "Epoch 10, Batch 14, Train Loss: 0.2518616020679474, Memory (GB): 7\n",
      "Epoch 10, Batch 15, Train Loss: 0.23484385013580322, Memory (GB): 7\n",
      "Epoch 10, Batch 16, Train Loss: 0.266316682100296, Memory (GB): 7\n",
      "Epoch 10, Batch 17, Train Loss: 0.24937161803245544, Memory (GB): 7\n",
      "Epoch 10, Batch 18, Train Loss: 0.23851941525936127, Memory (GB): 7\n",
      "Epoch 10, Batch 19, Train Loss: 0.23715516924858093, Memory (GB): 7\n",
      "Epoch 10, Batch 20, Train Loss: 0.2552047371864319, Memory (GB): 7\n",
      "Epoch 10, Batch 21, Train Loss: 0.23026829957962036, Memory (GB): 7\n",
      "Epoch 10, Batch 22, Train Loss: 0.26021820306777954, Memory (GB): 7\n",
      "Epoch 10, Batch 23, Train Loss: 0.22235362231731415, Memory (GB): 7\n",
      "Epoch 10, Batch 24, Train Loss: 0.23929837346076965, Memory (GB): 7\n",
      "Epoch 10, Batch 25, Train Loss: 0.23469790816307068, Memory (GB): 7\n",
      "Epoch 10, Batch 26, Train Loss: 0.21974456310272217, Memory (GB): 7\n",
      "Epoch 10, Batch 27, Train Loss: 0.24668237566947937, Memory (GB): 7\n",
      "Epoch 10, Batch 28, Train Loss: 0.250092089176178, Memory (GB): 7\n",
      "Epoch 10, Batch 29, Train Loss: 0.22350670397281647, Memory (GB): 7\n",
      "Epoch 10, Batch 30, Train Loss: 0.21914620697498322, Memory (GB): 7\n",
      "Epoch 10, Batch 31, Train Loss: 0.22906282544136047, Memory (GB): 7\n",
      "Epoch 10, Batch 32, Train Loss: 0.2493291199207306, Memory (GB): 7\n",
      "Epoch 10, Batch 33, Train Loss: 0.19896285235881805, Memory (GB): 7\n",
      "Epoch 10, Batch 34, Train Loss: 0.2560966908931732, Memory (GB): 7\n",
      "Epoch 10, Batch 35, Train Loss: 0.22613407671451569, Memory (GB): 7\n",
      "Epoch 10, Batch 36, Train Loss: 0.22747299075126648, Memory (GB): 7\n",
      "Epoch 10, Batch 37, Train Loss: 0.21971671283245087, Memory (GB): 7\n",
      "Epoch 10, Batch 38, Train Loss: 0.247893288731575, Memory (GB): 7\n",
      "Epoch 10, Batch 39, Train Loss: 0.22912243008613586, Memory (GB): 7\n",
      "Epoch 10, Batch 40, Train Loss: 0.1949464976787567, Memory (GB): 7\n",
      "Epoch 10, Batch 41, Train Loss: 0.20598652958869934, Memory (GB): 7\n",
      "Epoch 10, Batch 42, Train Loss: 0.23145072162151337, Memory (GB): 7\n",
      "Epoch 10, Batch 43, Train Loss: 0.19655683636665344, Memory (GB): 7\n",
      "Epoch 10, Batch 44, Train Loss: 0.22999727725982666, Memory (GB): 7\n",
      "Epoch 10, Batch 45, Train Loss: 0.24052134156227112, Memory (GB): 7\n",
      "Epoch 10, Batch 46, Train Loss: 0.24792666733264923, Memory (GB): 7\n",
      "Epoch 10, Batch 47, Train Loss: 0.2355976551771164, Memory (GB): 7\n",
      "Epoch 10, Batch 48, Train Loss: 0.23576484620571136, Memory (GB): 7\n",
      "Epoch 10, Batch 49, Train Loss: 0.2242344468832016, Memory (GB): 7\n",
      "Epoch 10, Batch 50, Train Loss: 0.21462996304035187, Memory (GB): 7\n",
      "Epoch 10, Batch 51, Train Loss: 0.23990565538406372, Memory (GB): 7\n",
      "Epoch 10, Batch 52, Train Loss: 0.23438003659248352, Memory (GB): 7\n",
      "Epoch 10, Batch 53, Train Loss: 0.2367689311504364, Memory (GB): 7\n",
      "Epoch 10, Batch 54, Train Loss: 0.21106521785259247, Memory (GB): 7\n",
      "Epoch 10, Batch 55, Train Loss: 0.24417349696159363, Memory (GB): 7\n",
      "Epoch 10, Batch 56, Train Loss: 0.22715428471565247, Memory (GB): 7\n",
      "Epoch 10, Batch 57, Train Loss: 0.2571626901626587, Memory (GB): 7\n",
      "Epoch 10, Batch 58, Train Loss: 0.24278172850608826, Memory (GB): 7\n",
      "Epoch 10, Batch 59, Train Loss: 0.21488744020462036, Memory (GB): 7\n",
      "Epoch 10, Batch 60, Train Loss: 0.22188854217529297, Memory (GB): 7\n",
      "Epoch 10, Batch 61, Train Loss: 0.22969916462898254, Memory (GB): 7\n",
      "Epoch 10, Batch 62, Train Loss: 0.2536945641040802, Memory (GB): 7\n",
      "Epoch 10, Batch 63, Train Loss: 0.21673353016376495, Memory (GB): 7\n",
      "Epoch 10, Batch 64, Train Loss: 0.2207169234752655, Memory (GB): 7\n",
      "Epoch 10, Batch 65, Train Loss: 0.24510005116462708, Memory (GB): 7\n",
      "Epoch 10, Batch 66, Train Loss: 0.22168195247650146, Memory (GB): 7\n",
      "Epoch 10, Batch 67, Train Loss: 0.2388710230588913, Memory (GB): 7\n",
      "Epoch 10, Batch 68, Train Loss: 0.20245984196662903, Memory (GB): 7\n",
      "Epoch 10, Batch 69, Train Loss: 0.22337289154529572, Memory (GB): 7\n",
      "Epoch 10, Batch 70, Train Loss: 0.2194930911064148, Memory (GB): 7\n",
      "Epoch 10, Batch 71, Train Loss: 0.24174407124519348, Memory (GB): 7\n",
      "Epoch 10, Batch 72, Train Loss: 0.2565191388130188, Memory (GB): 7\n",
      "Epoch 10, Batch 73, Train Loss: 0.23239964246749878, Memory (GB): 7\n",
      "Epoch 10, Batch 74, Train Loss: 0.23815123736858368, Memory (GB): 7\n",
      "Epoch 10, Batch 75, Train Loss: 0.22459879517555237, Memory (GB): 7\n",
      "Epoch 10, Batch 76, Train Loss: 0.19889003038406372, Memory (GB): 7\n",
      "Epoch 10, Batch 77, Train Loss: 0.2687434256076813, Memory (GB): 7\n",
      "Epoch 10, Batch 78, Train Loss: 0.23712080717086792, Memory (GB): 7\n",
      "Epoch 10, Batch 79, Train Loss: 0.22827009856700897, Memory (GB): 7\n",
      "Epoch 10, Batch 80, Train Loss: 0.21677330136299133, Memory (GB): 7\n",
      "Epoch 10, Batch 81, Train Loss: 0.2443886697292328, Memory (GB): 7\n",
      "Epoch 10, Batch 82, Train Loss: 0.20659133791923523, Memory (GB): 7\n",
      "Epoch 10, Batch 83, Train Loss: 0.26567041873931885, Memory (GB): 7\n",
      "Epoch 10, Batch 84, Train Loss: 0.22642609477043152, Memory (GB): 7\n",
      "Epoch 10, Batch 85, Train Loss: 0.19926109910011292, Memory (GB): 7\n",
      "Epoch 10, Batch 86, Train Loss: 0.23705820739269257, Memory (GB): 7\n",
      "Epoch 10, Batch 87, Train Loss: 0.24392005801200867, Memory (GB): 7\n",
      "Epoch 10, Batch 88, Train Loss: 0.2384248673915863, Memory (GB): 7\n",
      "Epoch 10, Batch 89, Train Loss: 0.23072728514671326, Memory (GB): 7\n",
      "Epoch 10, Batch 90, Train Loss: 0.23887106776237488, Memory (GB): 7\n",
      "Epoch 10, Batch 91, Train Loss: 0.2245807647705078, Memory (GB): 7\n",
      "Epoch 10, Batch 92, Train Loss: 0.25429201126098633, Memory (GB): 7\n",
      "Epoch 10, Batch 93, Train Loss: 0.2519446015357971, Memory (GB): 7\n",
      "Epoch 10, Batch 94, Train Loss: 0.21734273433685303, Memory (GB): 7\n",
      "Epoch 10, Batch 95, Train Loss: 0.24721018970012665, Memory (GB): 7\n",
      "Epoch 10, Batch 96, Train Loss: 0.23171229660511017, Memory (GB): 7\n",
      "Epoch 10, Batch 97, Train Loss: 0.22106559574604034, Memory (GB): 7\n",
      "Epoch 10, Batch 98, Train Loss: 0.26549044251441956, Memory (GB): 7\n",
      "Epoch 10, Batch 99, Train Loss: 0.24690133333206177, Memory (GB): 7\n",
      "Epoch 10, Batch 100, Train Loss: 0.2518666386604309, Memory (GB): 7\n",
      "Epoch 10, Batch 101, Train Loss: 0.23586386442184448, Memory (GB): 7\n",
      "Epoch 10, Batch 102, Train Loss: 0.24171997606754303, Memory (GB): 7\n",
      "Epoch 10, Batch 103, Train Loss: 0.22674043476581573, Memory (GB): 7\n",
      "Epoch 10, Batch 104, Train Loss: 0.2645106613636017, Memory (GB): 7\n",
      "Epoch 10, Batch 105, Train Loss: 0.21631765365600586, Memory (GB): 7\n",
      "Epoch 10, Batch 106, Train Loss: 0.279304563999176, Memory (GB): 7\n",
      "Epoch 10, Batch 107, Train Loss: 0.2195824831724167, Memory (GB): 7\n",
      "Epoch 10, Batch 108, Train Loss: 0.24098318815231323, Memory (GB): 7\n",
      "Epoch 10, Batch 109, Train Loss: 0.24915826320648193, Memory (GB): 7\n",
      "Epoch 10, Batch 110, Train Loss: 0.24800413846969604, Memory (GB): 7\n",
      "Epoch 10, Batch 111, Train Loss: 0.2222771942615509, Memory (GB): 7\n",
      "Epoch 10, Batch 112, Train Loss: 0.243586927652359, Memory (GB): 7\n",
      "Epoch 10, Batch 113, Train Loss: 0.24097269773483276, Memory (GB): 7\n",
      "Epoch 10, Batch 114, Train Loss: 0.24334189295768738, Memory (GB): 7\n",
      "Epoch 10, Batch 115, Train Loss: 0.22299520671367645, Memory (GB): 7\n",
      "Epoch 10, Batch 116, Train Loss: 0.21575720608234406, Memory (GB): 7\n",
      "Epoch 10, Batch 117, Train Loss: 0.23018044233322144, Memory (GB): 7\n",
      "Epoch 10, Batch 118, Train Loss: 0.2457553744316101, Memory (GB): 7\n",
      "Epoch 10, Batch 119, Train Loss: 0.22273142635822296, Memory (GB): 7\n",
      "Epoch 10, Batch 120, Train Loss: 0.2441052943468094, Memory (GB): 7\n",
      "Epoch 10, Batch 121, Train Loss: 0.23365537822246552, Memory (GB): 7\n",
      "Epoch 10, Batch 122, Train Loss: 0.2348642796278, Memory (GB): 7\n",
      "Epoch 10, Batch 123, Train Loss: 0.2607894837856293, Memory (GB): 7\n",
      "Epoch 10, Batch 124, Train Loss: 0.2499111294746399, Memory (GB): 7\n",
      "Epoch 10, Batch 125, Train Loss: 0.23302902281284332, Memory (GB): 7\n",
      "Epoch 10, Batch 126, Train Loss: 0.27303218841552734, Memory (GB): 7\n",
      "Epoch 10, Batch 127, Train Loss: 0.22849278151988983, Memory (GB): 7\n",
      "Epoch 10, Batch 128, Train Loss: 0.2639648914337158, Memory (GB): 7\n",
      "Epoch 10, Batch 129, Train Loss: 0.23938429355621338, Memory (GB): 7\n",
      "Epoch 10, Batch 130, Train Loss: 0.23431456089019775, Memory (GB): 7\n",
      "Epoch 10, Batch 131, Train Loss: 0.21355970203876495, Memory (GB): 7\n",
      "Epoch 10, Batch 132, Train Loss: 0.22713924944400787, Memory (GB): 7\n",
      "Epoch 10, Batch 133, Train Loss: 0.22832000255584717, Memory (GB): 7\n",
      "Epoch 10, Batch 134, Train Loss: 0.22973164916038513, Memory (GB): 7\n",
      "Epoch 10, Batch 135, Train Loss: 0.22588594257831573, Memory (GB): 7\n",
      "Epoch 10, Batch 136, Train Loss: 0.26858049631118774, Memory (GB): 7\n",
      "Epoch 10, Batch 137, Train Loss: 0.23416364192962646, Memory (GB): 7\n",
      "Epoch 10, Batch 138, Train Loss: 0.23623330891132355, Memory (GB): 7\n",
      "Epoch 10, Batch 139, Train Loss: 0.2515859007835388, Memory (GB): 7\n",
      "Epoch 10, Batch 140, Train Loss: 0.22887668013572693, Memory (GB): 7\n",
      "Epoch 10, Batch 141, Train Loss: 0.23894448578357697, Memory (GB): 7\n",
      "Epoch 10, Batch 142, Train Loss: 0.24797700345516205, Memory (GB): 7\n",
      "Epoch 10, Batch 143, Train Loss: 0.24417786300182343, Memory (GB): 7\n",
      "Epoch 10, Batch 144, Train Loss: 0.25077611207962036, Memory (GB): 7\n",
      "Epoch 10, Batch 145, Train Loss: 0.2661859095096588, Memory (GB): 7\n",
      "Epoch 10, Batch 146, Train Loss: 0.2599146068096161, Memory (GB): 7\n",
      "Epoch 10, Batch 147, Train Loss: 0.2438393533229828, Memory (GB): 7\n",
      "Epoch 10, Batch 148, Train Loss: 0.22531543672084808, Memory (GB): 7\n",
      "Epoch 10, Batch 149, Train Loss: 0.21766510605812073, Memory (GB): 7\n",
      "Epoch 10, Batch 150, Train Loss: 0.23065684735774994, Memory (GB): 7\n",
      "Epoch 10, Batch 151, Train Loss: 0.23426589369773865, Memory (GB): 7\n",
      "Epoch 10, Batch 152, Train Loss: 0.23307180404663086, Memory (GB): 7\n",
      "Epoch 10, Batch 153, Train Loss: 0.2374950796365738, Memory (GB): 7\n",
      "Epoch 10, Batch 154, Train Loss: 0.21395376324653625, Memory (GB): 7\n",
      "Epoch 10, Batch 155, Train Loss: 0.21476246416568756, Memory (GB): 7\n",
      "Epoch 10, Batch 156, Train Loss: 0.20587430894374847, Memory (GB): 7\n",
      "Epoch 10, Batch 157, Train Loss: 0.2624262273311615, Memory (GB): 7\n",
      "Epoch 10, Batch 158, Train Loss: 0.22268156707286835, Memory (GB): 7\n",
      "Epoch 10, Batch 159, Train Loss: 0.20198886096477509, Memory (GB): 7\n",
      "Epoch 10, Batch 160, Train Loss: 0.22189556062221527, Memory (GB): 7\n",
      "Epoch 10, Batch 161, Train Loss: 0.24996502697467804, Memory (GB): 7\n",
      "Epoch 10, Batch 162, Train Loss: 0.23525628447532654, Memory (GB): 7\n",
      "Epoch 10, Batch 163, Train Loss: 0.21173390746116638, Memory (GB): 7\n",
      "Epoch 10, Batch 164, Train Loss: 0.21479180455207825, Memory (GB): 7\n",
      "Epoch 10, Batch 165, Train Loss: 0.2412090301513672, Memory (GB): 7\n",
      "Epoch 10, Batch 166, Train Loss: 0.24428708851337433, Memory (GB): 7\n",
      "Epoch 10, Batch 167, Train Loss: 0.21892061829566956, Memory (GB): 7\n",
      "Epoch 10, Batch 168, Train Loss: 0.25249183177948, Memory (GB): 7\n",
      "Epoch 10, Batch 169, Train Loss: 0.23750188946723938, Memory (GB): 7\n",
      "Epoch 10, Batch 170, Train Loss: 0.2153472900390625, Memory (GB): 7\n",
      "Epoch 10, Batch 171, Train Loss: 0.22240911424160004, Memory (GB): 7\n",
      "Epoch 10, Batch 172, Train Loss: 0.24039411544799805, Memory (GB): 7\n",
      "Epoch 10, Batch 173, Train Loss: 0.2167498767375946, Memory (GB): 7\n",
      "Epoch 10, Batch 174, Train Loss: 0.23189476132392883, Memory (GB): 7\n",
      "Epoch 10, Batch 175, Train Loss: 0.23486509919166565, Memory (GB): 7\n",
      "Epoch 10, Batch 176, Train Loss: 0.22762984037399292, Memory (GB): 7\n",
      "Epoch 10, Batch 177, Train Loss: 0.21936936676502228, Memory (GB): 7\n",
      "Epoch 10, Batch 178, Train Loss: 0.2533857524394989, Memory (GB): 7\n",
      "Epoch 10, Batch 179, Train Loss: 0.25815850496292114, Memory (GB): 7\n",
      "[0.00035459364805116156, 0.00033686396564860347, 0.00032002076736617333, 0.0003040197289978647, 0.0002888187425479713, 0.0002743778054205728, 0.00026065891514954413, 0.0002476259693920668, 0.00023524467092246346, 0.00022348243737634028, 0.00021230831550752324, 0.0002016928997321471, 0.0001916082547455396, 0.0001820278420082627, 0.00017292644990784964, 0.00016428012741245704, 0.00015606612104183425, 0.00014826281498974255, 0.00014084967424025542, 0.00013380719052824263, 0.0001271168310018305, 0.00012076098945173893, 0.000114722939979152, 0.0001089867929801944, 0.00010353745333118465, 9.836058066462545e-05, 9.344255163139412e-05, 8.877042404982442e-05, 8.43319028473332e-05, 8.011530770496652e-05, 7.610954231971819e-05, 7.23040652037323e-05, 6.868886194354566e-05, 6.525441884636838e-05, 6.199169790404995e-05, 5.889211300884748e-05, 5.594750735840509e-05, 5.31501319904848e-05, 5.049262539096058e-05, 4.796799412141254e-05, 4.55695944153419e-05, 4.329111469457484e-05, 4.112655895984606e-05, 3.907023101185376e-05, 3.711671946126108e-05, 3.526088348819802e-05, 3.3497839313788115e-05, 3.1822947348098705e-05, 3.0231799980693765e-05, 2.8720209981659078e-05, 2.7284199482576123e-05, 2.5919989508447328e-05, 2.4623990033024957e-05, 2.33927905313737e-05, 2.2223151004805032e-05, 2.1111993454564762e-05, 2.0056393781836527e-05, 1.905357409274469e-05, 1.8100895388107462e-05, 1.7195850618702086e-05, 1.633605808776698e-05, 1.5519255183378634e-05, 1.4743292424209699e-05, 1.4006127802999213e-05, 1.3305821412849259e-05, 1.264053034220679e-05, 1.2008503825096446e-05, 1.1408078633841627e-05, 1.0837674702149547e-05, 1.0295790967042066e-05, 9.78100141868996e-06, 9.291951347755463e-06, 8.827353780367688e-06, 8.385986091349307e-06, 7.966686786781842e-06, 7.568352447442749e-06, 7.189934825070609e-06, 6.830438083817078e-06, 6.4889161796262235e-06, 6.164470370644913e-06, 5.856246852112667e-06, 5.563434509507032e-06, 5.285262784031682e-06, 5.020999644830098e-06, 4.769949662588593e-06, 4.5314521794591656e-06, 4.304879570486203e-06, 4.089635591961895e-06, 3.885153812363801e-06, 3.69089612174561e-06, 3.506351315658328e-06, 3.331033749875414e-06, 3.1644820623816416e-06, 3.00625795926256e-06, 2.855945061299431e-06, 2.7131478082344614e-06, 2.577490417822736e-06, 2.4486158969316e-06, 2.3261851020850195e-06, 2.209875846980768e-06, 2.09938205463173e-06, 1.994412951900143e-06, 1.8946923043051352e-06, 1.7999576890898804e-06, 1.7099598046353868e-06, 1.6244618144036161e-06, 1.543238723683435e-06, 1.4660767874992632e-06, 1.3927729481243003e-06, 1.323134300718086e-06, 1.2569775856821807e-06, 1.1941287063980717e-06, 1.1344222710781685e-06, 1.0777011575242593e-06, 1.0238160996480466e-06, 9.726252946656445e-07, 9.239940299323617e-07, 8.777943284357439e-07, 8.339046120139566e-07, 7.922093814132591e-07, 7.525989123425959e-07, 7.149689667254657e-07, 6.792205183891926e-07, 6.452594924697327e-07, 6.129965178462462e-07, 5.823466919539336e-07, 5.532293573562371e-07, 5.255678894884251e-07, 4.992894950140039e-07, 4.7432502026330377e-07, 4.5060876925013843e-07, 4.280783307876316e-07, 4.066744142482501e-07, 3.8634069353583757e-07, 3.670236588590457e-07, 3.4867247591609323e-07, 3.3123885212028875e-07, 3.1467690951427417e-07, 2.9894306403856043e-07, 2.839959108366323e-07, 2.697961152948008e-07, 2.563063095300607e-07, 2.434909940535577e-07, 2.3131644435087985e-07, 2.1975062213333582e-07, 2.0876309102666908e-07, 1.983249364753355e-07, 1.8840868965156878e-07, 1.7898825516899037e-07, 1.700388424105408e-07, 1.615369002900137e-07, 1.5346005527551304e-07, 1.457870525117374e-07, 1.3849769988615046e-07, 1.31572814891843e-07, 1.249941741472508e-07, 1.1874446543988826e-07, 1.1280724216789391e-07, 1.0716688005949918e-07, 1.0180853605652424e-07, 9.671810925369798e-08, 9.188220379101308e-08, 8.728809360146243e-08, 8.292368892138929e-08, 7.87775044753198e-08, 7.483862925155382e-08, 7.109669778897614e-08, 6.754186289952732e-08, 6.41647697545509e-08, 6.095653126682337e-08, 5.790870470348219e-08, 5.5013269468308106e-08, 5.226260599489272e-08, 4.964947569514806e-08, 4.7167001910390646e-08, 4.480865181487112e-08, 4.256821922412757e-08, 4.043980826292117e-08, 3.8417817849775135e-08, 3.649692695728637e-08, 3.467208060942205e-08, 3.2938476578950943e-08, 3.1291552750003394e-08, 2.9726975112503238e-08, 2.824062635687807e-08, 2.6828595039034155e-08, 2.5487165287082453e-08, 2.4212807022728324e-08, 2.300216667159192e-08, 2.1852058338012307e-08, 2.0759455421111698e-08, 1.9721482650056104e-08, 1.87354085175533e-08, 1.779863809167563e-08, 1.690870618709185e-08, 1.6063270877737256e-08, 1.5260107333850393e-08, 1.449710196715788e-08, 1.3772246868799978e-08]\n",
      "Epoch 11, Batch 0, Train Loss: 0.20979830622673035, Memory (GB): 7\n",
      "Epoch 11, Batch 1, Train Loss: 0.20915739238262177, Memory (GB): 7\n",
      "Epoch 11, Batch 2, Train Loss: 0.1912560909986496, Memory (GB): 7\n",
      "Epoch 11, Batch 3, Train Loss: 0.23626145720481873, Memory (GB): 7\n",
      "Epoch 11, Batch 4, Train Loss: 0.20702974498271942, Memory (GB): 7\n",
      "Epoch 11, Batch 5, Train Loss: 0.1866093873977661, Memory (GB): 7\n",
      "Epoch 11, Batch 6, Train Loss: 0.23152385652065277, Memory (GB): 7\n",
      "Epoch 11, Batch 7, Train Loss: 0.22066719830036163, Memory (GB): 7\n",
      "Epoch 11, Batch 8, Train Loss: 0.22932298481464386, Memory (GB): 7\n",
      "Epoch 11, Batch 9, Train Loss: 0.22381769120693207, Memory (GB): 7\n",
      "Epoch 11, Batch 10, Train Loss: 0.2210587114095688, Memory (GB): 7\n",
      "Epoch 11, Batch 11, Train Loss: 0.2524721622467041, Memory (GB): 7\n",
      "Epoch 11, Batch 12, Train Loss: 0.21572545170783997, Memory (GB): 7\n",
      "Epoch 11, Batch 13, Train Loss: 0.20258712768554688, Memory (GB): 7\n",
      "Epoch 11, Batch 14, Train Loss: 0.21363718807697296, Memory (GB): 7\n",
      "Epoch 11, Batch 15, Train Loss: 0.21278081834316254, Memory (GB): 7\n",
      "Epoch 11, Batch 16, Train Loss: 0.20864859223365784, Memory (GB): 7\n",
      "Epoch 11, Batch 17, Train Loss: 0.23273538053035736, Memory (GB): 7\n",
      "Epoch 11, Batch 18, Train Loss: 0.2098313271999359, Memory (GB): 7\n",
      "Epoch 11, Batch 19, Train Loss: 0.2334129959344864, Memory (GB): 7\n",
      "Epoch 11, Batch 20, Train Loss: 0.26843878626823425, Memory (GB): 7\n",
      "Epoch 11, Batch 21, Train Loss: 0.24041961133480072, Memory (GB): 7\n",
      "Epoch 11, Batch 22, Train Loss: 0.23500020802021027, Memory (GB): 7\n",
      "Epoch 11, Batch 23, Train Loss: 0.24841585755348206, Memory (GB): 7\n",
      "Epoch 11, Batch 24, Train Loss: 0.2505182921886444, Memory (GB): 7\n",
      "Epoch 11, Batch 25, Train Loss: 0.21498775482177734, Memory (GB): 7\n",
      "Epoch 11, Batch 26, Train Loss: 0.23173107206821442, Memory (GB): 7\n",
      "Epoch 11, Batch 27, Train Loss: 0.22208908200263977, Memory (GB): 7\n",
      "Epoch 11, Batch 28, Train Loss: 0.2155105471611023, Memory (GB): 7\n",
      "Epoch 11, Batch 29, Train Loss: 0.2339908480644226, Memory (GB): 7\n",
      "Epoch 11, Batch 30, Train Loss: 0.24198418855667114, Memory (GB): 7\n",
      "Epoch 11, Batch 31, Train Loss: 0.2459922581911087, Memory (GB): 7\n",
      "Epoch 11, Batch 32, Train Loss: 0.29343071579933167, Memory (GB): 7\n",
      "Epoch 11, Batch 33, Train Loss: 0.20706719160079956, Memory (GB): 7\n",
      "Epoch 11, Batch 34, Train Loss: 0.21884307265281677, Memory (GB): 7\n",
      "Epoch 11, Batch 35, Train Loss: 0.2376037836074829, Memory (GB): 7\n",
      "Epoch 11, Batch 36, Train Loss: 0.2357885241508484, Memory (GB): 7\n",
      "Epoch 11, Batch 37, Train Loss: 0.24060878157615662, Memory (GB): 7\n",
      "Epoch 11, Batch 38, Train Loss: 0.20621690154075623, Memory (GB): 7\n",
      "Epoch 11, Batch 39, Train Loss: 0.20354986190795898, Memory (GB): 7\n",
      "Epoch 11, Batch 40, Train Loss: 0.20204448699951172, Memory (GB): 7\n",
      "Epoch 11, Batch 41, Train Loss: 0.2365310788154602, Memory (GB): 7\n",
      "Epoch 11, Batch 42, Train Loss: 0.22115738689899445, Memory (GB): 7\n",
      "Epoch 11, Batch 43, Train Loss: 0.22672788798809052, Memory (GB): 7\n",
      "Epoch 11, Batch 44, Train Loss: 0.20948421955108643, Memory (GB): 7\n",
      "Epoch 11, Batch 45, Train Loss: 0.23418955504894257, Memory (GB): 7\n",
      "Epoch 11, Batch 46, Train Loss: 0.22661127150058746, Memory (GB): 7\n",
      "Epoch 11, Batch 47, Train Loss: 0.2382776141166687, Memory (GB): 7\n",
      "Epoch 11, Batch 48, Train Loss: 0.21434447169303894, Memory (GB): 7\n",
      "Epoch 11, Batch 49, Train Loss: 0.21617357432842255, Memory (GB): 7\n",
      "Epoch 11, Batch 50, Train Loss: 0.22985580563545227, Memory (GB): 7\n",
      "Epoch 11, Batch 51, Train Loss: 0.2473156750202179, Memory (GB): 7\n",
      "Epoch 11, Batch 52, Train Loss: 0.19953645765781403, Memory (GB): 7\n",
      "Epoch 11, Batch 53, Train Loss: 0.19483302533626556, Memory (GB): 7\n",
      "Epoch 11, Batch 54, Train Loss: 0.24388499557971954, Memory (GB): 7\n",
      "Epoch 11, Batch 55, Train Loss: 0.2466767579317093, Memory (GB): 7\n",
      "Epoch 11, Batch 56, Train Loss: 0.22709183394908905, Memory (GB): 7\n",
      "Epoch 11, Batch 57, Train Loss: 0.2167074978351593, Memory (GB): 7\n",
      "Epoch 11, Batch 58, Train Loss: 0.21441996097564697, Memory (GB): 7\n",
      "Epoch 11, Batch 59, Train Loss: 0.20775409042835236, Memory (GB): 7\n",
      "Epoch 11, Batch 60, Train Loss: 0.21822687983512878, Memory (GB): 7\n",
      "Epoch 11, Batch 61, Train Loss: 0.22356010973453522, Memory (GB): 7\n",
      "Epoch 11, Batch 62, Train Loss: 0.2094666212797165, Memory (GB): 7\n",
      "Epoch 11, Batch 63, Train Loss: 0.23364298045635223, Memory (GB): 7\n",
      "Epoch 11, Batch 64, Train Loss: 0.23333457112312317, Memory (GB): 7\n",
      "Epoch 11, Batch 65, Train Loss: 0.20853926241397858, Memory (GB): 7\n",
      "Epoch 11, Batch 66, Train Loss: 0.2594269812107086, Memory (GB): 7\n",
      "Epoch 11, Batch 67, Train Loss: 0.21901080012321472, Memory (GB): 7\n",
      "Epoch 11, Batch 68, Train Loss: 0.2605134844779968, Memory (GB): 7\n",
      "Epoch 11, Batch 69, Train Loss: 0.24804694950580597, Memory (GB): 7\n",
      "Epoch 11, Batch 70, Train Loss: 0.22089526057243347, Memory (GB): 7\n",
      "Epoch 11, Batch 71, Train Loss: 0.2309427559375763, Memory (GB): 7\n",
      "Epoch 11, Batch 72, Train Loss: 0.21471911668777466, Memory (GB): 7\n",
      "Epoch 11, Batch 73, Train Loss: 0.22580377757549286, Memory (GB): 7\n",
      "Epoch 11, Batch 74, Train Loss: 0.21642480790615082, Memory (GB): 7\n",
      "Epoch 11, Batch 75, Train Loss: 0.2265857458114624, Memory (GB): 7\n",
      "Epoch 11, Batch 76, Train Loss: 0.226632758975029, Memory (GB): 7\n",
      "Epoch 11, Batch 77, Train Loss: 0.1954450160264969, Memory (GB): 7\n",
      "Epoch 11, Batch 78, Train Loss: 0.21470637619495392, Memory (GB): 7\n",
      "Epoch 11, Batch 79, Train Loss: 0.21472212672233582, Memory (GB): 7\n",
      "Epoch 11, Batch 80, Train Loss: 0.2610112428665161, Memory (GB): 7\n",
      "Epoch 11, Batch 81, Train Loss: 0.2607010006904602, Memory (GB): 7\n",
      "Epoch 11, Batch 82, Train Loss: 0.19590575993061066, Memory (GB): 7\n",
      "Epoch 11, Batch 83, Train Loss: 0.2098757028579712, Memory (GB): 7\n",
      "Epoch 11, Batch 84, Train Loss: 0.20489402115345, Memory (GB): 7\n",
      "Epoch 11, Batch 85, Train Loss: 0.22972042858600616, Memory (GB): 7\n",
      "Epoch 11, Batch 86, Train Loss: 0.22933019697666168, Memory (GB): 7\n",
      "Epoch 11, Batch 87, Train Loss: 0.21561609208583832, Memory (GB): 7\n",
      "Epoch 11, Batch 88, Train Loss: 0.20461256802082062, Memory (GB): 7\n",
      "Epoch 11, Batch 89, Train Loss: 0.20812754333019257, Memory (GB): 7\n",
      "Epoch 11, Batch 90, Train Loss: 0.2251673936843872, Memory (GB): 7\n",
      "Epoch 11, Batch 91, Train Loss: 0.21512052416801453, Memory (GB): 7\n",
      "Epoch 11, Batch 92, Train Loss: 0.23223522305488586, Memory (GB): 7\n",
      "Epoch 11, Batch 93, Train Loss: 0.2094613015651703, Memory (GB): 7\n",
      "Epoch 11, Batch 94, Train Loss: 0.2166922688484192, Memory (GB): 7\n",
      "Epoch 11, Batch 95, Train Loss: 0.21414630115032196, Memory (GB): 7\n",
      "Epoch 11, Batch 96, Train Loss: 0.22646890580654144, Memory (GB): 7\n",
      "Epoch 11, Batch 97, Train Loss: 0.22122237086296082, Memory (GB): 7\n",
      "Epoch 11, Batch 98, Train Loss: 0.24240252375602722, Memory (GB): 7\n",
      "Epoch 11, Batch 99, Train Loss: 0.24505619704723358, Memory (GB): 7\n",
      "Epoch 11, Batch 100, Train Loss: 0.2660253942012787, Memory (GB): 7\n",
      "Epoch 11, Batch 101, Train Loss: 0.20787890255451202, Memory (GB): 7\n",
      "Epoch 11, Batch 102, Train Loss: 0.23097671568393707, Memory (GB): 7\n",
      "Epoch 11, Batch 103, Train Loss: 0.20262813568115234, Memory (GB): 7\n",
      "Epoch 11, Batch 104, Train Loss: 0.20879454910755157, Memory (GB): 7\n",
      "Epoch 11, Batch 105, Train Loss: 0.23605601489543915, Memory (GB): 7\n",
      "Epoch 11, Batch 106, Train Loss: 0.20201140642166138, Memory (GB): 7\n",
      "Epoch 11, Batch 107, Train Loss: 0.22946837544441223, Memory (GB): 7\n",
      "Epoch 11, Batch 108, Train Loss: 0.24652612209320068, Memory (GB): 7\n",
      "Epoch 11, Batch 109, Train Loss: 0.24175158143043518, Memory (GB): 7\n",
      "Epoch 11, Batch 110, Train Loss: 0.22165393829345703, Memory (GB): 7\n",
      "Epoch 11, Batch 111, Train Loss: 0.2068484127521515, Memory (GB): 7\n",
      "Epoch 11, Batch 112, Train Loss: 0.24215875566005707, Memory (GB): 7\n",
      "Epoch 11, Batch 113, Train Loss: 0.26373380422592163, Memory (GB): 7\n",
      "Epoch 11, Batch 114, Train Loss: 0.22273728251457214, Memory (GB): 7\n",
      "Epoch 11, Batch 115, Train Loss: 0.22200614213943481, Memory (GB): 7\n",
      "Epoch 11, Batch 116, Train Loss: 0.22072800993919373, Memory (GB): 7\n",
      "Epoch 11, Batch 117, Train Loss: 0.22224444150924683, Memory (GB): 7\n",
      "Epoch 11, Batch 118, Train Loss: 0.22965872287750244, Memory (GB): 7\n",
      "Epoch 11, Batch 119, Train Loss: 0.22681662440299988, Memory (GB): 7\n",
      "Epoch 11, Batch 120, Train Loss: 0.23051674664020538, Memory (GB): 7\n",
      "Epoch 11, Batch 121, Train Loss: 0.2262835055589676, Memory (GB): 7\n",
      "Epoch 11, Batch 122, Train Loss: 0.2236086130142212, Memory (GB): 7\n",
      "Epoch 11, Batch 123, Train Loss: 0.21885642409324646, Memory (GB): 7\n",
      "Epoch 11, Batch 124, Train Loss: 0.23873837292194366, Memory (GB): 7\n",
      "Epoch 11, Batch 125, Train Loss: 0.24047179520130157, Memory (GB): 7\n",
      "Epoch 11, Batch 126, Train Loss: 0.24777187407016754, Memory (GB): 7\n",
      "Epoch 11, Batch 127, Train Loss: 0.2493399828672409, Memory (GB): 7\n",
      "Epoch 11, Batch 128, Train Loss: 0.22842568159103394, Memory (GB): 7\n",
      "Epoch 11, Batch 129, Train Loss: 0.24510860443115234, Memory (GB): 7\n",
      "Epoch 11, Batch 130, Train Loss: 0.23091723024845123, Memory (GB): 7\n",
      "Epoch 11, Batch 131, Train Loss: 0.23965322971343994, Memory (GB): 7\n",
      "Epoch 11, Batch 132, Train Loss: 0.23970158398151398, Memory (GB): 7\n",
      "Epoch 11, Batch 133, Train Loss: 0.21220453083515167, Memory (GB): 7\n",
      "Epoch 11, Batch 134, Train Loss: 0.23285263776779175, Memory (GB): 7\n",
      "Epoch 11, Batch 135, Train Loss: 0.1935640573501587, Memory (GB): 7\n",
      "Epoch 11, Batch 136, Train Loss: 0.2251400202512741, Memory (GB): 7\n",
      "Epoch 11, Batch 137, Train Loss: 0.22897520661354065, Memory (GB): 7\n",
      "Epoch 11, Batch 138, Train Loss: 0.2596643567085266, Memory (GB): 7\n",
      "Epoch 11, Batch 139, Train Loss: 0.25560784339904785, Memory (GB): 7\n",
      "Epoch 11, Batch 140, Train Loss: 0.2341674566268921, Memory (GB): 7\n",
      "Epoch 11, Batch 141, Train Loss: 0.2313556671142578, Memory (GB): 7\n",
      "Epoch 11, Batch 142, Train Loss: 0.20567992329597473, Memory (GB): 7\n",
      "Epoch 11, Batch 143, Train Loss: 0.22537218034267426, Memory (GB): 7\n",
      "Epoch 11, Batch 144, Train Loss: 0.22536689043045044, Memory (GB): 7\n",
      "Epoch 11, Batch 145, Train Loss: 0.2477772831916809, Memory (GB): 7\n",
      "Epoch 11, Batch 146, Train Loss: 0.2338937520980835, Memory (GB): 7\n",
      "Epoch 11, Batch 147, Train Loss: 0.22206942737102509, Memory (GB): 7\n",
      "Epoch 11, Batch 148, Train Loss: 0.22632889449596405, Memory (GB): 7\n",
      "Epoch 11, Batch 149, Train Loss: 0.22981126606464386, Memory (GB): 7\n",
      "Epoch 11, Batch 150, Train Loss: 0.2442164421081543, Memory (GB): 7\n",
      "Epoch 11, Batch 151, Train Loss: 0.21154744923114777, Memory (GB): 7\n",
      "Epoch 11, Batch 152, Train Loss: 0.24098582565784454, Memory (GB): 7\n",
      "Epoch 11, Batch 153, Train Loss: 0.22916997969150543, Memory (GB): 7\n",
      "Epoch 11, Batch 154, Train Loss: 0.20747631788253784, Memory (GB): 7\n",
      "Epoch 11, Batch 155, Train Loss: 0.23020823299884796, Memory (GB): 7\n",
      "Epoch 11, Batch 156, Train Loss: 0.23810811340808868, Memory (GB): 7\n",
      "Epoch 11, Batch 157, Train Loss: 0.23875553905963898, Memory (GB): 7\n",
      "Epoch 11, Batch 158, Train Loss: 0.23410624265670776, Memory (GB): 7\n",
      "Epoch 11, Batch 159, Train Loss: 0.21752053499221802, Memory (GB): 7\n",
      "Epoch 11, Batch 160, Train Loss: 0.22371026873588562, Memory (GB): 7\n",
      "Epoch 11, Batch 161, Train Loss: 0.21725057065486908, Memory (GB): 7\n",
      "Epoch 11, Batch 162, Train Loss: 0.25728639960289, Memory (GB): 7\n",
      "Epoch 11, Batch 163, Train Loss: 0.20707662403583527, Memory (GB): 7\n",
      "Epoch 11, Batch 164, Train Loss: 0.21400097012519836, Memory (GB): 7\n",
      "Epoch 11, Batch 165, Train Loss: 0.21429017186164856, Memory (GB): 7\n",
      "Epoch 11, Batch 166, Train Loss: 0.24276147782802582, Memory (GB): 7\n",
      "Epoch 11, Batch 167, Train Loss: 0.22069327533245087, Memory (GB): 7\n",
      "Epoch 11, Batch 168, Train Loss: 0.2340155988931656, Memory (GB): 7\n",
      "Epoch 11, Batch 169, Train Loss: 0.22523902356624603, Memory (GB): 7\n",
      "Epoch 11, Batch 170, Train Loss: 0.20161664485931396, Memory (GB): 7\n",
      "Epoch 11, Batch 171, Train Loss: 0.20512235164642334, Memory (GB): 7\n",
      "Epoch 11, Batch 172, Train Loss: 0.23239457607269287, Memory (GB): 7\n",
      "Epoch 11, Batch 173, Train Loss: 0.22075414657592773, Memory (GB): 7\n",
      "Epoch 11, Batch 174, Train Loss: 0.24150462448596954, Memory (GB): 7\n",
      "Epoch 11, Batch 175, Train Loss: 0.24679358303546906, Memory (GB): 7\n",
      "Epoch 11, Batch 176, Train Loss: 0.22276028990745544, Memory (GB): 7\n",
      "Epoch 11, Batch 177, Train Loss: 0.22175806760787964, Memory (GB): 7\n",
      "Epoch 11, Batch 178, Train Loss: 0.2002943605184555, Memory (GB): 7\n",
      "Epoch 11, Batch 179, Train Loss: 0.22239802777767181, Memory (GB): 7\n",
      "[6.033305055140719e-05, 5.73163980238368e-05, 5.445057812264497e-05, 5.172804921651272e-05, 4.914164675568708e-05, 4.6684564417902704e-05, 4.435033619700758e-05, 4.213281938715719e-05, 4.002617841779934e-05, 3.8024869496909355e-05, 3.6123626022063886e-05, 3.4317444720960704e-05, 3.2601572484912654e-05, 3.0971493860667034e-05, 2.9422919167633673e-05, 2.7951773209251987e-05, 2.6554184548789387e-05, 2.522647532134992e-05, 2.396515155528242e-05, 2.2766893977518296e-05, 2.162854927864238e-05, 2.0547121814710254e-05, 1.951976572397474e-05, 1.8543777437776006e-05, 1.76165885658872e-05, 1.6735759137592843e-05, 1.58989711807132e-05, 1.5104022621677533e-05, 1.4348821490593654e-05, 1.3631380416063974e-05, 1.2949811395260779e-05, 1.2302320825497736e-05, 1.168720478422285e-05, 1.1102844545011708e-05, 1.0547702317761121e-05, 1.0020317201873062e-05, 9.519301341779412e-06, 9.043336274690437e-06, 8.591169460955915e-06, 8.161610987908118e-06, 7.753530438512714e-06, 7.365853916587079e-06, 6.997561220757725e-06, 6.647683159719837e-06, 6.315299001733844e-06, 5.999534051647153e-06, 5.699557349064794e-06, 5.414579481611554e-06, 5.143850507530977e-06, 4.886657982154427e-06, 4.6423250830467054e-06, 4.41020882889437e-06, 4.189698387449652e-06, 3.980213468077168e-06, 3.781202794673311e-06, 3.5921426549396435e-06, 3.412535522192662e-06, 3.241908746083028e-06, 3.0798133087788774e-06, 2.9258226433399335e-06, 2.7795315111729373e-06, 2.6405549356142898e-06, 2.508527188833575e-06, 2.3831008293918955e-06, 2.2639457879223005e-06, 2.150748498526186e-06, 2.043211073599877e-06, 1.941050519919882e-06, 1.8439979939238877e-06, 1.7517980942276939e-06, 1.6642081895163088e-06, 1.580997780040493e-06, 1.501947891038469e-06, 1.4268504964865449e-06, 1.3555079716622175e-06, 1.2877325730791066e-06, 1.2233459444251508e-06, 1.1621786472038938e-06, 1.1040697148436988e-06, 1.0488662291015137e-06, 9.964229176464385e-07, 9.466017717641164e-07, 8.992716831759104e-07, 8.543080990171148e-07, 8.115926940662593e-07, 7.710130593629464e-07, 7.324624063947987e-07, 6.958392860750589e-07, 6.610473217713061e-07, 6.279949556827407e-07, 5.965952078986034e-07, 5.667654475036733e-07, 5.384271751284897e-07, 5.115058163720654e-07, 4.859305255534622e-07, 4.6163399927578894e-07, 4.385522993119994e-07, 4.1662468434639945e-07, 3.957934501290794e-07, 3.760037776226254e-07, 3.5720358874149417e-07, 3.3934340930441946e-07, 3.223762388391984e-07, 3.062574268972385e-07, 2.9094455555237664e-07, 2.763973277747578e-07, 2.625774613860199e-07, 2.494485883167188e-07, 2.3697615890088286e-07, 2.251273509558387e-07, 2.1387098340804675e-07, 2.0317743423764442e-07, 1.9301856252576223e-07, 1.8336763439947408e-07, 1.7419925267950042e-07, 1.6548929004552537e-07, 1.5721482554324908e-07, 1.4935408426608657e-07, 1.4188638005278226e-07, 1.347920610501432e-07, 1.28052457997636e-07, 1.2164983509775416e-07, 1.1556734334286648e-07, 1.0978897617572315e-07, 1.0429952736693697e-07, 9.90845509985901e-08, 9.413032344866061e-08, 8.942380727622758e-08, 8.495261691241619e-08, 8.070498606679539e-08, 7.666973676345559e-08, 7.283624992528281e-08, 6.91944374290187e-08, 6.573471555756776e-08, 6.244797977968936e-08, 5.93255807907049e-08, 5.635930175116963e-08, 5.354133666361116e-08, 5.086426983043059e-08, 4.8321056338909077e-08, 4.590500352196361e-08, 4.3609753345865425e-08, 4.1429265678572145e-08, 3.9357802394643546e-08, 3.738991227491136e-08, 3.55204166611658e-08, 3.3744395828107496e-08, 3.205717603670211e-08, 3.045431723486702e-08, 2.8931601373123667e-08, 2.7485021304467483e-08, 2.6110770239244103e-08, 2.4805231727281894e-08, 2.3564970140917798e-08, 2.238672163387191e-08, 2.1267385552178315e-08, 2.0204016274569395e-08, 1.9193815460840924e-08, 1.823412468779888e-08, 1.7322418453408933e-08, 1.645629753073848e-08, 1.5633482654201556e-08, 1.4851808521491479e-08, 1.4109218095416902e-08, 1.3403757190646059e-08, 1.2733569331113754e-08, 1.2096890864558066e-08, 1.1492046321330165e-08, 1.0917444005263653e-08, 1.0371571805000473e-08, 9.852993214750445e-09, 9.360343554012923e-09, 8.892326376312276e-09, 8.447710057496667e-09, 8.025324554621831e-09, 7.62405832689074e-09, 7.2428554105462006e-09, 6.880712640018892e-09, 6.5366770080179466e-09, 6.209843157617049e-09, 5.899350999736197e-09, 5.604383449749385e-09, 5.324164277261917e-09, 5.05795606339882e-09, 4.80505826022888e-09, 4.564805347217434e-09, 4.336565079856563e-09, 4.119736825863735e-09, 3.913749984570548e-09, 3.71806248534202e-09, 3.5321593610749183e-09, 3.355551393021172e-09, 3.1877738233701134e-09, 3.0283851322016083e-09, 2.8769658755915278e-09, 2.73311758181195e-09, 2.596461702721353e-09, 2.466638617585285e-09, 2.3433066867060206e-09]\n",
      "Epoch 12, Batch 0, Train Loss: 0.183151513338089, Memory (GB): 7\n",
      "Epoch 12, Batch 1, Train Loss: 0.1908361315727234, Memory (GB): 7\n",
      "Epoch 12, Batch 2, Train Loss: 0.21059677004814148, Memory (GB): 7\n",
      "Epoch 12, Batch 3, Train Loss: 0.22870653867721558, Memory (GB): 7\n",
      "Epoch 12, Batch 4, Train Loss: 0.19783809781074524, Memory (GB): 7\n",
      "Epoch 12, Batch 5, Train Loss: 0.20034772157669067, Memory (GB): 7\n",
      "Epoch 12, Batch 6, Train Loss: 0.19902637600898743, Memory (GB): 7\n",
      "Epoch 12, Batch 7, Train Loss: 0.19530236721038818, Memory (GB): 7\n",
      "Epoch 12, Batch 8, Train Loss: 0.22085721790790558, Memory (GB): 7\n",
      "Epoch 12, Batch 9, Train Loss: 0.1915481686592102, Memory (GB): 7\n",
      "Epoch 12, Batch 10, Train Loss: 0.18471334874629974, Memory (GB): 7\n",
      "Epoch 12, Batch 11, Train Loss: 0.21617621183395386, Memory (GB): 7\n",
      "Epoch 12, Batch 12, Train Loss: 0.22549474239349365, Memory (GB): 7\n",
      "Epoch 12, Batch 13, Train Loss: 0.20188045501708984, Memory (GB): 7\n",
      "Epoch 12, Batch 14, Train Loss: 0.1925269067287445, Memory (GB): 7\n",
      "Epoch 12, Batch 15, Train Loss: 0.21165716648101807, Memory (GB): 7\n",
      "Epoch 12, Batch 16, Train Loss: 0.20853914320468903, Memory (GB): 7\n",
      "Epoch 12, Batch 17, Train Loss: 0.18793824315071106, Memory (GB): 7\n",
      "Epoch 12, Batch 18, Train Loss: 0.19946587085723877, Memory (GB): 7\n",
      "Epoch 12, Batch 19, Train Loss: 0.2175392359495163, Memory (GB): 7\n",
      "Epoch 12, Batch 20, Train Loss: 0.22926053404808044, Memory (GB): 7\n",
      "Epoch 12, Batch 21, Train Loss: 0.20853203535079956, Memory (GB): 7\n",
      "Epoch 12, Batch 22, Train Loss: 0.20771844685077667, Memory (GB): 7\n",
      "Epoch 12, Batch 23, Train Loss: 0.2364247888326645, Memory (GB): 7\n",
      "Epoch 12, Batch 24, Train Loss: 0.2039002925157547, Memory (GB): 7\n",
      "Epoch 12, Batch 25, Train Loss: 0.20855319499969482, Memory (GB): 7\n",
      "Epoch 12, Batch 26, Train Loss: 0.23115848004817963, Memory (GB): 7\n",
      "Epoch 12, Batch 27, Train Loss: 0.20674242079257965, Memory (GB): 7\n",
      "Epoch 12, Batch 28, Train Loss: 0.20603325963020325, Memory (GB): 7\n",
      "Epoch 12, Batch 29, Train Loss: 0.1980912834405899, Memory (GB): 7\n",
      "Epoch 12, Batch 30, Train Loss: 0.21896392107009888, Memory (GB): 7\n",
      "Epoch 12, Batch 31, Train Loss: 0.20192228257656097, Memory (GB): 7\n",
      "Epoch 12, Batch 32, Train Loss: 0.22754380106925964, Memory (GB): 7\n",
      "Epoch 12, Batch 33, Train Loss: 0.21344363689422607, Memory (GB): 7\n",
      "Epoch 12, Batch 34, Train Loss: 0.22719267010688782, Memory (GB): 7\n",
      "Epoch 12, Batch 35, Train Loss: 0.2247346192598343, Memory (GB): 7\n",
      "Epoch 12, Batch 36, Train Loss: 0.22825288772583008, Memory (GB): 7\n",
      "Epoch 12, Batch 37, Train Loss: 0.20825330913066864, Memory (GB): 7\n",
      "Epoch 12, Batch 38, Train Loss: 0.2195603996515274, Memory (GB): 7\n",
      "Epoch 12, Batch 39, Train Loss: 0.21970948576927185, Memory (GB): 7\n",
      "Epoch 12, Batch 40, Train Loss: 0.22664935886859894, Memory (GB): 7\n",
      "Epoch 12, Batch 41, Train Loss: 0.24186570942401886, Memory (GB): 7\n",
      "Epoch 12, Batch 42, Train Loss: 0.24215276539325714, Memory (GB): 7\n",
      "Epoch 12, Batch 43, Train Loss: 0.2159746289253235, Memory (GB): 7\n",
      "Epoch 12, Batch 44, Train Loss: 0.22763849794864655, Memory (GB): 7\n",
      "Epoch 12, Batch 45, Train Loss: 0.2138611227273941, Memory (GB): 7\n",
      "Epoch 12, Batch 46, Train Loss: 0.21729528903961182, Memory (GB): 7\n",
      "Epoch 12, Batch 47, Train Loss: 0.197480708360672, Memory (GB): 7\n",
      "Epoch 12, Batch 48, Train Loss: 0.2139883041381836, Memory (GB): 7\n",
      "Epoch 12, Batch 49, Train Loss: 0.22350865602493286, Memory (GB): 7\n",
      "Epoch 12, Batch 50, Train Loss: 0.21865837275981903, Memory (GB): 7\n",
      "Epoch 12, Batch 51, Train Loss: 0.21688833832740784, Memory (GB): 7\n",
      "Epoch 12, Batch 52, Train Loss: 0.2191295176744461, Memory (GB): 7\n",
      "Epoch 12, Batch 53, Train Loss: 0.20592902600765228, Memory (GB): 7\n",
      "Epoch 12, Batch 54, Train Loss: 0.20424343645572662, Memory (GB): 7\n",
      "Epoch 12, Batch 55, Train Loss: 0.22601935267448425, Memory (GB): 7\n",
      "Epoch 12, Batch 56, Train Loss: 0.21275286376476288, Memory (GB): 7\n",
      "Epoch 12, Batch 57, Train Loss: 0.2273203283548355, Memory (GB): 7\n",
      "Epoch 12, Batch 58, Train Loss: 0.2201748639345169, Memory (GB): 7\n",
      "Epoch 12, Batch 59, Train Loss: 0.2532101273536682, Memory (GB): 7\n",
      "Epoch 12, Batch 60, Train Loss: 0.2119854986667633, Memory (GB): 7\n",
      "Epoch 12, Batch 61, Train Loss: 0.22922751307487488, Memory (GB): 7\n",
      "Epoch 12, Batch 62, Train Loss: 0.1980905830860138, Memory (GB): 7\n",
      "Epoch 12, Batch 63, Train Loss: 0.2066483497619629, Memory (GB): 7\n",
      "Epoch 12, Batch 64, Train Loss: 0.24076130986213684, Memory (GB): 7\n",
      "Epoch 12, Batch 65, Train Loss: 0.2037956863641739, Memory (GB): 7\n",
      "Epoch 12, Batch 66, Train Loss: 0.20782388746738434, Memory (GB): 7\n",
      "Epoch 12, Batch 67, Train Loss: 0.1977059543132782, Memory (GB): 7\n",
      "Epoch 12, Batch 68, Train Loss: 0.19938918948173523, Memory (GB): 7\n",
      "Epoch 12, Batch 69, Train Loss: 0.22432027757167816, Memory (GB): 7\n",
      "Epoch 12, Batch 70, Train Loss: 0.2017628401517868, Memory (GB): 7\n",
      "Epoch 12, Batch 71, Train Loss: 0.21829049289226532, Memory (GB): 7\n",
      "Epoch 12, Batch 72, Train Loss: 0.21651393175125122, Memory (GB): 7\n",
      "Epoch 12, Batch 73, Train Loss: 0.20377063751220703, Memory (GB): 7\n",
      "Epoch 12, Batch 74, Train Loss: 0.23893950879573822, Memory (GB): 7\n",
      "Epoch 12, Batch 75, Train Loss: 0.21981534361839294, Memory (GB): 7\n",
      "Epoch 12, Batch 76, Train Loss: 0.19964691996574402, Memory (GB): 7\n",
      "Epoch 12, Batch 77, Train Loss: 0.18461370468139648, Memory (GB): 7\n",
      "Epoch 12, Batch 78, Train Loss: 0.218450665473938, Memory (GB): 7\n",
      "Epoch 12, Batch 79, Train Loss: 0.2236512005329132, Memory (GB): 7\n",
      "Epoch 12, Batch 80, Train Loss: 0.22060254216194153, Memory (GB): 7\n",
      "Epoch 12, Batch 81, Train Loss: 0.21273526549339294, Memory (GB): 7\n",
      "Epoch 12, Batch 82, Train Loss: 0.19046103954315186, Memory (GB): 7\n",
      "Epoch 12, Batch 83, Train Loss: 0.20265088975429535, Memory (GB): 7\n",
      "Epoch 12, Batch 84, Train Loss: 0.19572757184505463, Memory (GB): 7\n",
      "Epoch 12, Batch 85, Train Loss: 0.2142234742641449, Memory (GB): 7\n",
      "Epoch 12, Batch 86, Train Loss: 0.20663869380950928, Memory (GB): 7\n",
      "Epoch 12, Batch 87, Train Loss: 0.19327953457832336, Memory (GB): 7\n",
      "Epoch 12, Batch 88, Train Loss: 0.2175191193819046, Memory (GB): 7\n",
      "Epoch 12, Batch 89, Train Loss: 0.21722452342510223, Memory (GB): 7\n",
      "Epoch 12, Batch 90, Train Loss: 0.25111451745033264, Memory (GB): 7\n",
      "Epoch 12, Batch 91, Train Loss: 0.20585289597511292, Memory (GB): 7\n",
      "Epoch 12, Batch 92, Train Loss: 0.18585188686847687, Memory (GB): 7\n",
      "Epoch 12, Batch 93, Train Loss: 0.21895794570446014, Memory (GB): 7\n",
      "Epoch 12, Batch 94, Train Loss: 0.2055729627609253, Memory (GB): 7\n",
      "Epoch 12, Batch 95, Train Loss: 0.1932479590177536, Memory (GB): 7\n",
      "Epoch 12, Batch 96, Train Loss: 0.2253245860338211, Memory (GB): 7\n",
      "Epoch 12, Batch 97, Train Loss: 0.22473829984664917, Memory (GB): 7\n",
      "Epoch 12, Batch 98, Train Loss: 0.19751961529254913, Memory (GB): 7\n",
      "Epoch 12, Batch 99, Train Loss: 0.19881053268909454, Memory (GB): 7\n",
      "Epoch 12, Batch 100, Train Loss: 0.1984679400920868, Memory (GB): 7\n",
      "Epoch 12, Batch 101, Train Loss: 0.2088129222393036, Memory (GB): 7\n",
      "Epoch 12, Batch 102, Train Loss: 0.20794092118740082, Memory (GB): 7\n",
      "Epoch 12, Batch 103, Train Loss: 0.21932826936244965, Memory (GB): 7\n",
      "Epoch 12, Batch 104, Train Loss: 0.19303807616233826, Memory (GB): 7\n",
      "Epoch 12, Batch 105, Train Loss: 0.21055476367473602, Memory (GB): 7\n",
      "Epoch 12, Batch 106, Train Loss: 0.1833989918231964, Memory (GB): 7\n",
      "Epoch 12, Batch 107, Train Loss: 0.19457344710826874, Memory (GB): 7\n",
      "Epoch 12, Batch 108, Train Loss: 0.19582006335258484, Memory (GB): 7\n",
      "Epoch 12, Batch 109, Train Loss: 0.20421336591243744, Memory (GB): 7\n",
      "Epoch 12, Batch 110, Train Loss: 0.20616857707500458, Memory (GB): 7\n",
      "Epoch 12, Batch 111, Train Loss: 0.21204008162021637, Memory (GB): 7\n",
      "Epoch 12, Batch 112, Train Loss: 0.21991761028766632, Memory (GB): 7\n",
      "Epoch 12, Batch 113, Train Loss: 0.20431968569755554, Memory (GB): 7\n",
      "Epoch 12, Batch 114, Train Loss: 0.2001977264881134, Memory (GB): 7\n",
      "Epoch 12, Batch 115, Train Loss: 0.1987413465976715, Memory (GB): 7\n",
      "Epoch 12, Batch 116, Train Loss: 0.19685234129428864, Memory (GB): 7\n",
      "Epoch 12, Batch 117, Train Loss: 0.19651930034160614, Memory (GB): 7\n",
      "Epoch 12, Batch 118, Train Loss: 0.20855113863945007, Memory (GB): 7\n",
      "Epoch 12, Batch 119, Train Loss: 0.2096339613199234, Memory (GB): 7\n",
      "Epoch 12, Batch 120, Train Loss: 0.2158966064453125, Memory (GB): 7\n",
      "Epoch 12, Batch 121, Train Loss: 0.1928091049194336, Memory (GB): 7\n",
      "Epoch 12, Batch 122, Train Loss: 0.2216777354478836, Memory (GB): 7\n",
      "Epoch 12, Batch 123, Train Loss: 0.1750415712594986, Memory (GB): 7\n",
      "Epoch 12, Batch 124, Train Loss: 0.2157442420721054, Memory (GB): 7\n",
      "Epoch 12, Batch 125, Train Loss: 0.21420609951019287, Memory (GB): 7\n",
      "Epoch 12, Batch 126, Train Loss: 0.1882861852645874, Memory (GB): 7\n",
      "Epoch 12, Batch 127, Train Loss: 0.23588049411773682, Memory (GB): 7\n",
      "Epoch 12, Batch 128, Train Loss: 0.21299512684345245, Memory (GB): 7\n",
      "Epoch 12, Batch 129, Train Loss: 0.23054057359695435, Memory (GB): 7\n",
      "Epoch 12, Batch 130, Train Loss: 0.2306969314813614, Memory (GB): 7\n",
      "Epoch 12, Batch 131, Train Loss: 0.20383493602275848, Memory (GB): 7\n",
      "Epoch 12, Batch 132, Train Loss: 0.23479293286800385, Memory (GB): 7\n",
      "Epoch 12, Batch 133, Train Loss: 0.2288702130317688, Memory (GB): 7\n",
      "Epoch 12, Batch 134, Train Loss: 0.20803850889205933, Memory (GB): 7\n",
      "Epoch 12, Batch 135, Train Loss: 0.2521459758281708, Memory (GB): 7\n",
      "Epoch 12, Batch 136, Train Loss: 0.2413782775402069, Memory (GB): 7\n",
      "Epoch 12, Batch 137, Train Loss: 0.2324341982603073, Memory (GB): 7\n",
      "Epoch 12, Batch 138, Train Loss: 0.23437516391277313, Memory (GB): 7\n",
      "Epoch 12, Batch 139, Train Loss: 0.2128104269504547, Memory (GB): 7\n",
      "Epoch 12, Batch 140, Train Loss: 0.2090943157672882, Memory (GB): 7\n",
      "Epoch 12, Batch 141, Train Loss: 0.22735841572284698, Memory (GB): 7\n",
      "Epoch 12, Batch 142, Train Loss: 0.2539902329444885, Memory (GB): 7\n",
      "Epoch 12, Batch 143, Train Loss: 0.23783458769321442, Memory (GB): 7\n",
      "Epoch 12, Batch 144, Train Loss: 0.27004101872444153, Memory (GB): 7\n",
      "Epoch 12, Batch 145, Train Loss: 0.22267328202724457, Memory (GB): 7\n",
      "Epoch 12, Batch 146, Train Loss: 0.2244122326374054, Memory (GB): 7\n",
      "Epoch 12, Batch 147, Train Loss: 0.2308999001979828, Memory (GB): 7\n",
      "Epoch 12, Batch 148, Train Loss: 0.2455369383096695, Memory (GB): 7\n",
      "Epoch 12, Batch 149, Train Loss: 0.22472798824310303, Memory (GB): 7\n",
      "Epoch 12, Batch 150, Train Loss: 0.22765108942985535, Memory (GB): 7\n",
      "Epoch 12, Batch 151, Train Loss: 0.21568836271762848, Memory (GB): 7\n",
      "Epoch 12, Batch 152, Train Loss: 0.25275883078575134, Memory (GB): 7\n",
      "Epoch 12, Batch 153, Train Loss: 0.23597045242786407, Memory (GB): 7\n",
      "Epoch 12, Batch 154, Train Loss: 0.2267516404390335, Memory (GB): 7\n",
      "Epoch 12, Batch 155, Train Loss: 0.2279023677110672, Memory (GB): 7\n",
      "Epoch 12, Batch 156, Train Loss: 0.2530616223812103, Memory (GB): 7\n",
      "Epoch 12, Batch 157, Train Loss: 0.23919421434402466, Memory (GB): 7\n",
      "Epoch 12, Batch 158, Train Loss: 0.20694242417812347, Memory (GB): 7\n",
      "Epoch 12, Batch 159, Train Loss: 0.22966216504573822, Memory (GB): 7\n",
      "Epoch 12, Batch 160, Train Loss: 0.22217051684856415, Memory (GB): 7\n",
      "Epoch 12, Batch 161, Train Loss: 0.20654180645942688, Memory (GB): 7\n",
      "Epoch 12, Batch 162, Train Loss: 0.22312919795513153, Memory (GB): 7\n",
      "Epoch 12, Batch 163, Train Loss: 0.22955426573753357, Memory (GB): 7\n",
      "Epoch 12, Batch 164, Train Loss: 0.22380349040031433, Memory (GB): 7\n",
      "Epoch 12, Batch 165, Train Loss: 0.24230298399925232, Memory (GB): 7\n",
      "Epoch 12, Batch 166, Train Loss: 0.20204712450504303, Memory (GB): 7\n",
      "Epoch 12, Batch 167, Train Loss: 0.21971920132637024, Memory (GB): 7\n",
      "Epoch 12, Batch 168, Train Loss: 0.20605233311653137, Memory (GB): 7\n",
      "Epoch 12, Batch 169, Train Loss: 0.22329066693782806, Memory (GB): 7\n",
      "Epoch 12, Batch 170, Train Loss: 0.23628300428390503, Memory (GB): 7\n",
      "Epoch 12, Batch 171, Train Loss: 0.21681921184062958, Memory (GB): 7\n",
      "Epoch 12, Batch 172, Train Loss: 0.22694937884807587, Memory (GB): 7\n",
      "Epoch 12, Batch 173, Train Loss: 0.2309393435716629, Memory (GB): 7\n",
      "Epoch 12, Batch 174, Train Loss: 0.22587762773036957, Memory (GB): 7\n",
      "Epoch 12, Batch 175, Train Loss: 0.21323779225349426, Memory (GB): 7\n",
      "Epoch 12, Batch 176, Train Loss: 0.22045966982841492, Memory (GB): 7\n",
      "Epoch 12, Batch 177, Train Loss: 0.2193494737148285, Memory (GB): 7\n",
      "Epoch 12, Batch 178, Train Loss: 0.19842125475406647, Memory (GB): 7\n",
      "Epoch 12, Batch 179, Train Loss: 0.2301510125398636, Memory (GB): 7\n",
      "[6.893158507178198e-05, 6.548500581819293e-05, 6.22107555272833e-05, 5.910021775091912e-05, 5.61452068633731e-05, 5.333794652020447e-05, 5.067104919419424e-05, 4.813749673448453e-05, 4.573062189776029e-05, 4.344409080287228e-05, 4.1271886262728657e-05, 3.9208291949592236e-05, 3.724787735211259e-05, 3.5385483484507e-05, 3.361620931028167e-05, 3.1935398844767555e-05, 3.0338628902529185e-05, 2.8821697457402705e-05, 2.7380612584532585e-05, 2.6011581955305965e-05, 2.471100285754065e-05, 2.3475452714663613e-05, 2.2301680078930446e-05, 2.1186596074983897e-05, 2.0127266271234705e-05, 1.912090295767299e-05, 1.816485780978933e-05, 1.7256614919299852e-05, 1.6393784173334856e-05, 1.5574094964668125e-05, 1.4795390216434712e-05, 1.4055620705612971e-05, 1.3352839670332324e-05, 1.2685197686815717e-05, 1.2050937802474921e-05, 1.1448390912351176e-05, 1.0875971366733617e-05, 1.0332172798396935e-05, 9.81556415847708e-06, 9.324785950553228e-06, 8.858546653025566e-06, 8.415619320374291e-06, 7.994838354355577e-06, 7.595096436637796e-06, 7.215341614805904e-06, 6.854574534065608e-06, 6.511845807362325e-06, 6.186253516994212e-06, 5.876940841144501e-06, 5.583093799087279e-06, 5.303939109132914e-06, 5.0387421536762665e-06, 4.786805045992459e-06, 4.54746479369283e-06, 4.320091554008191e-06, 4.104086976307778e-06, 3.898882627492391e-06, 3.7039384961177695e-06, 3.5187415713118813e-06, 3.3428044927462854e-06, 3.175664268108975e-06, 3.016881054703525e-06, 2.8660370019683455e-06, 2.7227351518699277e-06, 2.586598394276436e-06, 2.4572684745626102e-06, 2.3344050508344816e-06, 2.2176847982927567e-06, 2.106800558378119e-06, 2.001460530459211e-06, 1.9013875039362524e-06, 1.8063181287394393e-06, 1.7160022223024655e-06, 1.6302021111873423e-06, 1.5486920056279758e-06, 1.4712574053465777e-06, 1.397694535079248e-06, 1.3278098083252863e-06, 1.2614193179090213e-06, 1.1983483520135704e-06, 1.1384309344128927e-06, 1.0815093876922474e-06, 1.0274339183076346e-06, 9.76062222392253e-07, 9.272591112726409e-07, 8.808961557090082e-07, 8.368513479235583e-07, 7.950087805273802e-07, 7.552583415010115e-07, 7.174954244259602e-07, 6.81620653204662e-07, 6.475396205444295e-07, 6.151626395172075e-07, 5.844045075413473e-07, 5.551842821642801e-07, 5.274250680560659e-07, 5.010538146532629e-07, 4.760011239205996e-07, 4.522010677245693e-07, 4.295910143383411e-07, 4.081114636214241e-07, 3.8770589044035283e-07, 3.6832059591833517e-07, 3.4990456612241847e-07, 3.3240933781629764e-07, 3.157888709254825e-07, 2.999994273792084e-07, 2.849994560102479e-07, 2.7074948320973556e-07, 2.5721200904924903e-07, 2.443514085967863e-07, 2.3213383816694712e-07, 2.2052714625859974e-07, 2.0950078894566966e-07, 1.990257494983861e-07, 1.8907446202346675e-07, 1.7962073892229362e-07, 1.7063970197617884e-07, 1.6210771687736992e-07, 1.5400233103350136e-07, 1.4630221448182624e-07, 1.3898710375773487e-07, 1.320377485698483e-07, 1.2543586114135573e-07, 1.1916406808428795e-07, 1.1320586468007357e-07, 1.0754557144606994e-07, 1.0216829287376645e-07, 9.705987823007798e-08, 9.220688431857419e-08, 8.759654010264548e-08, 8.321671309751319e-08, 7.905587744263752e-08, 7.510308357050565e-08, 7.134792939198036e-08, 6.77805329223814e-08, 6.439150627626228e-08, 6.117193096244915e-08, 5.8113334414326695e-08, 5.520766769361035e-08, 5.244728430892986e-08, 4.982492009348336e-08, 4.733367408880917e-08, 4.496699038436873e-08, 4.271864086515025e-08, 4.058270882189275e-08, 3.855357338079811e-08, 3.662589471175821e-08, 3.4794599976170316e-08, 3.305486997736178e-08, 3.140212647849369e-08, 2.9832020154569006e-08, 2.834041914684056e-08, 2.6923398189498532e-08, 2.5577228280023585e-08, 2.4298366866022425e-08, 2.3083448522721283e-08, 2.1929276096585208e-08, 2.083281229175597e-08, 1.9791171677168166e-08, 1.8801613093309772e-08, 1.7861532438644268e-08, 1.6968455816712046e-08, 1.612003302587644e-08, 1.531403137458263e-08, 1.4548329805853484e-08, 1.3820913315560805e-08, 1.3129867649782781e-08, 1.2473374267293633e-08, 1.1849705553928954e-08, 1.1257220276232506e-08, 1.069435926242088e-08, 1.0159641299299832e-08, 9.65165923433485e-09, 9.169076272618104e-09, 8.710622458987197e-09, 8.275091336037838e-09, 7.861336769235944e-09, 7.468269930774146e-09, 7.094856434235433e-09, 6.7401136125236635e-09, 6.4031079318974815e-09, 6.082952535302606e-09, 5.778804908537472e-09, 5.489864663110603e-09, 5.215371429955069e-09, 4.954602858457319e-09, 4.706872715534452e-09, 4.471529079757727e-09, 4.247952625769846e-09, 4.035554994481351e-09, 3.833777244757282e-09, 3.64208838251942e-09, 3.459983963393449e-09, 3.2869847652237736e-09, 3.1226355269625866e-09, 2.9665037506144554e-09, 2.818178563083734e-09, 2.6772696349295465e-09]\n",
      "Epoch 13, Batch 0, Train Loss: 0.20556442439556122, Memory (GB): 7\n",
      "Epoch 13, Batch 1, Train Loss: 0.18555568158626556, Memory (GB): 7\n",
      "Epoch 13, Batch 2, Train Loss: 0.19419518113136292, Memory (GB): 7\n",
      "Epoch 13, Batch 3, Train Loss: 0.1822853833436966, Memory (GB): 7\n",
      "Epoch 13, Batch 4, Train Loss: 0.1967371106147766, Memory (GB): 7\n",
      "Epoch 13, Batch 5, Train Loss: 0.19453105330467224, Memory (GB): 7\n",
      "Epoch 13, Batch 6, Train Loss: 0.19321581721305847, Memory (GB): 7\n",
      "Epoch 13, Batch 7, Train Loss: 0.21965248882770538, Memory (GB): 7\n",
      "Epoch 13, Batch 8, Train Loss: 0.1876130849123001, Memory (GB): 7\n",
      "Epoch 13, Batch 9, Train Loss: 0.20922785997390747, Memory (GB): 7\n",
      "Epoch 13, Batch 10, Train Loss: 0.20255987346172333, Memory (GB): 7\n",
      "Epoch 13, Batch 11, Train Loss: 0.17040282487869263, Memory (GB): 7\n",
      "Epoch 13, Batch 12, Train Loss: 0.20343099534511566, Memory (GB): 7\n",
      "Epoch 13, Batch 13, Train Loss: 0.18686947226524353, Memory (GB): 7\n",
      "Epoch 13, Batch 14, Train Loss: 0.17965662479400635, Memory (GB): 7\n",
      "Epoch 13, Batch 15, Train Loss: 0.19991977512836456, Memory (GB): 7\n",
      "Epoch 13, Batch 16, Train Loss: 0.19623665511608124, Memory (GB): 7\n",
      "Epoch 13, Batch 17, Train Loss: 0.2123909443616867, Memory (GB): 7\n",
      "Epoch 13, Batch 18, Train Loss: 0.20925453305244446, Memory (GB): 7\n",
      "Epoch 13, Batch 19, Train Loss: 0.19470462203025818, Memory (GB): 7\n",
      "Epoch 13, Batch 20, Train Loss: 0.1928700953722, Memory (GB): 7\n",
      "Epoch 13, Batch 21, Train Loss: 0.18949724733829498, Memory (GB): 7\n",
      "Epoch 13, Batch 22, Train Loss: 0.18376024067401886, Memory (GB): 7\n",
      "Epoch 13, Batch 23, Train Loss: 0.19795110821723938, Memory (GB): 7\n",
      "Epoch 13, Batch 24, Train Loss: 0.1629761904478073, Memory (GB): 7\n",
      "Epoch 13, Batch 25, Train Loss: 0.18639890849590302, Memory (GB): 7\n",
      "Epoch 13, Batch 26, Train Loss: 0.1803080290555954, Memory (GB): 7\n",
      "Epoch 13, Batch 27, Train Loss: 0.18342430889606476, Memory (GB): 7\n",
      "Epoch 13, Batch 28, Train Loss: 0.17936289310455322, Memory (GB): 7\n",
      "Epoch 13, Batch 29, Train Loss: 0.18844851851463318, Memory (GB): 7\n",
      "Epoch 13, Batch 30, Train Loss: 0.197294682264328, Memory (GB): 7\n",
      "Epoch 13, Batch 31, Train Loss: 0.17224204540252686, Memory (GB): 7\n",
      "Epoch 13, Batch 32, Train Loss: 0.17897088825702667, Memory (GB): 7\n",
      "Epoch 13, Batch 33, Train Loss: 0.1920575201511383, Memory (GB): 7\n",
      "Epoch 13, Batch 34, Train Loss: 0.21556057035923004, Memory (GB): 7\n",
      "Epoch 13, Batch 35, Train Loss: 0.19995412230491638, Memory (GB): 7\n",
      "Epoch 13, Batch 36, Train Loss: 0.17498737573623657, Memory (GB): 7\n",
      "Epoch 13, Batch 37, Train Loss: 0.2097567617893219, Memory (GB): 7\n",
      "Epoch 13, Batch 38, Train Loss: 0.17900140583515167, Memory (GB): 7\n",
      "Epoch 13, Batch 39, Train Loss: 0.1728236824274063, Memory (GB): 7\n",
      "Epoch 13, Batch 40, Train Loss: 0.21569351851940155, Memory (GB): 7\n",
      "Epoch 13, Batch 41, Train Loss: 0.22412808239459991, Memory (GB): 7\n",
      "Epoch 13, Batch 42, Train Loss: 0.19853894412517548, Memory (GB): 7\n",
      "Epoch 13, Batch 43, Train Loss: 0.16462786495685577, Memory (GB): 7\n",
      "Epoch 13, Batch 44, Train Loss: 0.23715431988239288, Memory (GB): 7\n",
      "Epoch 13, Batch 45, Train Loss: 0.21816548705101013, Memory (GB): 7\n",
      "Epoch 13, Batch 46, Train Loss: 0.21091340482234955, Memory (GB): 7\n",
      "Epoch 13, Batch 47, Train Loss: 0.19014041125774384, Memory (GB): 7\n",
      "Epoch 13, Batch 48, Train Loss: 0.19988246262073517, Memory (GB): 7\n",
      "Epoch 13, Batch 49, Train Loss: 0.20763914287090302, Memory (GB): 7\n",
      "Epoch 13, Batch 50, Train Loss: 0.20438678562641144, Memory (GB): 7\n",
      "Epoch 13, Batch 51, Train Loss: 0.19174423813819885, Memory (GB): 7\n",
      "Epoch 13, Batch 52, Train Loss: 0.19915953278541565, Memory (GB): 7\n",
      "Epoch 13, Batch 53, Train Loss: 0.2115606814622879, Memory (GB): 7\n",
      "Epoch 13, Batch 54, Train Loss: 0.19708217680454254, Memory (GB): 7\n",
      "Epoch 13, Batch 55, Train Loss: 0.19062869250774384, Memory (GB): 7\n",
      "Epoch 13, Batch 56, Train Loss: 0.20515143871307373, Memory (GB): 7\n",
      "Epoch 13, Batch 57, Train Loss: 0.21571886539459229, Memory (GB): 7\n",
      "Epoch 13, Batch 58, Train Loss: 0.21430276334285736, Memory (GB): 7\n",
      "Epoch 13, Batch 59, Train Loss: 0.21759822964668274, Memory (GB): 7\n",
      "Epoch 13, Batch 60, Train Loss: 0.23887494206428528, Memory (GB): 7\n",
      "Epoch 13, Batch 61, Train Loss: 0.19550101459026337, Memory (GB): 7\n",
      "Epoch 13, Batch 62, Train Loss: 0.21430015563964844, Memory (GB): 7\n",
      "Epoch 13, Batch 63, Train Loss: 0.2316219061613083, Memory (GB): 7\n",
      "Epoch 13, Batch 64, Train Loss: 0.22670118510723114, Memory (GB): 7\n",
      "Epoch 13, Batch 65, Train Loss: 0.20992109179496765, Memory (GB): 7\n",
      "Epoch 13, Batch 66, Train Loss: 0.2077368050813675, Memory (GB): 7\n",
      "Epoch 13, Batch 67, Train Loss: 0.23064610362052917, Memory (GB): 7\n",
      "Epoch 13, Batch 68, Train Loss: 0.2260231077671051, Memory (GB): 7\n",
      "Epoch 13, Batch 69, Train Loss: 0.2515104413032532, Memory (GB): 7\n",
      "Epoch 13, Batch 70, Train Loss: 0.21538157761096954, Memory (GB): 7\n",
      "Epoch 13, Batch 71, Train Loss: 0.22860874235630035, Memory (GB): 7\n",
      "Epoch 13, Batch 72, Train Loss: 0.20744583010673523, Memory (GB): 7\n",
      "Epoch 13, Batch 73, Train Loss: 0.20333276689052582, Memory (GB): 7\n",
      "Epoch 13, Batch 74, Train Loss: 0.21442754566669464, Memory (GB): 7\n",
      "Epoch 13, Batch 75, Train Loss: 0.23460924625396729, Memory (GB): 7\n",
      "Epoch 13, Batch 76, Train Loss: 0.21272000670433044, Memory (GB): 7\n",
      "Epoch 13, Batch 77, Train Loss: 0.21593202650547028, Memory (GB): 7\n",
      "Epoch 13, Batch 78, Train Loss: 0.20986616611480713, Memory (GB): 7\n",
      "Epoch 13, Batch 79, Train Loss: 0.23037928342819214, Memory (GB): 7\n",
      "Epoch 13, Batch 80, Train Loss: 0.18997547030448914, Memory (GB): 7\n",
      "Epoch 13, Batch 81, Train Loss: 0.18984222412109375, Memory (GB): 7\n",
      "Epoch 13, Batch 82, Train Loss: 0.19932806491851807, Memory (GB): 7\n",
      "Epoch 13, Batch 83, Train Loss: 0.20220735669136047, Memory (GB): 7\n",
      "Epoch 13, Batch 84, Train Loss: 0.22501200437545776, Memory (GB): 7\n",
      "Epoch 13, Batch 85, Train Loss: 0.17990539968013763, Memory (GB): 7\n",
      "Epoch 13, Batch 86, Train Loss: 0.21449530124664307, Memory (GB): 7\n",
      "Epoch 13, Batch 87, Train Loss: 0.21269062161445618, Memory (GB): 7\n",
      "Epoch 13, Batch 88, Train Loss: 0.22605332732200623, Memory (GB): 7\n",
      "Epoch 13, Batch 89, Train Loss: 0.20742204785346985, Memory (GB): 7\n",
      "Epoch 13, Batch 90, Train Loss: 0.2232537716627121, Memory (GB): 7\n",
      "Epoch 13, Batch 91, Train Loss: 0.181706503033638, Memory (GB): 7\n",
      "Epoch 13, Batch 92, Train Loss: 0.18460966646671295, Memory (GB): 7\n",
      "Epoch 13, Batch 93, Train Loss: 0.19401536881923676, Memory (GB): 7\n",
      "Epoch 13, Batch 94, Train Loss: 0.19890283048152924, Memory (GB): 7\n",
      "Epoch 13, Batch 95, Train Loss: 0.18526595830917358, Memory (GB): 7\n",
      "Epoch 13, Batch 96, Train Loss: 0.19048236310482025, Memory (GB): 7\n",
      "Epoch 13, Batch 97, Train Loss: 0.20979149639606476, Memory (GB): 7\n",
      "Epoch 13, Batch 98, Train Loss: 0.18922202289104462, Memory (GB): 7\n",
      "Epoch 13, Batch 99, Train Loss: 0.20308725535869598, Memory (GB): 7\n",
      "Epoch 13, Batch 100, Train Loss: 0.2087307721376419, Memory (GB): 7\n",
      "Epoch 13, Batch 101, Train Loss: 0.24469006061553955, Memory (GB): 7\n",
      "Epoch 13, Batch 102, Train Loss: 0.20793554186820984, Memory (GB): 7\n",
      "Epoch 13, Batch 103, Train Loss: 0.19610540568828583, Memory (GB): 7\n",
      "Epoch 13, Batch 104, Train Loss: 0.19745562970638275, Memory (GB): 7\n",
      "Epoch 13, Batch 105, Train Loss: 0.2091500163078308, Memory (GB): 7\n",
      "Epoch 13, Batch 106, Train Loss: 0.21673829853534698, Memory (GB): 7\n",
      "Epoch 13, Batch 107, Train Loss: 0.1924617439508438, Memory (GB): 7\n",
      "Epoch 13, Batch 108, Train Loss: 0.18744559586048126, Memory (GB): 7\n",
      "Epoch 13, Batch 109, Train Loss: 0.18527258932590485, Memory (GB): 7\n",
      "Epoch 13, Batch 110, Train Loss: 0.1998542994260788, Memory (GB): 7\n",
      "Epoch 13, Batch 111, Train Loss: 0.17109453678131104, Memory (GB): 7\n",
      "Epoch 13, Batch 112, Train Loss: 0.19962410628795624, Memory (GB): 7\n",
      "Epoch 13, Batch 113, Train Loss: 0.19699442386627197, Memory (GB): 7\n",
      "Epoch 13, Batch 114, Train Loss: 0.1872778683900833, Memory (GB): 7\n",
      "Epoch 13, Batch 115, Train Loss: 0.18384942412376404, Memory (GB): 7\n",
      "Epoch 13, Batch 116, Train Loss: 0.19475755095481873, Memory (GB): 7\n",
      "Epoch 13, Batch 117, Train Loss: 0.21248938143253326, Memory (GB): 7\n",
      "Epoch 13, Batch 118, Train Loss: 0.1841973066329956, Memory (GB): 7\n",
      "Epoch 13, Batch 119, Train Loss: 0.2009171098470688, Memory (GB): 7\n",
      "Epoch 13, Batch 120, Train Loss: 0.2078593522310257, Memory (GB): 7\n",
      "Epoch 13, Batch 121, Train Loss: 0.18964345753192902, Memory (GB): 7\n",
      "Epoch 13, Batch 122, Train Loss: 0.2077886462211609, Memory (GB): 7\n",
      "Epoch 13, Batch 123, Train Loss: 0.1774008572101593, Memory (GB): 7\n",
      "Epoch 13, Batch 124, Train Loss: 0.19926388561725616, Memory (GB): 7\n",
      "Epoch 13, Batch 125, Train Loss: 0.2038387954235077, Memory (GB): 7\n",
      "Epoch 13, Batch 126, Train Loss: 0.1856333315372467, Memory (GB): 7\n",
      "Epoch 13, Batch 127, Train Loss: 0.1870785653591156, Memory (GB): 7\n",
      "Epoch 13, Batch 128, Train Loss: 0.19406439363956451, Memory (GB): 7\n",
      "Epoch 13, Batch 129, Train Loss: 0.19450993835926056, Memory (GB): 7\n",
      "Epoch 13, Batch 130, Train Loss: 0.19622938334941864, Memory (GB): 7\n",
      "Epoch 13, Batch 131, Train Loss: 0.20485341548919678, Memory (GB): 7\n",
      "Epoch 13, Batch 132, Train Loss: 0.2016206532716751, Memory (GB): 7\n",
      "Epoch 13, Batch 133, Train Loss: 0.20762713253498077, Memory (GB): 7\n",
      "Epoch 13, Batch 134, Train Loss: 0.18389774858951569, Memory (GB): 7\n",
      "Epoch 13, Batch 135, Train Loss: 0.23648612201213837, Memory (GB): 7\n",
      "Epoch 13, Batch 136, Train Loss: 0.18588025867938995, Memory (GB): 7\n",
      "Epoch 13, Batch 137, Train Loss: 0.17441704869270325, Memory (GB): 7\n",
      "Epoch 13, Batch 138, Train Loss: 0.20375269651412964, Memory (GB): 7\n",
      "Epoch 13, Batch 139, Train Loss: 0.20714305341243744, Memory (GB): 7\n",
      "Epoch 13, Batch 140, Train Loss: 0.18918727338314056, Memory (GB): 7\n",
      "Epoch 13, Batch 141, Train Loss: 0.2026306837797165, Memory (GB): 7\n",
      "Epoch 13, Batch 142, Train Loss: 0.18142083287239075, Memory (GB): 7\n",
      "Epoch 13, Batch 143, Train Loss: 0.17836181819438934, Memory (GB): 7\n",
      "Epoch 13, Batch 144, Train Loss: 0.21795785427093506, Memory (GB): 7\n",
      "Epoch 13, Batch 145, Train Loss: 0.22382470965385437, Memory (GB): 7\n",
      "Epoch 13, Batch 146, Train Loss: 0.1920003890991211, Memory (GB): 7\n",
      "Epoch 13, Batch 147, Train Loss: 0.18115311861038208, Memory (GB): 7\n",
      "Epoch 13, Batch 148, Train Loss: 0.22540906071662903, Memory (GB): 7\n",
      "Epoch 13, Batch 149, Train Loss: 0.22152464091777802, Memory (GB): 7\n",
      "Epoch 13, Batch 150, Train Loss: 0.21131780743598938, Memory (GB): 7\n",
      "Epoch 13, Batch 151, Train Loss: 0.19815899431705475, Memory (GB): 7\n",
      "Epoch 13, Batch 152, Train Loss: 0.1911611258983612, Memory (GB): 7\n",
      "Epoch 13, Batch 153, Train Loss: 0.1959465742111206, Memory (GB): 7\n",
      "Epoch 13, Batch 154, Train Loss: 0.2021174132823944, Memory (GB): 7\n",
      "Epoch 13, Batch 155, Train Loss: 0.20935630798339844, Memory (GB): 7\n",
      "Epoch 13, Batch 156, Train Loss: 0.21287120878696442, Memory (GB): 7\n",
      "Epoch 13, Batch 157, Train Loss: 0.21510861814022064, Memory (GB): 7\n",
      "Epoch 13, Batch 158, Train Loss: 0.22879387438297272, Memory (GB): 7\n",
      "Epoch 13, Batch 159, Train Loss: 0.2065601795911789, Memory (GB): 7\n",
      "Epoch 13, Batch 160, Train Loss: 0.22021262347698212, Memory (GB): 7\n",
      "Epoch 13, Batch 161, Train Loss: 0.201802596449852, Memory (GB): 7\n",
      "Epoch 13, Batch 162, Train Loss: 0.21397502720355988, Memory (GB): 7\n",
      "Epoch 13, Batch 163, Train Loss: 0.223043754696846, Memory (GB): 7\n",
      "Epoch 13, Batch 164, Train Loss: 0.21364562213420868, Memory (GB): 7\n",
      "Epoch 13, Batch 165, Train Loss: 0.21382690966129303, Memory (GB): 7\n",
      "Epoch 13, Batch 166, Train Loss: 0.206910640001297, Memory (GB): 7\n",
      "Epoch 13, Batch 167, Train Loss: 0.21921373903751373, Memory (GB): 7\n",
      "Epoch 13, Batch 168, Train Loss: 0.2325025349855423, Memory (GB): 7\n",
      "Epoch 13, Batch 169, Train Loss: 0.21123254299163818, Memory (GB): 7\n",
      "Epoch 13, Batch 170, Train Loss: 0.2178211212158203, Memory (GB): 7\n",
      "Epoch 13, Batch 171, Train Loss: 0.2088291049003601, Memory (GB): 7\n",
      "Epoch 13, Batch 172, Train Loss: 0.20807869732379913, Memory (GB): 7\n",
      "Epoch 13, Batch 173, Train Loss: 0.21383614838123322, Memory (GB): 7\n",
      "Epoch 13, Batch 174, Train Loss: 0.2310904562473297, Memory (GB): 7\n",
      "Epoch 13, Batch 175, Train Loss: 0.2116853892803192, Memory (GB): 7\n",
      "Epoch 13, Batch 176, Train Loss: 0.20000571012496948, Memory (GB): 7\n",
      "Epoch 13, Batch 177, Train Loss: 0.2035958170890808, Memory (GB): 7\n",
      "Epoch 13, Batch 178, Train Loss: 0.2221182882785797, Memory (GB): 7\n",
      "Epoch 13, Batch 179, Train Loss: 0.21949446201324463, Memory (GB): 7\n",
      "[0.0005313993451400298, 0.0005048293778830282, 0.00047958790898887697, 0.000455608513539433, 0.0004328280878624614, 0.0004111866834693382, 0.0003906273492958714, 0.00037109598183107797, 0.0003525411827395239, 0.0003349141236025477, 0.0003181684174224201, 0.00030225999655129914, 0.00028714699672373415, 0.00027278964688754766, 0.00025915016454317, 0.0002461926563160117, 0.00023388302350021083, 0.00022218887232520066, 0.0002110794287089402, 0.00020052545727349336, 0.00019049918440981872, 0.00018097422518932786, 0.00017192551392986133, 0.0001633292382333683, 0.0001551627763216998, 0.00014740463750561477, 0.00014003440563033413, 0.00013303268534881738, 0.00012638105108137647, 0.00012006199852730767, 0.00011405889860094224, 0.00010835595367089517, 0.0001029381559873504, 9.77912481879829e-05, 9.290168577858372e-05, 8.825660148965453e-05, 8.384377141517182e-05, 7.965158284441322e-05, 7.566900370219248e-05, 7.188555351708288e-05, 6.829127584122871e-05, 6.487671204916729e-05, 6.163287644670897e-05, 5.8551232624373495e-05, 5.562367099315482e-05, 5.284248744349705e-05, 5.020036307132219e-05, 4.769034491775608e-05, 4.530582767186828e-05, 4.304053628827486e-05, 4.0888509473861094e-05, 3.884408400016807e-05, 3.690187980015964e-05, 3.505678581015169e-05, 3.33039465196441e-05, 3.163874919366188e-05, 3.0056811733978803e-05, 2.8553971147279848e-05, 2.712627258991587e-05, 2.5769958960420058e-05, 2.4481461012399067e-05, 2.3257387961779094e-05, 2.2094518563690122e-05, 2.0989792635505634e-05, 1.9940303003730358e-05, 1.8943287853543842e-05, 1.7996123460866637e-05, 1.7096317287823304e-05, 1.6241501423432143e-05, 1.5429426352260535e-05, 1.4657955034647513e-05, 1.3925057282915129e-05, 1.322880441876937e-05, 1.2567364197830898e-05, 1.193899598793936e-05, 1.1342046188542387e-05, 1.0774943879115269e-05, 1.0236196685159506e-05, 9.724386850901528e-06, 9.238167508356461e-06, 8.776259132938625e-06, 8.337446176291702e-06, 7.920573867477114e-06, 7.52454517410326e-06, 7.148317915398096e-06, 6.790902019628196e-06, 6.451356918646782e-06, 6.128789072714441e-06, 5.8223496190787205e-06, 5.531232138124783e-06, 5.2546705312185444e-06, 4.991937004657614e-06, 4.742340154424732e-06, 4.505223146703497e-06, 4.279961989368322e-06, 4.065963889899905e-06, 3.862665695404912e-06, 3.6695324106346663e-06, 3.486055790102932e-06, 3.311753000597785e-06, 3.1461653505678957e-06, 2.9888570830395015e-06, 2.8394142288875277e-06, 2.69744351744315e-06, 2.562571341570993e-06, 2.4344427744924424e-06, 2.312720635767822e-06, 2.19708460397943e-06, 2.087230373780458e-06, 1.982868855091436e-06, 1.8837254123368626e-06, 1.7895391417200212e-06, 1.700062184634019e-06, 1.615059075402318e-06, 1.5343061216322022e-06, 1.4575908155505916e-06, 1.3847112747730627e-06, 1.3154757110344086e-06, 1.2497019254826885e-06, 1.187216829208554e-06, 1.1278559877481257e-06, 1.07146318836072e-06, 1.0178900289426836e-06, 9.66995527495549e-07, 9.186457511207719e-07, 8.727134635647332e-07, 8.290777903864966e-07, 7.876239008671715e-07, 7.482427058238134e-07, 7.108305705326225e-07, 6.752890420059911e-07, 6.415245899056912e-07, 6.094483604104071e-07, 5.789759423898865e-07, 5.500271452703922e-07, 5.225257880068726e-07, 4.963994986065294e-07, 4.715795236762025e-07, 4.4800054749239234e-07, 4.256005201177726e-07, 4.043204941118841e-07, 3.841044694062898e-07, 3.6489924593597554e-07, 3.466542836391769e-07, 3.293215694572176e-07, 3.128554909843566e-07, 2.9721271643513904e-07, 2.8235208061338194e-07, 2.682344765827127e-07, 2.548227527535772e-07, 2.4208161511589834e-07, 2.299775343601035e-07, 2.1847865764209836e-07, 2.0755472475999336e-07, 1.9717698852199353e-07, 1.8731813909589401e-07, 1.7795223214109933e-07, 1.6905462053404427e-07, 1.6060188950734213e-07, 1.5257179503197496e-07, 1.449432052803762e-07, 1.3769604501635731e-07, 1.3081124276553937e-07, 1.2427068062726238e-07, 1.1805714659589932e-07, 1.1215428926610436e-07, 1.0654657480279914e-07, 1.0121924606265918e-07, 9.615828375952623e-08, 9.135036957154992e-08, 8.678285109297238e-08, 8.244370853832384e-08, 7.832152311140766e-08, 7.440544695583724e-08, 7.068517460804532e-08, 6.715091587764308e-08, 6.3793370083761e-08, 6.06037015795729e-08, 5.7573516500594276e-08, 5.4694840675564504e-08, 5.196009864178628e-08, 4.936209370969696e-08, 4.6893989024212115e-08, 4.4549289573001537e-08, 4.232182509435148e-08, 4.020573383963391e-08, 3.8195447147652136e-08, 3.6285674790269544e-08, 3.447139105075608e-08, 3.274782149821828e-08, 3.111043042330734e-08, 2.9554908902142012e-08, 2.8077163457034897e-08, 2.6673305284183135e-08, 2.5339640019973978e-08, 2.4072658018975275e-08, 2.2869025118026533e-08, 2.1725573862125187e-08, 2.0639295169018938e-08]\n",
      "Epoch 14, Batch 0, Train Loss: 0.20880426466464996, Memory (GB): 7\n",
      "Epoch 14, Batch 1, Train Loss: 0.20013129711151123, Memory (GB): 7\n",
      "Epoch 14, Batch 2, Train Loss: 0.21349403262138367, Memory (GB): 7\n",
      "Epoch 14, Batch 3, Train Loss: 0.20953001081943512, Memory (GB): 7\n",
      "Epoch 14, Batch 4, Train Loss: 0.18484799563884735, Memory (GB): 7\n",
      "Epoch 14, Batch 5, Train Loss: 0.17437709867954254, Memory (GB): 7\n",
      "Epoch 14, Batch 6, Train Loss: 0.17801566421985626, Memory (GB): 7\n",
      "Epoch 14, Batch 7, Train Loss: 0.19071653485298157, Memory (GB): 7\n",
      "Epoch 14, Batch 8, Train Loss: 0.196336567401886, Memory (GB): 7\n",
      "Epoch 14, Batch 9, Train Loss: 0.18161217868328094, Memory (GB): 7\n",
      "Epoch 14, Batch 10, Train Loss: 0.1726199984550476, Memory (GB): 7\n",
      "Epoch 14, Batch 11, Train Loss: 0.16870370507240295, Memory (GB): 7\n",
      "Epoch 14, Batch 12, Train Loss: 0.17179961502552032, Memory (GB): 7\n",
      "Epoch 14, Batch 13, Train Loss: 0.201858788728714, Memory (GB): 7\n",
      "Epoch 14, Batch 14, Train Loss: 0.16478869318962097, Memory (GB): 7\n",
      "Epoch 14, Batch 15, Train Loss: 0.18420569598674774, Memory (GB): 7\n",
      "Epoch 14, Batch 16, Train Loss: 0.17242564260959625, Memory (GB): 7\n",
      "Epoch 14, Batch 17, Train Loss: 0.17482982575893402, Memory (GB): 7\n",
      "Epoch 14, Batch 18, Train Loss: 0.19895488023757935, Memory (GB): 7\n",
      "Epoch 14, Batch 19, Train Loss: 0.1974555253982544, Memory (GB): 7\n",
      "Epoch 14, Batch 20, Train Loss: 0.19759699702262878, Memory (GB): 7\n",
      "Epoch 14, Batch 21, Train Loss: 0.1925532966852188, Memory (GB): 7\n",
      "Epoch 14, Batch 22, Train Loss: 0.16673222184181213, Memory (GB): 7\n",
      "Epoch 14, Batch 23, Train Loss: 0.1834549605846405, Memory (GB): 7\n",
      "Epoch 14, Batch 24, Train Loss: 0.1862592250108719, Memory (GB): 7\n",
      "Epoch 14, Batch 25, Train Loss: 0.18165947496891022, Memory (GB): 7\n",
      "Epoch 14, Batch 26, Train Loss: 0.19491522014141083, Memory (GB): 7\n",
      "Epoch 14, Batch 27, Train Loss: 0.1995318979024887, Memory (GB): 7\n",
      "Epoch 14, Batch 28, Train Loss: 0.16147808730602264, Memory (GB): 7\n",
      "Epoch 14, Batch 29, Train Loss: 0.16736243665218353, Memory (GB): 7\n",
      "Epoch 14, Batch 30, Train Loss: 0.20186614990234375, Memory (GB): 7\n",
      "Epoch 14, Batch 31, Train Loss: 0.18360888957977295, Memory (GB): 7\n",
      "Epoch 14, Batch 32, Train Loss: 0.19096414744853973, Memory (GB): 7\n",
      "Epoch 14, Batch 33, Train Loss: 0.1783905029296875, Memory (GB): 7\n",
      "Epoch 14, Batch 34, Train Loss: 0.17942756414413452, Memory (GB): 7\n",
      "Epoch 14, Batch 35, Train Loss: 0.18561308085918427, Memory (GB): 7\n",
      "Epoch 14, Batch 36, Train Loss: 0.1753605455160141, Memory (GB): 7\n",
      "Epoch 14, Batch 37, Train Loss: 0.17339861392974854, Memory (GB): 7\n",
      "Epoch 14, Batch 38, Train Loss: 0.1854563057422638, Memory (GB): 7\n",
      "Epoch 14, Batch 39, Train Loss: 0.18165895342826843, Memory (GB): 7\n",
      "Epoch 14, Batch 40, Train Loss: 0.18470880389213562, Memory (GB): 7\n",
      "Epoch 14, Batch 41, Train Loss: 0.16946496069431305, Memory (GB): 7\n",
      "Epoch 14, Batch 42, Train Loss: 0.1836858093738556, Memory (GB): 7\n",
      "Epoch 14, Batch 43, Train Loss: 0.19064131379127502, Memory (GB): 7\n",
      "Epoch 14, Batch 44, Train Loss: 0.17842647433280945, Memory (GB): 7\n",
      "Epoch 14, Batch 45, Train Loss: 0.16826817393302917, Memory (GB): 7\n",
      "Epoch 14, Batch 46, Train Loss: 0.16960038244724274, Memory (GB): 7\n",
      "Epoch 14, Batch 47, Train Loss: 0.17049963772296906, Memory (GB): 7\n",
      "Epoch 14, Batch 48, Train Loss: 0.1712639480829239, Memory (GB): 7\n",
      "Epoch 14, Batch 49, Train Loss: 0.18249770998954773, Memory (GB): 7\n",
      "Epoch 14, Batch 50, Train Loss: 0.17440736293792725, Memory (GB): 7\n",
      "Epoch 14, Batch 51, Train Loss: 0.20370133221149445, Memory (GB): 7\n",
      "Epoch 14, Batch 52, Train Loss: 0.17134177684783936, Memory (GB): 7\n",
      "Epoch 14, Batch 53, Train Loss: 0.18229131400585175, Memory (GB): 7\n",
      "Epoch 14, Batch 54, Train Loss: 0.1607561856508255, Memory (GB): 7\n",
      "Epoch 14, Batch 55, Train Loss: 0.17982608079910278, Memory (GB): 7\n",
      "Epoch 14, Batch 56, Train Loss: 0.16344301402568817, Memory (GB): 7\n",
      "Epoch 14, Batch 57, Train Loss: 0.18557202816009521, Memory (GB): 7\n",
      "Epoch 14, Batch 58, Train Loss: 0.19576631486415863, Memory (GB): 7\n",
      "Epoch 14, Batch 59, Train Loss: 0.1639813780784607, Memory (GB): 7\n",
      "Epoch 14, Batch 60, Train Loss: 0.18201784789562225, Memory (GB): 7\n",
      "Epoch 14, Batch 61, Train Loss: 0.16377028822898865, Memory (GB): 7\n",
      "Epoch 14, Batch 62, Train Loss: 0.19758827984333038, Memory (GB): 7\n",
      "Epoch 14, Batch 63, Train Loss: 0.17646253108978271, Memory (GB): 7\n",
      "Epoch 14, Batch 64, Train Loss: 0.18738338351249695, Memory (GB): 7\n",
      "Epoch 14, Batch 65, Train Loss: 0.17318157851696014, Memory (GB): 7\n",
      "Epoch 14, Batch 66, Train Loss: 0.1885874718427658, Memory (GB): 7\n",
      "Epoch 14, Batch 67, Train Loss: 0.16695474088191986, Memory (GB): 7\n",
      "Epoch 14, Batch 68, Train Loss: 0.17969129979610443, Memory (GB): 7\n",
      "Epoch 14, Batch 69, Train Loss: 0.19465966522693634, Memory (GB): 7\n",
      "Epoch 14, Batch 70, Train Loss: 0.17985770106315613, Memory (GB): 7\n",
      "Epoch 14, Batch 71, Train Loss: 0.19145458936691284, Memory (GB): 7\n",
      "Epoch 14, Batch 72, Train Loss: 0.190714493393898, Memory (GB): 7\n",
      "Epoch 14, Batch 73, Train Loss: 0.20346419513225555, Memory (GB): 7\n",
      "Epoch 14, Batch 74, Train Loss: 0.17998933792114258, Memory (GB): 7\n",
      "Epoch 14, Batch 75, Train Loss: 0.19300737977027893, Memory (GB): 7\n",
      "Epoch 14, Batch 76, Train Loss: 0.2375621497631073, Memory (GB): 7\n",
      "Epoch 14, Batch 77, Train Loss: 0.18895097076892853, Memory (GB): 7\n",
      "Epoch 14, Batch 78, Train Loss: 0.21489138901233673, Memory (GB): 7\n",
      "Epoch 14, Batch 79, Train Loss: 0.21834561228752136, Memory (GB): 7\n",
      "Epoch 14, Batch 80, Train Loss: 0.18908709287643433, Memory (GB): 7\n",
      "Epoch 14, Batch 81, Train Loss: 0.2118082046508789, Memory (GB): 7\n",
      "Epoch 14, Batch 82, Train Loss: 0.21875545382499695, Memory (GB): 7\n",
      "Epoch 14, Batch 83, Train Loss: 0.20232360064983368, Memory (GB): 7\n",
      "Epoch 14, Batch 84, Train Loss: 0.18446899950504303, Memory (GB): 7\n",
      "Epoch 14, Batch 85, Train Loss: 0.18198953568935394, Memory (GB): 7\n",
      "Epoch 14, Batch 86, Train Loss: 0.19951574504375458, Memory (GB): 7\n",
      "Epoch 14, Batch 87, Train Loss: 0.2077270746231079, Memory (GB): 7\n",
      "Epoch 14, Batch 88, Train Loss: 0.18940332531929016, Memory (GB): 7\n",
      "Epoch 14, Batch 89, Train Loss: 0.21346667408943176, Memory (GB): 7\n",
      "Epoch 14, Batch 90, Train Loss: 0.2116391807794571, Memory (GB): 7\n",
      "Epoch 14, Batch 91, Train Loss: 0.20644932985305786, Memory (GB): 7\n",
      "Epoch 14, Batch 92, Train Loss: 0.17232029139995575, Memory (GB): 7\n",
      "Epoch 14, Batch 93, Train Loss: 0.1833680421113968, Memory (GB): 7\n",
      "Epoch 14, Batch 94, Train Loss: 0.185040682554245, Memory (GB): 7\n",
      "Epoch 14, Batch 95, Train Loss: 0.1836450695991516, Memory (GB): 7\n",
      "Epoch 14, Batch 96, Train Loss: 0.1752636581659317, Memory (GB): 7\n",
      "Epoch 14, Batch 97, Train Loss: 0.193895161151886, Memory (GB): 7\n",
      "Epoch 14, Batch 98, Train Loss: 0.18898022174835205, Memory (GB): 7\n",
      "Epoch 14, Batch 99, Train Loss: 0.16132354736328125, Memory (GB): 7\n",
      "Epoch 14, Batch 100, Train Loss: 0.20762689411640167, Memory (GB): 7\n",
      "Epoch 14, Batch 101, Train Loss: 0.2156151831150055, Memory (GB): 7\n",
      "Epoch 14, Batch 102, Train Loss: 0.20261040329933167, Memory (GB): 7\n",
      "Epoch 14, Batch 103, Train Loss: 0.1886776238679886, Memory (GB): 7\n",
      "Epoch 14, Batch 104, Train Loss: 0.1789529025554657, Memory (GB): 7\n",
      "Epoch 14, Batch 105, Train Loss: 0.20765183866024017, Memory (GB): 7\n",
      "Epoch 14, Batch 106, Train Loss: 0.2176736444234848, Memory (GB): 7\n",
      "Epoch 14, Batch 107, Train Loss: 0.17825564742088318, Memory (GB): 7\n",
      "Epoch 14, Batch 108, Train Loss: 0.1771184206008911, Memory (GB): 7\n",
      "Epoch 14, Batch 109, Train Loss: 0.19933722913265228, Memory (GB): 7\n",
      "Epoch 14, Batch 110, Train Loss: 0.2079918533563614, Memory (GB): 7\n",
      "Epoch 14, Batch 111, Train Loss: 0.19881273806095123, Memory (GB): 7\n",
      "Epoch 14, Batch 112, Train Loss: 0.16168609261512756, Memory (GB): 7\n",
      "Epoch 14, Batch 113, Train Loss: 0.1750376969575882, Memory (GB): 7\n",
      "Epoch 14, Batch 114, Train Loss: 0.20465855300426483, Memory (GB): 7\n",
      "Epoch 14, Batch 115, Train Loss: 0.22262069582939148, Memory (GB): 7\n",
      "Epoch 14, Batch 116, Train Loss: 0.21261410415172577, Memory (GB): 7\n",
      "Epoch 14, Batch 117, Train Loss: 0.21199394762516022, Memory (GB): 7\n",
      "Epoch 14, Batch 118, Train Loss: 0.18819482624530792, Memory (GB): 7\n",
      "Epoch 14, Batch 119, Train Loss: 0.16878411173820496, Memory (GB): 7\n",
      "Epoch 14, Batch 120, Train Loss: 0.17091812193393707, Memory (GB): 7\n",
      "Epoch 14, Batch 121, Train Loss: 0.1789727658033371, Memory (GB): 7\n",
      "Epoch 14, Batch 122, Train Loss: 0.20128795504570007, Memory (GB): 7\n",
      "Epoch 14, Batch 123, Train Loss: 0.21684587001800537, Memory (GB): 7\n",
      "Epoch 14, Batch 124, Train Loss: 0.20670518279075623, Memory (GB): 7\n",
      "Epoch 14, Batch 125, Train Loss: 0.18931476771831512, Memory (GB): 7\n",
      "Epoch 14, Batch 126, Train Loss: 0.20210954546928406, Memory (GB): 7\n",
      "Epoch 14, Batch 127, Train Loss: 0.17384642362594604, Memory (GB): 7\n",
      "Epoch 14, Batch 128, Train Loss: 0.16757497191429138, Memory (GB): 7\n",
      "Epoch 14, Batch 129, Train Loss: 0.19406083226203918, Memory (GB): 7\n",
      "Epoch 14, Batch 130, Train Loss: 0.17486782371997833, Memory (GB): 7\n",
      "Epoch 14, Batch 131, Train Loss: 0.18537381291389465, Memory (GB): 7\n",
      "Epoch 14, Batch 132, Train Loss: 0.18751753866672516, Memory (GB): 7\n",
      "Epoch 14, Batch 133, Train Loss: 0.18238960206508636, Memory (GB): 7\n",
      "Epoch 14, Batch 134, Train Loss: 0.21205034852027893, Memory (GB): 7\n",
      "Epoch 14, Batch 135, Train Loss: 0.18693341314792633, Memory (GB): 7\n",
      "Epoch 14, Batch 136, Train Loss: 0.16729944944381714, Memory (GB): 7\n",
      "Epoch 14, Batch 137, Train Loss: 0.1936095952987671, Memory (GB): 7\n",
      "Epoch 14, Batch 138, Train Loss: 0.20165039598941803, Memory (GB): 7\n",
      "Epoch 14, Batch 139, Train Loss: 0.19883610308170319, Memory (GB): 7\n",
      "Epoch 14, Batch 140, Train Loss: 0.18310606479644775, Memory (GB): 7\n",
      "Epoch 14, Batch 141, Train Loss: 0.18704040348529816, Memory (GB): 7\n",
      "Epoch 14, Batch 142, Train Loss: 0.16141462326049805, Memory (GB): 7\n",
      "Epoch 14, Batch 143, Train Loss: 0.1823062002658844, Memory (GB): 7\n",
      "Epoch 14, Batch 144, Train Loss: 0.17856398224830627, Memory (GB): 7\n",
      "Epoch 14, Batch 145, Train Loss: 0.17268159985542297, Memory (GB): 7\n",
      "Epoch 14, Batch 146, Train Loss: 0.18860673904418945, Memory (GB): 7\n",
      "Epoch 14, Batch 147, Train Loss: 0.177677184343338, Memory (GB): 7\n",
      "Epoch 14, Batch 148, Train Loss: 0.19842424988746643, Memory (GB): 7\n",
      "Epoch 14, Batch 149, Train Loss: 0.17384615540504456, Memory (GB): 7\n",
      "Epoch 14, Batch 150, Train Loss: 0.1675160974264145, Memory (GB): 7\n",
      "Epoch 14, Batch 151, Train Loss: 0.21335597336292267, Memory (GB): 7\n",
      "Epoch 14, Batch 152, Train Loss: 0.17689257860183716, Memory (GB): 7\n",
      "Epoch 14, Batch 153, Train Loss: 0.17955677211284637, Memory (GB): 7\n",
      "Epoch 14, Batch 154, Train Loss: 0.1955842524766922, Memory (GB): 7\n",
      "Epoch 14, Batch 155, Train Loss: 0.18559366464614868, Memory (GB): 7\n",
      "Epoch 14, Batch 156, Train Loss: 0.1819966584444046, Memory (GB): 7\n",
      "Epoch 14, Batch 157, Train Loss: 0.2023671418428421, Memory (GB): 7\n",
      "Epoch 14, Batch 158, Train Loss: 0.20814146101474762, Memory (GB): 7\n",
      "Epoch 14, Batch 159, Train Loss: 0.19500741362571716, Memory (GB): 7\n",
      "Epoch 14, Batch 160, Train Loss: 0.21165552735328674, Memory (GB): 7\n",
      "Epoch 14, Batch 161, Train Loss: 0.19539502263069153, Memory (GB): 7\n",
      "Epoch 14, Batch 162, Train Loss: 0.20889490842819214, Memory (GB): 7\n",
      "Epoch 14, Batch 163, Train Loss: 0.20257486402988434, Memory (GB): 7\n",
      "Epoch 14, Batch 164, Train Loss: 0.1866532266139984, Memory (GB): 7\n",
      "Epoch 14, Batch 165, Train Loss: 0.1677616387605667, Memory (GB): 7\n",
      "Epoch 14, Batch 166, Train Loss: 0.1965566873550415, Memory (GB): 7\n",
      "Epoch 14, Batch 167, Train Loss: 0.20766201615333557, Memory (GB): 7\n",
      "Epoch 14, Batch 168, Train Loss: 0.22061479091644287, Memory (GB): 7\n",
      "Epoch 14, Batch 169, Train Loss: 0.17622049152851105, Memory (GB): 7\n",
      "Epoch 14, Batch 170, Train Loss: 0.20188657939434052, Memory (GB): 7\n",
      "Epoch 14, Batch 171, Train Loss: 0.20410606265068054, Memory (GB): 7\n",
      "Epoch 14, Batch 172, Train Loss: 0.1972610354423523, Memory (GB): 7\n",
      "Epoch 14, Batch 173, Train Loss: 0.20312608778476715, Memory (GB): 7\n",
      "Epoch 14, Batch 174, Train Loss: 0.20931538939476013, Memory (GB): 7\n",
      "Epoch 14, Batch 175, Train Loss: 0.20427286624908447, Memory (GB): 7\n",
      "Epoch 14, Batch 176, Train Loss: 0.18766914308071136, Memory (GB): 7\n",
      "Epoch 14, Batch 177, Train Loss: 0.19099466502666473, Memory (GB): 7\n",
      "Epoch 14, Batch 178, Train Loss: 0.1964666247367859, Memory (GB): 7\n",
      "Epoch 14, Batch 179, Train Loss: 0.2096036672592163, Memory (GB): 7\n",
      "[0.0009019509754878959, 0.0008568534267135013, 0.0008140107553778266, 0.0007733102176089347, 0.0007346447067284879, 0.0006979124713920633, 0.0006630168478224601, 0.0006298660054313371, 0.0005983727051597702, 0.0005684540699017819, 0.0005400313664066924, 0.0005130297980863579, 0.00048737830818203985, 0.00046300939277293794, 0.00043985892313429107, 0.0004178659769775763, 0.0003969726781286976, 0.0003771240442222628, 0.0003582678420111497, 0.00034035444991059214, 0.00032333672741506245, 0.0003071698910443095, 0.0002918113964920939, 0.0002772208266674891, 0.00026335978533411454, 0.00025019179606740896, 0.00023768220626403842, 0.00022579809595083648, 0.00021450819115329458, 0.0002037827815956299, 0.00019359364251584843, 0.00018391396039005607, 0.00017471826237055313, 0.0001659823492520255, 0.0001576832317894242, 0.0001497990701999529, 0.00014230911668995527, 0.00013519366085545755, 0.00012843397781268464, 0.00012201227892205045, 0.0001159116649759479, 0.0001101160817271505, 0.00010461027764079295, 9.937976375875329e-05, 9.44107755708156e-05, 8.969023679227492e-05, 8.520572495266107e-05, 8.094543870502801e-05, 7.689816676977664e-05, 7.30532584312878e-05, 6.940059550972337e-05, 6.593056573423724e-05, 6.263403744752536e-05, 5.950233557514913e-05, 5.652721879639164e-05, 5.370085785657207e-05, 5.1015814963743454e-05, 4.84650242155563e-05, 4.604177300477847e-05, 4.3739684354539545e-05, 4.155270013681257e-05, 3.947506512997194e-05, 3.750131187347334e-05, 3.562624627979967e-05, 3.3844933965809676e-05, 3.215268726751921e-05, 3.054505290414323e-05, 2.9017800258936056e-05, 2.7566910245989257e-05, 2.6188564733689795e-05, 2.4879136497005298e-05, 2.363517967215504e-05, 2.2453420688547288e-05, 2.1330749654119912e-05, 2.0264212171413916e-05, 1.925100156284322e-05, 1.8288451484701057e-05, 1.737402891046601e-05, 1.650532746494271e-05, 1.5680061091695564e-05, 1.4896058037110797e-05, 1.4151255135255258e-05, 1.3443692378492491e-05, 1.2771507759567863e-05, 1.2132932371589477e-05, 1.1526285753010002e-05, 1.0949971465359496e-05, 1.0402472892091525e-05, 9.882349247486946e-06, 9.388231785112595e-06, 8.918820195856967e-06, 8.47287918606412e-06, 8.04923522676091e-06, 7.64677346542287e-06, 7.264434792151726e-06, 6.901213052544138e-06, 6.556152399916931e-06, 6.228344779921083e-06, 5.916927540925028e-06, 5.621081163878778e-06, 5.340027105684841e-06, 5.0730257504005974e-06, 4.819374462880567e-06, 4.578405739736539e-06, 4.349485452749711e-06, 4.132011180112226e-06, 3.925410621106614e-06, 3.7291400900512806e-06, 3.542683085548717e-06, 3.365548931271282e-06, 3.1972714847077165e-06, 3.0374079104723323e-06, 2.8855375149487164e-06, 2.7412606392012796e-06, 2.6041976072412162e-06, 2.473987726879155e-06, 2.350288340535195e-06, 2.2327739235084358e-06, 2.1211352273330143e-06, 2.0150784659663643e-06, 1.914324542668046e-06, 1.818608315534644e-06, 1.727677899757911e-06, 1.6412940047700153e-06, 1.559229304531515e-06, 1.481267839304939e-06, 1.4072044473396924e-06, 1.336844224972707e-06, 1.2700020137240725e-06, 1.2065019130378686e-06, 1.146176817385974e-06, 1.0888679765166759e-06, 1.0344245776908425e-06, 9.827033488062998e-07, 9.335681813659853e-07, 8.868897722976858e-07, 8.42545283682801e-07, 8.004180194986615e-07, 7.603971185237279e-07, 7.223772625975414e-07, 6.862583994676643e-07, 6.519454794942813e-07, 6.193482055195675e-07, 5.883807952435886e-07, 5.589617554814096e-07, 5.310136677073389e-07, 5.044629843219717e-07, 4.792398351058731e-07, 4.552778433505797e-07, 4.325139511830503e-07, 4.108882536238979e-07, 3.9034384094270304e-07, 3.70826648895568e-07, 3.522853164507894e-07, 3.346710506282501e-07, 3.1793749809683755e-07, 3.020406231919955e-07, 2.8693859203239576e-07, 2.72591662430776e-07, 2.589620793092372e-07, 2.4601397534377545e-07, 2.3371327657658646e-07, 2.220276127477572e-07, 2.109262321103693e-07, 2.003799205048508e-07, 1.9036092447960822e-07, 1.808428782556278e-07, 1.7180073434284646e-07, 1.632106976257041e-07, 1.550501627444189e-07, 1.4729765460719786e-07, 1.3993277187683803e-07, 1.3293613328299617e-07, 1.262893266188463e-07, 1.1997486028790394e-07, 1.1397611727350875e-07, 1.0827731140983332e-07, 1.028634458393417e-07, 9.772027354737462e-08, 9.283425987000595e-08, 8.819254687650557e-08, 8.378291953268028e-08, 7.95937735560463e-08, 7.561408487824393e-08, 7.183338063433175e-08, 6.824171160261515e-08, 6.482962602248438e-08, 6.158814472136017e-08, 5.850873748529215e-08, 5.558330061102754e-08, 5.280413558047616e-08, 5.016392880145235e-08, 4.7655732361379706e-08, 4.527294574331073e-08, 4.30092984561452e-08, 4.0858833533337913e-08, 3.881589185667104e-08, 3.687509726383749e-08, 3.5031342400645645e-08]\n",
      "Epoch 15, Batch 0, Train Loss: 0.19161202013492584, Memory (GB): 7\n",
      "Epoch 15, Batch 1, Train Loss: 0.17137877643108368, Memory (GB): 7\n",
      "Epoch 15, Batch 2, Train Loss: 0.16021408140659332, Memory (GB): 7\n",
      "Epoch 15, Batch 3, Train Loss: 0.1881900578737259, Memory (GB): 7\n",
      "Epoch 15, Batch 4, Train Loss: 0.18811316788196564, Memory (GB): 7\n",
      "Epoch 15, Batch 5, Train Loss: 0.15874898433685303, Memory (GB): 7\n",
      "Epoch 15, Batch 6, Train Loss: 0.19792917370796204, Memory (GB): 7\n",
      "Epoch 15, Batch 7, Train Loss: 0.2096833884716034, Memory (GB): 7\n",
      "Epoch 15, Batch 8, Train Loss: 0.18533381819725037, Memory (GB): 7\n",
      "Epoch 15, Batch 9, Train Loss: 0.17279557883739471, Memory (GB): 7\n",
      "Epoch 15, Batch 10, Train Loss: 0.21427103877067566, Memory (GB): 7\n",
      "Epoch 15, Batch 11, Train Loss: 0.17609168589115143, Memory (GB): 7\n",
      "Epoch 15, Batch 12, Train Loss: 0.19077080488204956, Memory (GB): 7\n",
      "Epoch 15, Batch 13, Train Loss: 0.19556982815265656, Memory (GB): 7\n",
      "Epoch 15, Batch 14, Train Loss: 0.1879134178161621, Memory (GB): 7\n",
      "Epoch 15, Batch 15, Train Loss: 0.17972934246063232, Memory (GB): 7\n",
      "Epoch 15, Batch 16, Train Loss: 0.17739354074001312, Memory (GB): 7\n",
      "Epoch 15, Batch 17, Train Loss: 0.19664782285690308, Memory (GB): 7\n",
      "Epoch 15, Batch 18, Train Loss: 0.20771090686321259, Memory (GB): 7\n",
      "Epoch 15, Batch 19, Train Loss: 0.19441305100917816, Memory (GB): 7\n",
      "Epoch 15, Batch 20, Train Loss: 0.18979868292808533, Memory (GB): 7\n",
      "Epoch 15, Batch 21, Train Loss: 0.17766062915325165, Memory (GB): 7\n",
      "Epoch 15, Batch 22, Train Loss: 0.16818931698799133, Memory (GB): 7\n",
      "Epoch 15, Batch 23, Train Loss: 0.20539024472236633, Memory (GB): 7\n",
      "Epoch 15, Batch 24, Train Loss: 0.1819160133600235, Memory (GB): 7\n",
      "Epoch 15, Batch 25, Train Loss: 0.15605250000953674, Memory (GB): 7\n",
      "Epoch 15, Batch 26, Train Loss: 0.1818331778049469, Memory (GB): 7\n",
      "Epoch 15, Batch 27, Train Loss: 0.18882735073566437, Memory (GB): 7\n",
      "Epoch 15, Batch 28, Train Loss: 0.15738271176815033, Memory (GB): 7\n",
      "Epoch 15, Batch 29, Train Loss: 0.18374736607074738, Memory (GB): 7\n",
      "Epoch 15, Batch 30, Train Loss: 0.16825711727142334, Memory (GB): 7\n",
      "Epoch 15, Batch 31, Train Loss: 0.17485341429710388, Memory (GB): 7\n",
      "Epoch 15, Batch 32, Train Loss: 0.1678164154291153, Memory (GB): 7\n",
      "Epoch 15, Batch 33, Train Loss: 0.1763734519481659, Memory (GB): 7\n",
      "Epoch 15, Batch 34, Train Loss: 0.18745505809783936, Memory (GB): 7\n",
      "Epoch 15, Batch 35, Train Loss: 0.15770578384399414, Memory (GB): 7\n",
      "Epoch 15, Batch 36, Train Loss: 0.1810712069272995, Memory (GB): 7\n",
      "Epoch 15, Batch 37, Train Loss: 0.1717817634344101, Memory (GB): 7\n",
      "Epoch 15, Batch 38, Train Loss: 0.16984964907169342, Memory (GB): 7\n",
      "Epoch 15, Batch 39, Train Loss: 0.1929241418838501, Memory (GB): 7\n",
      "Epoch 15, Batch 40, Train Loss: 0.17285805940628052, Memory (GB): 7\n",
      "Epoch 15, Batch 41, Train Loss: 0.16771666705608368, Memory (GB): 7\n",
      "Epoch 15, Batch 42, Train Loss: 0.17755524814128876, Memory (GB): 7\n",
      "Epoch 15, Batch 43, Train Loss: 0.1736384630203247, Memory (GB): 7\n",
      "Epoch 15, Batch 44, Train Loss: 0.17197631299495697, Memory (GB): 7\n",
      "Epoch 15, Batch 45, Train Loss: 0.17658142745494843, Memory (GB): 7\n",
      "Epoch 15, Batch 46, Train Loss: 0.16810433566570282, Memory (GB): 7\n",
      "Epoch 15, Batch 47, Train Loss: 0.1882622390985489, Memory (GB): 7\n",
      "Epoch 15, Batch 48, Train Loss: 0.1630713939666748, Memory (GB): 7\n",
      "Epoch 15, Batch 49, Train Loss: 0.1799437701702118, Memory (GB): 7\n",
      "Epoch 15, Batch 50, Train Loss: 0.1655031442642212, Memory (GB): 7\n",
      "Epoch 15, Batch 51, Train Loss: 0.17053915560245514, Memory (GB): 7\n",
      "Epoch 15, Batch 52, Train Loss: 0.1844147890806198, Memory (GB): 7\n",
      "Epoch 15, Batch 53, Train Loss: 0.16409866511821747, Memory (GB): 7\n",
      "Epoch 15, Batch 54, Train Loss: 0.17126768827438354, Memory (GB): 7\n",
      "Epoch 15, Batch 55, Train Loss: 0.20016691088676453, Memory (GB): 7\n",
      "Epoch 15, Batch 56, Train Loss: 0.1532932072877884, Memory (GB): 7\n",
      "Epoch 15, Batch 57, Train Loss: 0.1769474595785141, Memory (GB): 7\n",
      "Epoch 15, Batch 58, Train Loss: 0.18449056148529053, Memory (GB): 7\n",
      "Epoch 15, Batch 59, Train Loss: 0.16203315556049347, Memory (GB): 7\n",
      "Epoch 15, Batch 60, Train Loss: 0.201818585395813, Memory (GB): 7\n",
      "Epoch 15, Batch 61, Train Loss: 0.16417118906974792, Memory (GB): 7\n",
      "Epoch 15, Batch 62, Train Loss: 0.18683941662311554, Memory (GB): 7\n",
      "Epoch 15, Batch 63, Train Loss: 0.1666802614927292, Memory (GB): 7\n",
      "Epoch 15, Batch 64, Train Loss: 0.17796434462070465, Memory (GB): 7\n",
      "Epoch 15, Batch 65, Train Loss: 0.16303855180740356, Memory (GB): 7\n",
      "Epoch 15, Batch 66, Train Loss: 0.17571133375167847, Memory (GB): 7\n",
      "Epoch 15, Batch 67, Train Loss: 0.18987897038459778, Memory (GB): 7\n",
      "Epoch 15, Batch 68, Train Loss: 0.18519312143325806, Memory (GB): 7\n",
      "Epoch 15, Batch 69, Train Loss: 0.163752943277359, Memory (GB): 7\n",
      "Epoch 15, Batch 70, Train Loss: 0.16495488584041595, Memory (GB): 7\n",
      "Epoch 15, Batch 71, Train Loss: 0.15788277983665466, Memory (GB): 7\n",
      "Epoch 15, Batch 72, Train Loss: 0.16874387860298157, Memory (GB): 7\n",
      "Epoch 15, Batch 73, Train Loss: 0.1611662656068802, Memory (GB): 7\n",
      "Epoch 15, Batch 74, Train Loss: 0.17539680004119873, Memory (GB): 7\n",
      "Epoch 15, Batch 75, Train Loss: 0.18239633738994598, Memory (GB): 7\n",
      "Epoch 15, Batch 76, Train Loss: 0.1974271833896637, Memory (GB): 7\n",
      "Epoch 15, Batch 77, Train Loss: 0.1770775318145752, Memory (GB): 7\n",
      "Epoch 15, Batch 78, Train Loss: 0.19089505076408386, Memory (GB): 7\n",
      "Epoch 15, Batch 79, Train Loss: 0.16643059253692627, Memory (GB): 7\n",
      "Epoch 15, Batch 80, Train Loss: 0.18453016877174377, Memory (GB): 7\n",
      "Epoch 15, Batch 81, Train Loss: 0.18680067360401154, Memory (GB): 7\n",
      "Epoch 15, Batch 82, Train Loss: 0.1815505027770996, Memory (GB): 7\n",
      "Epoch 15, Batch 83, Train Loss: 0.16958661377429962, Memory (GB): 7\n",
      "Epoch 15, Batch 84, Train Loss: 0.15932762622833252, Memory (GB): 7\n",
      "Epoch 15, Batch 85, Train Loss: 0.17875789105892181, Memory (GB): 7\n",
      "Epoch 15, Batch 86, Train Loss: 0.17584054172039032, Memory (GB): 7\n",
      "Epoch 15, Batch 87, Train Loss: 0.16822221875190735, Memory (GB): 7\n",
      "Epoch 15, Batch 88, Train Loss: 0.19836780428886414, Memory (GB): 7\n",
      "Epoch 15, Batch 89, Train Loss: 0.1831379234790802, Memory (GB): 7\n",
      "Epoch 15, Batch 90, Train Loss: 0.1927562654018402, Memory (GB): 7\n",
      "Epoch 15, Batch 91, Train Loss: 0.17144520580768585, Memory (GB): 7\n",
      "Epoch 15, Batch 92, Train Loss: 0.16683080792427063, Memory (GB): 7\n",
      "Epoch 15, Batch 93, Train Loss: 0.16936564445495605, Memory (GB): 7\n",
      "Epoch 15, Batch 94, Train Loss: 0.17080166935920715, Memory (GB): 7\n",
      "Epoch 15, Batch 95, Train Loss: 0.18733876943588257, Memory (GB): 7\n",
      "Epoch 15, Batch 96, Train Loss: 0.16768577694892883, Memory (GB): 7\n",
      "Epoch 15, Batch 97, Train Loss: 0.15763142704963684, Memory (GB): 7\n",
      "Epoch 15, Batch 98, Train Loss: 0.16319429874420166, Memory (GB): 7\n",
      "Epoch 15, Batch 99, Train Loss: 0.1860249638557434, Memory (GB): 7\n",
      "Epoch 15, Batch 100, Train Loss: 0.178239643573761, Memory (GB): 7\n",
      "Epoch 15, Batch 101, Train Loss: 0.1960291564464569, Memory (GB): 7\n",
      "Epoch 15, Batch 102, Train Loss: 0.1762937307357788, Memory (GB): 7\n",
      "Epoch 15, Batch 103, Train Loss: 0.1992262899875641, Memory (GB): 7\n",
      "Epoch 15, Batch 104, Train Loss: 0.16170448064804077, Memory (GB): 7\n",
      "Epoch 15, Batch 105, Train Loss: 0.1636831909418106, Memory (GB): 7\n",
      "Epoch 15, Batch 106, Train Loss: 0.1857372373342514, Memory (GB): 7\n",
      "Epoch 15, Batch 107, Train Loss: 0.18213774263858795, Memory (GB): 7\n",
      "Epoch 15, Batch 108, Train Loss: 0.190273255109787, Memory (GB): 7\n",
      "Epoch 15, Batch 109, Train Loss: 0.17331969738006592, Memory (GB): 7\n",
      "Epoch 15, Batch 110, Train Loss: 0.19817915558815002, Memory (GB): 7\n",
      "Epoch 15, Batch 111, Train Loss: 0.1870799958705902, Memory (GB): 7\n",
      "Epoch 15, Batch 112, Train Loss: 0.22213390469551086, Memory (GB): 7\n",
      "Epoch 15, Batch 113, Train Loss: 0.2072518914937973, Memory (GB): 7\n",
      "Epoch 15, Batch 114, Train Loss: 0.21867410838603973, Memory (GB): 7\n",
      "Epoch 15, Batch 115, Train Loss: 0.19648057222366333, Memory (GB): 7\n",
      "Epoch 15, Batch 116, Train Loss: 0.1811022013425827, Memory (GB): 7\n",
      "Epoch 15, Batch 117, Train Loss: 0.1961517632007599, Memory (GB): 7\n",
      "Epoch 15, Batch 118, Train Loss: 0.185252383351326, Memory (GB): 7\n",
      "Epoch 15, Batch 119, Train Loss: 0.1941005438566208, Memory (GB): 7\n",
      "Epoch 15, Batch 120, Train Loss: 0.21235011518001556, Memory (GB): 7\n",
      "Epoch 15, Batch 121, Train Loss: 0.18825064599514008, Memory (GB): 7\n",
      "Epoch 15, Batch 122, Train Loss: 0.19581806659698486, Memory (GB): 7\n",
      "Epoch 15, Batch 123, Train Loss: 0.1920069009065628, Memory (GB): 7\n",
      "Epoch 15, Batch 124, Train Loss: 0.2004067748785019, Memory (GB): 7\n",
      "Epoch 15, Batch 125, Train Loss: 0.1989668905735016, Memory (GB): 7\n",
      "Epoch 15, Batch 126, Train Loss: 0.18153685331344604, Memory (GB): 7\n",
      "Epoch 15, Batch 127, Train Loss: 0.2011708766222, Memory (GB): 7\n",
      "Epoch 15, Batch 128, Train Loss: 0.19141791760921478, Memory (GB): 7\n",
      "Epoch 15, Batch 129, Train Loss: 0.1860789954662323, Memory (GB): 7\n",
      "Epoch 15, Batch 130, Train Loss: 0.18814170360565186, Memory (GB): 7\n",
      "Epoch 15, Batch 131, Train Loss: 0.20260706543922424, Memory (GB): 7\n",
      "Epoch 15, Batch 132, Train Loss: 0.18935859203338623, Memory (GB): 7\n",
      "Epoch 15, Batch 133, Train Loss: 0.18148906528949738, Memory (GB): 7\n",
      "Epoch 15, Batch 134, Train Loss: 0.1858348399400711, Memory (GB): 7\n",
      "Epoch 15, Batch 135, Train Loss: 0.18332388997077942, Memory (GB): 7\n",
      "Epoch 15, Batch 136, Train Loss: 0.16686269640922546, Memory (GB): 7\n",
      "Epoch 15, Batch 137, Train Loss: 0.17225758731365204, Memory (GB): 7\n",
      "Epoch 15, Batch 138, Train Loss: 0.1948365718126297, Memory (GB): 7\n",
      "Epoch 15, Batch 139, Train Loss: 0.18975746631622314, Memory (GB): 7\n",
      "Epoch 15, Batch 140, Train Loss: 0.21189367771148682, Memory (GB): 7\n",
      "Epoch 15, Batch 141, Train Loss: 0.19413071870803833, Memory (GB): 7\n",
      "Epoch 15, Batch 142, Train Loss: 0.1787773072719574, Memory (GB): 7\n",
      "Epoch 15, Batch 143, Train Loss: 0.17892059683799744, Memory (GB): 7\n",
      "Epoch 15, Batch 144, Train Loss: 0.16795362532138824, Memory (GB): 7\n",
      "Epoch 15, Batch 145, Train Loss: 0.18845929205417633, Memory (GB): 7\n",
      "Epoch 15, Batch 146, Train Loss: 0.17935530841350555, Memory (GB): 7\n",
      "Epoch 15, Batch 147, Train Loss: 0.19667568802833557, Memory (GB): 7\n",
      "Epoch 15, Batch 148, Train Loss: 0.16456249356269836, Memory (GB): 7\n",
      "Epoch 15, Batch 149, Train Loss: 0.20240463316440582, Memory (GB): 7\n",
      "Epoch 15, Batch 150, Train Loss: 0.17161144316196442, Memory (GB): 7\n",
      "Epoch 15, Batch 151, Train Loss: 0.16614508628845215, Memory (GB): 7\n",
      "Epoch 15, Batch 152, Train Loss: 0.16948726773262024, Memory (GB): 7\n",
      "Epoch 15, Batch 153, Train Loss: 0.17871548235416412, Memory (GB): 7\n",
      "Epoch 15, Batch 154, Train Loss: 0.1599317193031311, Memory (GB): 7\n",
      "Epoch 15, Batch 155, Train Loss: 0.16666950285434723, Memory (GB): 7\n",
      "Epoch 15, Batch 156, Train Loss: 0.17712906002998352, Memory (GB): 7\n",
      "Epoch 15, Batch 157, Train Loss: 0.19882476329803467, Memory (GB): 7\n",
      "Epoch 15, Batch 158, Train Loss: 0.16171881556510925, Memory (GB): 7\n",
      "Epoch 15, Batch 159, Train Loss: 0.16701120138168335, Memory (GB): 7\n",
      "Epoch 15, Batch 160, Train Loss: 0.1841442734003067, Memory (GB): 7\n",
      "Epoch 15, Batch 161, Train Loss: 0.1630243957042694, Memory (GB): 7\n",
      "Epoch 15, Batch 162, Train Loss: 0.16393324732780457, Memory (GB): 7\n",
      "Epoch 15, Batch 163, Train Loss: 0.19095146656036377, Memory (GB): 7\n",
      "Epoch 15, Batch 164, Train Loss: 0.18789850175380707, Memory (GB): 7\n",
      "Epoch 15, Batch 165, Train Loss: 0.1776711642742157, Memory (GB): 7\n",
      "Epoch 15, Batch 166, Train Loss: 0.19331543147563934, Memory (GB): 7\n",
      "Epoch 15, Batch 167, Train Loss: 0.17369219660758972, Memory (GB): 7\n",
      "Epoch 15, Batch 168, Train Loss: 0.1666894257068634, Memory (GB): 7\n",
      "Epoch 15, Batch 169, Train Loss: 0.1674480140209198, Memory (GB): 7\n",
      "Epoch 15, Batch 170, Train Loss: 0.16222785413265228, Memory (GB): 7\n",
      "Epoch 15, Batch 171, Train Loss: 0.1580319106578827, Memory (GB): 7\n",
      "Epoch 15, Batch 172, Train Loss: 0.16887561976909637, Memory (GB): 7\n",
      "Epoch 15, Batch 173, Train Loss: 0.18549786508083344, Memory (GB): 7\n",
      "Epoch 15, Batch 174, Train Loss: 0.1632016897201538, Memory (GB): 7\n",
      "Epoch 15, Batch 175, Train Loss: 0.18002139031887054, Memory (GB): 7\n",
      "Epoch 15, Batch 176, Train Loss: 0.1915714591741562, Memory (GB): 7\n",
      "Epoch 15, Batch 177, Train Loss: 0.17249464988708496, Memory (GB): 7\n",
      "Epoch 15, Batch 178, Train Loss: 0.18566825985908508, Memory (GB): 7\n",
      "Epoch 15, Batch 179, Train Loss: 0.1819942742586136, Memory (GB): 7\n",
      "[0.0006492698098664242, 0.0006168063193731029, 0.0005859660034044475, 0.0005566677032342254, 0.000528834318072514, 0.0005023926021688882, 0.00047727297206044385, 0.0004534093234574217, 0.0004307388572845505, 0.0004092019144203228, 0.0003887418186993067, 0.00036930472776434145, 0.0003508394913761241, 0.0003332975168073181, 0.00031663264096695205, 0.0003008010089186047, 0.0002857609584726743, 0.00027147291054904055, 0.0002578992650215886, 0.0002450043017705091, 0.00023275408668198364, 0.0002211163823478844, 0.0002100605632304902, 0.00019955753506896556, 0.00018957965831551736, 0.0001801006753997414, 0.00017109564162975436, 0.00016254085954826665, 0.00015441381657085333, 0.0001466931257423106, 0.0001393584694551951, 0.0001323905459824353, 0.00012577101868331355, 0.00011948246774914786, 0.00011350834436169045, 0.00010783292714360591, 0.00010244128078642566, 9.731921674710435e-05, 9.245325590974918e-05, 8.783059311426162e-05, 8.343906345854857e-05, 7.926711028562116e-05, 7.530375477134005e-05, 7.153856703277309e-05, 6.796163868113438e-05, 6.45635567470777e-05, 6.133537890972381e-05, 5.826860996423761e-05, 5.5355179466025734e-05, 5.2587420492724424e-05, 4.99580494680882e-05, 4.7460146994683776e-05, 4.5087139644949606e-05, 4.283278266270213e-05, 4.0691143529567043e-05, 3.865658635308866e-05, 3.672375703543425e-05, 3.488756918366253e-05, 3.31431907244794e-05, 3.1486031188255425e-05, 2.9911729628842633e-05, 2.8416143147400516e-05, 2.699533599003048e-05, 2.5645569190528947e-05, 2.436329073100251e-05, 2.314512619445238e-05, 2.1987869884729747e-05, 2.088847639049328e-05, 1.9844052570968613e-05, 1.8851849942420176e-05, 1.790925744529917e-05, 1.7013794573034208e-05, 1.6163104844382502e-05, 1.5354949602163364e-05, 1.4587202122055207e-05, 1.3857842015952435e-05, 1.3164949915154814e-05, 1.2506702419397072e-05, 1.1881367298427218e-05, 1.1287298933505859e-05, 1.0722933986830572e-05, 1.0186787287489042e-05, 9.677447923114588e-06, 9.193575526958859e-06, 8.733896750610916e-06, 8.297201913080367e-06, 7.882341817426348e-06, 7.488224726555034e-06, 7.113813490227281e-06, 6.7581228157159165e-06, 6.42021667493012e-06, 6.09920584118361e-06, 5.794245549124435e-06, 5.504533271668213e-06, 5.229306608084799e-06, 4.967841277680562e-06, 4.719449213796532e-06, 4.483476753106706e-06, 4.25930291545137e-06, 4.046337769678802e-06, 3.84402088119486e-06, 3.6518198371351168e-06, 3.469228845278362e-06, 3.295767403014446e-06, 3.130979032863722e-06, 2.9744300812205348e-06, 2.8257085771595086e-06, 2.684423148301533e-06, 2.5502019908864547e-06, 2.422691891342133e-06, 2.3015572967750256e-06, 2.1864794319362753e-06, 2.0771554603394607e-06, 1.9732976873224887e-06, 1.8746328029563646e-06, 1.7809011628085458e-06, 1.6918561046681176e-06, 1.6072632994347126e-06, 1.5269001344629761e-06, 1.4505551277398268e-06, 1.3780273713528359e-06, 1.3091260027851938e-06, 1.2436697026459338e-06, 1.1814862175136375e-06, 1.122411906637955e-06, 1.0662913113060578e-06, 1.0129767457407546e-06, 9.623279084537165e-07, 9.142115130310314e-07, 8.685009373794792e-07, 8.250758905105052e-07, 7.8382209598498e-07, 7.446309911857313e-07, 7.07399441626445e-07, 6.720294695451224e-07, 6.38427996067866e-07, 6.065065962644725e-07, 5.761812664512492e-07, 5.473722031286869e-07, 5.200035929722524e-07, 4.940034133236399e-07, 4.693032426574579e-07, 4.45838080524585e-07, 4.2354617649835554e-07, 4.023688676734378e-07, 3.822504242897659e-07, 3.631379030752775e-07, 3.4498100792151373e-07, 3.277319575254379e-07, 3.1134535964916594e-07, 2.9577809166670766e-07, 2.809891870833723e-07, 2.669397277292036e-07, 2.5359274134274333e-07, 2.4091310427560625e-07, 2.2886744906182583e-07, 2.1742407660873475e-07, 2.0655287277829794e-07, 1.9622522913938299e-07, 1.864139676824139e-07, 1.7709326929829311e-07, 1.6823860583337846e-07, 1.5982667554170947e-07, 1.5183534176462408e-07, 1.4424357467639286e-07, 1.370313959425732e-07, 1.3017982614544445e-07, 1.2367083483817222e-07, 1.174872930962636e-07, 1.1161292844145045e-07, 1.0603228201937791e-07, 1.0073066791840907e-07, 9.56941345224886e-08, 9.090942779636415e-08, 8.636395640654599e-08, 8.204575858621867e-08, 7.79434706569077e-08, 7.404629712406229e-08, 7.034398226785919e-08, 6.682678315446624e-08, 6.34854439967429e-08, 6.031117179690578e-08, 5.729561320706049e-08, 5.4430832546707455e-08, 5.170929091937209e-08, 4.912382637340346e-08, 4.6667635054733295e-08, 4.433425330199661e-08, 4.21175406368968e-08, 4.0011663605051956e-08, 3.8011080424799346e-08, 3.6110526403559364e-08, 3.43050000833814e-08, 3.258975007921234e-08, 3.096026257525171e-08, 2.9412249446489114e-08, 2.7941636974164674e-08, 2.6544555125456436e-08, 2.5217327369183616e-08]\n",
      "Epoch 16, Batch 0, Train Loss: 0.1532665193080902, Memory (GB): 7\n",
      "Epoch 16, Batch 1, Train Loss: 0.17265093326568604, Memory (GB): 7\n",
      "Epoch 16, Batch 2, Train Loss: 0.16673383116722107, Memory (GB): 7\n",
      "Epoch 16, Batch 3, Train Loss: 0.1525883674621582, Memory (GB): 7\n",
      "Epoch 16, Batch 4, Train Loss: 0.14890649914741516, Memory (GB): 7\n",
      "Epoch 16, Batch 5, Train Loss: 0.14966122806072235, Memory (GB): 7\n",
      "Epoch 16, Batch 6, Train Loss: 0.1657380312681198, Memory (GB): 7\n",
      "Epoch 16, Batch 7, Train Loss: 0.1543041467666626, Memory (GB): 7\n",
      "Epoch 16, Batch 8, Train Loss: 0.1473381668329239, Memory (GB): 7\n",
      "Epoch 16, Batch 9, Train Loss: 0.19207698106765747, Memory (GB): 7\n",
      "Epoch 16, Batch 10, Train Loss: 0.1614222228527069, Memory (GB): 7\n",
      "Epoch 16, Batch 11, Train Loss: 0.1434394121170044, Memory (GB): 7\n",
      "Epoch 16, Batch 12, Train Loss: 0.16867348551750183, Memory (GB): 7\n",
      "Epoch 16, Batch 13, Train Loss: 0.1587921530008316, Memory (GB): 7\n",
      "Epoch 16, Batch 14, Train Loss: 0.1699991524219513, Memory (GB): 7\n",
      "Epoch 16, Batch 15, Train Loss: 0.17873375117778778, Memory (GB): 7\n",
      "Epoch 16, Batch 16, Train Loss: 0.1679498553276062, Memory (GB): 7\n",
      "Epoch 16, Batch 17, Train Loss: 0.19760510325431824, Memory (GB): 7\n",
      "Epoch 16, Batch 18, Train Loss: 0.15823636949062347, Memory (GB): 7\n",
      "Epoch 16, Batch 19, Train Loss: 0.18255120515823364, Memory (GB): 7\n",
      "Epoch 16, Batch 20, Train Loss: 0.16971006989479065, Memory (GB): 7\n",
      "Epoch 16, Batch 21, Train Loss: 0.19994086027145386, Memory (GB): 7\n",
      "Epoch 16, Batch 22, Train Loss: 0.1883687525987625, Memory (GB): 7\n",
      "Epoch 16, Batch 23, Train Loss: 0.1743832379579544, Memory (GB): 7\n",
      "Epoch 16, Batch 24, Train Loss: 0.1925441175699234, Memory (GB): 7\n",
      "Epoch 16, Batch 25, Train Loss: 0.18950487673282623, Memory (GB): 7\n",
      "Epoch 16, Batch 26, Train Loss: 0.20649133622646332, Memory (GB): 7\n",
      "Epoch 16, Batch 27, Train Loss: 0.20514756441116333, Memory (GB): 7\n",
      "Epoch 16, Batch 28, Train Loss: 0.17160750925540924, Memory (GB): 7\n",
      "Epoch 16, Batch 29, Train Loss: 0.1722124218940735, Memory (GB): 7\n",
      "Epoch 16, Batch 30, Train Loss: 0.19010257720947266, Memory (GB): 7\n",
      "Epoch 16, Batch 31, Train Loss: 0.19394175708293915, Memory (GB): 7\n",
      "Epoch 16, Batch 32, Train Loss: 0.17763468623161316, Memory (GB): 7\n",
      "Epoch 16, Batch 33, Train Loss: 0.19690439105033875, Memory (GB): 7\n",
      "Epoch 16, Batch 34, Train Loss: 0.2101450264453888, Memory (GB): 7\n",
      "Epoch 16, Batch 35, Train Loss: 0.1601778268814087, Memory (GB): 7\n",
      "Epoch 16, Batch 36, Train Loss: 0.19627922773361206, Memory (GB): 7\n",
      "Epoch 16, Batch 37, Train Loss: 0.18208277225494385, Memory (GB): 7\n",
      "Epoch 16, Batch 38, Train Loss: 0.16335228085517883, Memory (GB): 7\n",
      "Epoch 16, Batch 39, Train Loss: 0.1734481155872345, Memory (GB): 7\n",
      "Epoch 16, Batch 40, Train Loss: 0.20022527873516083, Memory (GB): 7\n",
      "Epoch 16, Batch 41, Train Loss: 0.1616121083498001, Memory (GB): 7\n",
      "Epoch 16, Batch 42, Train Loss: 0.16179974377155304, Memory (GB): 7\n",
      "Epoch 16, Batch 43, Train Loss: 0.16755978763103485, Memory (GB): 7\n",
      "Epoch 16, Batch 44, Train Loss: 0.17749078571796417, Memory (GB): 7\n",
      "Epoch 16, Batch 45, Train Loss: 0.16899077594280243, Memory (GB): 7\n",
      "Epoch 16, Batch 46, Train Loss: 0.16499879956245422, Memory (GB): 7\n",
      "Epoch 16, Batch 47, Train Loss: 0.1734776496887207, Memory (GB): 7\n",
      "Epoch 16, Batch 48, Train Loss: 0.18137767910957336, Memory (GB): 7\n",
      "Epoch 16, Batch 49, Train Loss: 0.1948113739490509, Memory (GB): 7\n",
      "Epoch 16, Batch 50, Train Loss: 0.17703644931316376, Memory (GB): 7\n",
      "Epoch 16, Batch 51, Train Loss: 0.19335556030273438, Memory (GB): 7\n",
      "Epoch 16, Batch 52, Train Loss: 0.17622394859790802, Memory (GB): 7\n",
      "Epoch 16, Batch 53, Train Loss: 0.16905595362186432, Memory (GB): 7\n",
      "Epoch 16, Batch 54, Train Loss: 0.1587640941143036, Memory (GB): 7\n",
      "Epoch 16, Batch 55, Train Loss: 0.17284342646598816, Memory (GB): 7\n",
      "Epoch 16, Batch 56, Train Loss: 0.18867991864681244, Memory (GB): 7\n",
      "Epoch 16, Batch 57, Train Loss: 0.16819746792316437, Memory (GB): 7\n",
      "Epoch 16, Batch 58, Train Loss: 0.18491461873054504, Memory (GB): 7\n",
      "Epoch 16, Batch 59, Train Loss: 0.1738615334033966, Memory (GB): 7\n",
      "Epoch 16, Batch 60, Train Loss: 0.15751373767852783, Memory (GB): 7\n",
      "Epoch 16, Batch 61, Train Loss: 0.1729147732257843, Memory (GB): 7\n",
      "Epoch 16, Batch 62, Train Loss: 0.16577856242656708, Memory (GB): 7\n",
      "Epoch 16, Batch 63, Train Loss: 0.19167223572731018, Memory (GB): 7\n",
      "Epoch 16, Batch 64, Train Loss: 0.17252039909362793, Memory (GB): 7\n",
      "Epoch 16, Batch 65, Train Loss: 0.16324666142463684, Memory (GB): 7\n",
      "Epoch 16, Batch 66, Train Loss: 0.14862044155597687, Memory (GB): 7\n",
      "Epoch 16, Batch 67, Train Loss: 0.15025269985198975, Memory (GB): 7\n",
      "Epoch 16, Batch 68, Train Loss: 0.1795392781496048, Memory (GB): 7\n",
      "Epoch 16, Batch 69, Train Loss: 0.1682422012090683, Memory (GB): 7\n",
      "Epoch 16, Batch 70, Train Loss: 0.14826761186122894, Memory (GB): 7\n",
      "Epoch 16, Batch 71, Train Loss: 0.15035387873649597, Memory (GB): 7\n",
      "Epoch 16, Batch 72, Train Loss: 0.18625867366790771, Memory (GB): 7\n",
      "Epoch 16, Batch 73, Train Loss: 0.1567743867635727, Memory (GB): 7\n",
      "Epoch 16, Batch 74, Train Loss: 0.15545667707920074, Memory (GB): 7\n",
      "Epoch 16, Batch 75, Train Loss: 0.16436846554279327, Memory (GB): 7\n",
      "Epoch 16, Batch 76, Train Loss: 0.16605247557163239, Memory (GB): 7\n",
      "Epoch 16, Batch 77, Train Loss: 0.1508190780878067, Memory (GB): 7\n",
      "Epoch 16, Batch 78, Train Loss: 0.18530257046222687, Memory (GB): 7\n",
      "Epoch 16, Batch 79, Train Loss: 0.15255987644195557, Memory (GB): 7\n",
      "Epoch 16, Batch 80, Train Loss: 0.1666470468044281, Memory (GB): 7\n",
      "Epoch 16, Batch 81, Train Loss: 0.17293231189250946, Memory (GB): 7\n",
      "Epoch 16, Batch 82, Train Loss: 0.17197653651237488, Memory (GB): 7\n",
      "Epoch 16, Batch 83, Train Loss: 0.15674643218517303, Memory (GB): 7\n",
      "Epoch 16, Batch 84, Train Loss: 0.16043508052825928, Memory (GB): 7\n",
      "Epoch 16, Batch 85, Train Loss: 0.168656125664711, Memory (GB): 7\n",
      "Epoch 16, Batch 86, Train Loss: 0.18434613943099976, Memory (GB): 7\n",
      "Epoch 16, Batch 87, Train Loss: 0.16481147706508636, Memory (GB): 7\n",
      "Epoch 16, Batch 88, Train Loss: 0.15763621032238007, Memory (GB): 7\n",
      "Epoch 16, Batch 89, Train Loss: 0.1538754254579544, Memory (GB): 7\n",
      "Epoch 16, Batch 90, Train Loss: 0.17080430686473846, Memory (GB): 7\n",
      "Epoch 16, Batch 91, Train Loss: 0.16191044449806213, Memory (GB): 7\n",
      "Epoch 16, Batch 92, Train Loss: 0.13988542556762695, Memory (GB): 7\n",
      "Epoch 16, Batch 93, Train Loss: 0.15517377853393555, Memory (GB): 7\n",
      "Epoch 16, Batch 94, Train Loss: 0.17063438892364502, Memory (GB): 7\n",
      "Epoch 16, Batch 95, Train Loss: 0.1555735319852829, Memory (GB): 7\n",
      "Epoch 16, Batch 96, Train Loss: 0.1481853425502777, Memory (GB): 7\n",
      "Epoch 16, Batch 97, Train Loss: 0.15198038518428802, Memory (GB): 7\n",
      "Epoch 16, Batch 98, Train Loss: 0.17358636856079102, Memory (GB): 7\n",
      "Epoch 16, Batch 99, Train Loss: 0.15274640917778015, Memory (GB): 7\n",
      "Epoch 16, Batch 100, Train Loss: 0.15980879962444305, Memory (GB): 7\n",
      "Epoch 16, Batch 101, Train Loss: 0.15703605115413666, Memory (GB): 7\n",
      "Epoch 16, Batch 102, Train Loss: 0.15422502160072327, Memory (GB): 7\n",
      "Epoch 16, Batch 103, Train Loss: 0.16512639820575714, Memory (GB): 7\n",
      "Epoch 16, Batch 104, Train Loss: 0.16180987656116486, Memory (GB): 7\n",
      "Epoch 16, Batch 105, Train Loss: 0.17868056893348694, Memory (GB): 7\n",
      "Epoch 16, Batch 106, Train Loss: 0.16461807489395142, Memory (GB): 7\n",
      "Epoch 16, Batch 107, Train Loss: 0.16016332805156708, Memory (GB): 7\n",
      "Epoch 16, Batch 108, Train Loss: 0.15225237607955933, Memory (GB): 7\n",
      "Epoch 16, Batch 109, Train Loss: 0.15103483200073242, Memory (GB): 7\n",
      "Epoch 16, Batch 110, Train Loss: 0.19502422213554382, Memory (GB): 7\n",
      "Epoch 16, Batch 111, Train Loss: 0.17025752365589142, Memory (GB): 7\n",
      "Epoch 16, Batch 112, Train Loss: 0.17545464634895325, Memory (GB): 7\n",
      "Epoch 16, Batch 113, Train Loss: 0.15593793988227844, Memory (GB): 7\n",
      "Epoch 16, Batch 114, Train Loss: 0.18449142575263977, Memory (GB): 7\n",
      "Epoch 16, Batch 115, Train Loss: 0.17031392455101013, Memory (GB): 7\n",
      "Epoch 16, Batch 116, Train Loss: 0.18226438760757446, Memory (GB): 7\n",
      "Epoch 16, Batch 117, Train Loss: 0.1974610537290573, Memory (GB): 7\n",
      "Epoch 16, Batch 118, Train Loss: 0.16221974790096283, Memory (GB): 7\n",
      "Epoch 16, Batch 119, Train Loss: 0.1573059856891632, Memory (GB): 7\n",
      "Epoch 16, Batch 120, Train Loss: 0.19116587936878204, Memory (GB): 7\n",
      "Epoch 16, Batch 121, Train Loss: 0.18679073452949524, Memory (GB): 7\n",
      "Epoch 16, Batch 122, Train Loss: 0.17447176575660706, Memory (GB): 7\n",
      "Epoch 16, Batch 123, Train Loss: 0.16706058382987976, Memory (GB): 7\n",
      "Epoch 16, Batch 124, Train Loss: 0.17440856993198395, Memory (GB): 7\n",
      "Epoch 16, Batch 125, Train Loss: 0.1705150157213211, Memory (GB): 7\n",
      "Epoch 16, Batch 126, Train Loss: 0.16484804451465607, Memory (GB): 7\n",
      "Epoch 16, Batch 127, Train Loss: 0.18824946880340576, Memory (GB): 7\n",
      "Epoch 16, Batch 128, Train Loss: 0.17713718116283417, Memory (GB): 7\n",
      "Epoch 16, Batch 129, Train Loss: 0.18129709362983704, Memory (GB): 7\n",
      "Epoch 16, Batch 130, Train Loss: 0.1803528219461441, Memory (GB): 7\n",
      "Epoch 16, Batch 131, Train Loss: 0.1790464222431183, Memory (GB): 7\n",
      "Epoch 16, Batch 132, Train Loss: 0.17579495906829834, Memory (GB): 7\n",
      "Epoch 16, Batch 133, Train Loss: 0.2011481672525406, Memory (GB): 7\n",
      "Epoch 16, Batch 134, Train Loss: 0.1983400583267212, Memory (GB): 7\n",
      "Epoch 16, Batch 135, Train Loss: 0.18484708666801453, Memory (GB): 7\n",
      "Epoch 16, Batch 136, Train Loss: 0.16469526290893555, Memory (GB): 7\n",
      "Epoch 16, Batch 137, Train Loss: 0.17235669493675232, Memory (GB): 7\n",
      "Epoch 16, Batch 138, Train Loss: 0.16217820346355438, Memory (GB): 7\n",
      "Epoch 16, Batch 139, Train Loss: 0.17633746564388275, Memory (GB): 7\n",
      "Epoch 16, Batch 140, Train Loss: 0.18168146908283234, Memory (GB): 7\n",
      "Epoch 16, Batch 141, Train Loss: 0.1764879673719406, Memory (GB): 7\n",
      "Epoch 16, Batch 142, Train Loss: 0.1748390644788742, Memory (GB): 7\n",
      "Epoch 16, Batch 143, Train Loss: 0.18317434191703796, Memory (GB): 7\n",
      "Epoch 16, Batch 144, Train Loss: 0.1769447773694992, Memory (GB): 7\n",
      "Epoch 16, Batch 145, Train Loss: 0.19441981613636017, Memory (GB): 7\n",
      "Epoch 16, Batch 146, Train Loss: 0.16580082476139069, Memory (GB): 7\n",
      "Epoch 16, Batch 147, Train Loss: 0.16116443276405334, Memory (GB): 7\n",
      "Epoch 16, Batch 148, Train Loss: 0.16713210940361023, Memory (GB): 7\n",
      "Epoch 16, Batch 149, Train Loss: 0.15957169234752655, Memory (GB): 7\n",
      "Epoch 16, Batch 150, Train Loss: 0.1795143336057663, Memory (GB): 7\n",
      "Epoch 16, Batch 151, Train Loss: 0.1930859386920929, Memory (GB): 7\n",
      "Epoch 16, Batch 152, Train Loss: 0.16880753636360168, Memory (GB): 7\n",
      "Epoch 16, Batch 153, Train Loss: 0.17884771525859833, Memory (GB): 7\n",
      "Epoch 16, Batch 154, Train Loss: 0.17438237369060516, Memory (GB): 7\n",
      "Epoch 16, Batch 155, Train Loss: 0.18637053668498993, Memory (GB): 7\n",
      "Epoch 16, Batch 156, Train Loss: 0.18324072659015656, Memory (GB): 7\n",
      "Epoch 16, Batch 157, Train Loss: 0.16914619505405426, Memory (GB): 7\n",
      "Epoch 16, Batch 158, Train Loss: 0.19429242610931396, Memory (GB): 7\n",
      "Epoch 16, Batch 159, Train Loss: 0.15980832278728485, Memory (GB): 7\n",
      "Epoch 16, Batch 160, Train Loss: 0.16876360774040222, Memory (GB): 7\n",
      "Epoch 16, Batch 161, Train Loss: 0.17755945026874542, Memory (GB): 7\n",
      "Epoch 16, Batch 162, Train Loss: 0.1766146421432495, Memory (GB): 7\n",
      "Epoch 16, Batch 163, Train Loss: 0.1754779815673828, Memory (GB): 7\n",
      "Epoch 16, Batch 164, Train Loss: 0.1760818213224411, Memory (GB): 7\n",
      "Epoch 16, Batch 165, Train Loss: 0.18669328093528748, Memory (GB): 7\n",
      "Epoch 16, Batch 166, Train Loss: 0.1691282093524933, Memory (GB): 7\n",
      "Epoch 16, Batch 167, Train Loss: 0.1685521900653839, Memory (GB): 7\n",
      "Epoch 16, Batch 168, Train Loss: 0.17264731228351593, Memory (GB): 7\n",
      "Epoch 16, Batch 169, Train Loss: 0.16000309586524963, Memory (GB): 7\n",
      "Epoch 16, Batch 170, Train Loss: 0.16134916245937347, Memory (GB): 7\n",
      "Epoch 16, Batch 171, Train Loss: 0.18471461534500122, Memory (GB): 7\n",
      "Epoch 16, Batch 172, Train Loss: 0.19163401424884796, Memory (GB): 7\n",
      "Epoch 16, Batch 173, Train Loss: 0.1830686628818512, Memory (GB): 7\n",
      "Epoch 16, Batch 174, Train Loss: 0.16352713108062744, Memory (GB): 7\n",
      "Epoch 16, Batch 175, Train Loss: 0.15674278140068054, Memory (GB): 7\n",
      "Epoch 16, Batch 176, Train Loss: 0.17580464482307434, Memory (GB): 7\n",
      "Epoch 16, Batch 177, Train Loss: 0.1498996913433075, Memory (GB): 7\n",
      "Epoch 16, Batch 178, Train Loss: 0.16786497831344604, Memory (GB): 7\n",
      "Epoch 16, Batch 179, Train Loss: 0.15989503264427185, Memory (GB): 7\n",
      "[9.549150281254086e-05, 9.07169276719138e-05, 8.618108128831811e-05, 8.187202722390219e-05, 7.777842586270708e-05, 7.388950456957172e-05, 7.019502934109311e-05, 6.668527787403845e-05, 6.335101398033653e-05, 6.01834632813197e-05, 5.7174290117253705e-05, 5.431557561139103e-05, 5.159979683082147e-05, 4.9019806989280405e-05, 4.656881663981638e-05, 4.424037580782556e-05, 4.202835701743427e-05, 3.992693916656257e-05, 3.793059220823443e-05, 3.603406259782271e-05, 3.423235946793157e-05, 3.252074149453499e-05, 3.0894704419808236e-05, 2.9349969198817817e-05, 2.788247073887692e-05, 2.6488347201933082e-05, 2.5163929841836418e-05, 2.3905733349744597e-05, 2.271044668225737e-05, 2.1574924348144495e-05, 2.0496178130737274e-05, 1.947136922420041e-05, 1.8497800762990387e-05, 1.757291072484087e-05, 1.6694265188598823e-05, 1.5859551929168877e-05, 1.5066574332710438e-05, 1.4313245616074915e-05, 1.3597583335271169e-05, 1.2917704168507607e-05, 1.2271818960082226e-05, 1.1658228012078115e-05, 1.1075316611474207e-05, 1.0521550780900497e-05, 9.995473241855471e-06, 9.495699579762697e-06, 9.020914600774562e-06, 8.569868870735833e-06, 8.14137542719904e-06, 7.73430665583909e-06, 7.347591323047132e-06, 6.980211756894778e-06, 6.63120116905004e-06, 6.299641110597535e-06, 5.98465905506766e-06, 5.685426102314275e-06, 5.401154797198562e-06, 5.131097057338633e-06, 4.874542204471702e-06, 4.630815094248116e-06, 4.39927433953571e-06, 4.179310622558924e-06, 3.970345091430978e-06, 3.7718278368594282e-06, 3.583236445016457e-06, 3.4040746227656337e-06, 3.233870891627351e-06, 3.0721773470459847e-06, 2.9185684796936844e-06, 2.7726400557090005e-06, 2.63400805292355e-06, 2.5023076502773725e-06, 2.3771922677635035e-06, 2.2583326543753285e-06, 2.1454160216565617e-06, 2.0381452205737333e-06, 1.936237959545046e-06, 1.8394260615677942e-06, 1.7474547584894048e-06, 1.660082020564935e-06, 1.5770779195366879e-06, 1.4982240235598531e-06, 1.4233128223818606e-06, 1.3521471812627675e-06, 1.2845398221996291e-06, 1.2203128310896477e-06, 1.1592971895351653e-06, 1.1013323300584068e-06, 1.0462657135554868e-06, 9.939524278777123e-07, 9.442548064838267e-07, 8.97042066159635e-07, 8.521899628516535e-07, 8.095804647090708e-07, 7.691014414736172e-07, 7.306463693999365e-07, 6.941140509299394e-07, 6.594083483834426e-07, 6.264379309642704e-07, 5.951160344160568e-07, 5.653602326952538e-07, 5.370922210604911e-07, 5.102376100074665e-07, 4.847257295070933e-07, 4.604894430317387e-07, 4.3746497088015165e-07, 4.155917223361441e-07, 3.9481213621933674e-07, 3.7507152940836996e-07, 3.563179529379515e-07, 3.385020552910538e-07, 3.215769525265012e-07, 3.054981049001762e-07, 2.902231996551673e-07, 2.757120396724089e-07, 2.619264376887884e-07, 2.48830115804349e-07, 2.3638861001413156e-07, 2.2456917951342494e-07, 2.1334072053775368e-07, 2.0267368451086603e-07, 1.9254000028532263e-07, 1.8291300027105657e-07, 1.7376735025750368e-07, 1.6507898274462847e-07, 1.568250336073971e-07, 1.4898378192702725e-07, 1.4153459283067582e-07, 1.344578631891421e-07, 1.2773497002968493e-07, 1.2134822152820068e-07, 1.1528081045179064e-07, 1.0951676992920115e-07, 1.0404093143274103e-07, 9.883888486110401e-08, 9.389694061804883e-08, 8.920209358714635e-08, 8.474198890778903e-08, 8.050488946239957e-08, 7.64796449892796e-08, 7.265566273981561e-08, 6.902287960282485e-08, 6.55717356226836e-08, 6.229314884154942e-08, 5.917849139947192e-08, 5.621956682949834e-08, 5.340858848802341e-08, 5.073815906362224e-08, 4.820125111044111e-08, 4.5791188554919054e-08, 4.35016291271731e-08, 4.132654767081445e-08, 3.926022028727373e-08, 3.729720927291004e-08, 3.543234880926453e-08, 3.3660731368801305e-08, 3.1977694800361245e-08, 3.037881006034318e-08, 2.8859869557326013e-08, 2.7416876079459714e-08, 2.6046032275486723e-08, 2.4743730661712385e-08, 2.3506544128626764e-08, 2.2331216922195424e-08, 2.1214656076085652e-08, 2.0153923272281363e-08, 1.9146227108667294e-08, 1.8188915753233932e-08, 1.727946996557223e-08, 1.6415496467293622e-08, 1.559472164392894e-08, 1.4814985561732493e-08, 1.4074236283645868e-08, 1.3370524469463576e-08, 1.2701998245990395e-08, 1.2066898333690873e-08, 1.1463553417006328e-08, 1.0890375746156014e-08, 1.0345856958848214e-08, 9.828564110905804e-09, 9.337135905360512e-09, 8.870279110092484e-09, 8.42676515458786e-09, 8.005426896858469e-09, 7.605155552015546e-09, 7.224897774414766e-09, 6.8636528856940284e-09, 6.5204702414093265e-09, 6.194446729338859e-09, 5.884724392871916e-09, 5.5904881732283185e-09, 5.3109637645669024e-09, 5.045415576338559e-09, 4.79314479752163e-09, 4.553487557645548e-09, 4.325813179763268e-09, 4.109522520775106e-09, 3.904046394736352e-09, 3.7088440749995317e-09]\n",
      "Epoch 17, Batch 0, Train Loss: 0.16387870907783508, Memory (GB): 7\n",
      "Epoch 17, Batch 1, Train Loss: 0.1740761697292328, Memory (GB): 7\n",
      "Epoch 17, Batch 2, Train Loss: 0.1591666042804718, Memory (GB): 7\n",
      "Epoch 17, Batch 3, Train Loss: 0.15416546165943146, Memory (GB): 7\n",
      "Epoch 17, Batch 4, Train Loss: 0.17271804809570312, Memory (GB): 7\n",
      "Epoch 17, Batch 5, Train Loss: 0.15146169066429138, Memory (GB): 7\n",
      "Epoch 17, Batch 6, Train Loss: 0.16163647174835205, Memory (GB): 7\n",
      "Epoch 17, Batch 7, Train Loss: 0.12786835432052612, Memory (GB): 7\n",
      "Epoch 17, Batch 8, Train Loss: 0.14564122259616852, Memory (GB): 7\n",
      "Epoch 17, Batch 9, Train Loss: 0.14232699573040009, Memory (GB): 7\n",
      "Epoch 17, Batch 10, Train Loss: 0.13749153912067413, Memory (GB): 7\n",
      "Epoch 17, Batch 11, Train Loss: 0.16388177871704102, Memory (GB): 7\n",
      "Epoch 17, Batch 12, Train Loss: 0.13448692858219147, Memory (GB): 7\n",
      "Epoch 17, Batch 13, Train Loss: 0.135588601231575, Memory (GB): 7\n",
      "Epoch 17, Batch 14, Train Loss: 0.14813102781772614, Memory (GB): 7\n",
      "Epoch 17, Batch 15, Train Loss: 0.15441374480724335, Memory (GB): 7\n",
      "Epoch 17, Batch 16, Train Loss: 0.12887316942214966, Memory (GB): 7\n",
      "Epoch 17, Batch 17, Train Loss: 0.13406306505203247, Memory (GB): 7\n",
      "Epoch 17, Batch 18, Train Loss: 0.15909597277641296, Memory (GB): 7\n",
      "Epoch 17, Batch 19, Train Loss: 0.14526021480560303, Memory (GB): 7\n",
      "Epoch 17, Batch 20, Train Loss: 0.1351475715637207, Memory (GB): 7\n",
      "Epoch 17, Batch 21, Train Loss: 0.1684325784444809, Memory (GB): 7\n",
      "Epoch 17, Batch 22, Train Loss: 0.16965077817440033, Memory (GB): 7\n",
      "Epoch 17, Batch 23, Train Loss: 0.1500253528356552, Memory (GB): 7\n",
      "Epoch 17, Batch 24, Train Loss: 0.13707825541496277, Memory (GB): 7\n",
      "Epoch 17, Batch 25, Train Loss: 0.16440995037555695, Memory (GB): 7\n",
      "Epoch 17, Batch 26, Train Loss: 0.16314098238945007, Memory (GB): 7\n",
      "Epoch 17, Batch 27, Train Loss: 0.15405994653701782, Memory (GB): 7\n",
      "Epoch 17, Batch 28, Train Loss: 0.18738199770450592, Memory (GB): 7\n",
      "Epoch 17, Batch 29, Train Loss: 0.14457935094833374, Memory (GB): 7\n",
      "Epoch 17, Batch 30, Train Loss: 0.15832550823688507, Memory (GB): 7\n",
      "Epoch 17, Batch 31, Train Loss: 0.16414223611354828, Memory (GB): 7\n",
      "Epoch 17, Batch 32, Train Loss: 0.1804039180278778, Memory (GB): 7\n",
      "Epoch 17, Batch 33, Train Loss: 0.15019963681697845, Memory (GB): 7\n",
      "Epoch 17, Batch 34, Train Loss: 0.16658741235733032, Memory (GB): 7\n",
      "Epoch 17, Batch 35, Train Loss: 0.14662973582744598, Memory (GB): 7\n",
      "Epoch 17, Batch 36, Train Loss: 0.1562475860118866, Memory (GB): 7\n",
      "Epoch 17, Batch 37, Train Loss: 0.1527927964925766, Memory (GB): 7\n",
      "Epoch 17, Batch 38, Train Loss: 0.17840376496315002, Memory (GB): 7\n",
      "Epoch 17, Batch 39, Train Loss: 0.1615668535232544, Memory (GB): 7\n",
      "Epoch 17, Batch 40, Train Loss: 0.14142738282680511, Memory (GB): 7\n",
      "Epoch 17, Batch 41, Train Loss: 0.1547383964061737, Memory (GB): 7\n",
      "Epoch 17, Batch 42, Train Loss: 0.16180728375911713, Memory (GB): 7\n",
      "Epoch 17, Batch 43, Train Loss: 0.14229977130889893, Memory (GB): 7\n",
      "Epoch 17, Batch 44, Train Loss: 0.19307824969291687, Memory (GB): 7\n",
      "Epoch 17, Batch 45, Train Loss: 0.19076624512672424, Memory (GB): 7\n",
      "Epoch 17, Batch 46, Train Loss: 0.14804428815841675, Memory (GB): 7\n",
      "Epoch 17, Batch 47, Train Loss: 0.15197989344596863, Memory (GB): 7\n",
      "Epoch 17, Batch 48, Train Loss: 0.1636064648628235, Memory (GB): 7\n",
      "Epoch 17, Batch 49, Train Loss: 0.17603297531604767, Memory (GB): 7\n",
      "Epoch 17, Batch 50, Train Loss: 0.1508694887161255, Memory (GB): 7\n",
      "Epoch 17, Batch 51, Train Loss: 0.19063469767570496, Memory (GB): 7\n",
      "Epoch 17, Batch 52, Train Loss: 0.17115627229213715, Memory (GB): 7\n",
      "Epoch 17, Batch 53, Train Loss: 0.16128282248973846, Memory (GB): 7\n",
      "Epoch 17, Batch 54, Train Loss: 0.1708999127149582, Memory (GB): 7\n",
      "Epoch 17, Batch 55, Train Loss: 0.17118220031261444, Memory (GB): 7\n",
      "Epoch 17, Batch 56, Train Loss: 0.16099309921264648, Memory (GB): 7\n",
      "Epoch 17, Batch 57, Train Loss: 0.20295460522174835, Memory (GB): 7\n",
      "Epoch 17, Batch 58, Train Loss: 0.19003170728683472, Memory (GB): 7\n",
      "Epoch 17, Batch 59, Train Loss: 0.167423278093338, Memory (GB): 7\n",
      "Epoch 17, Batch 60, Train Loss: 0.15891912579536438, Memory (GB): 7\n",
      "Epoch 17, Batch 61, Train Loss: 0.15649166703224182, Memory (GB): 7\n",
      "Epoch 17, Batch 62, Train Loss: 0.16389161348342896, Memory (GB): 7\n",
      "Epoch 17, Batch 63, Train Loss: 0.15295535326004028, Memory (GB): 7\n",
      "Epoch 17, Batch 64, Train Loss: 0.1716790348291397, Memory (GB): 7\n",
      "Epoch 17, Batch 65, Train Loss: 0.13779141008853912, Memory (GB): 7\n",
      "Epoch 17, Batch 66, Train Loss: 0.13655845820903778, Memory (GB): 7\n",
      "Epoch 17, Batch 67, Train Loss: 0.155230313539505, Memory (GB): 7\n",
      "Epoch 17, Batch 68, Train Loss: 0.15794268250465393, Memory (GB): 7\n",
      "Epoch 17, Batch 69, Train Loss: 0.16968132555484772, Memory (GB): 7\n",
      "Epoch 17, Batch 70, Train Loss: 0.14257413148880005, Memory (GB): 7\n",
      "Epoch 17, Batch 71, Train Loss: 0.14908137917518616, Memory (GB): 7\n",
      "Epoch 17, Batch 72, Train Loss: 0.1501859426498413, Memory (GB): 7\n",
      "Epoch 17, Batch 73, Train Loss: 0.16296032071113586, Memory (GB): 7\n",
      "Epoch 17, Batch 74, Train Loss: 0.18393442034721375, Memory (GB): 7\n",
      "Epoch 17, Batch 75, Train Loss: 0.1671251654624939, Memory (GB): 7\n",
      "Epoch 17, Batch 76, Train Loss: 0.14627300202846527, Memory (GB): 7\n",
      "Epoch 17, Batch 77, Train Loss: 0.15112899243831635, Memory (GB): 7\n",
      "Epoch 17, Batch 78, Train Loss: 0.16096998751163483, Memory (GB): 7\n",
      "Epoch 17, Batch 79, Train Loss: 0.13431410491466522, Memory (GB): 7\n",
      "Epoch 17, Batch 80, Train Loss: 0.15372498333454132, Memory (GB): 7\n",
      "Epoch 17, Batch 81, Train Loss: 0.1366979479789734, Memory (GB): 7\n",
      "Epoch 17, Batch 82, Train Loss: 0.143163800239563, Memory (GB): 7\n",
      "Epoch 17, Batch 83, Train Loss: 0.16556552052497864, Memory (GB): 7\n",
      "Epoch 17, Batch 84, Train Loss: 0.15762028098106384, Memory (GB): 7\n",
      "Epoch 17, Batch 85, Train Loss: 0.15775485336780548, Memory (GB): 7\n",
      "Epoch 17, Batch 86, Train Loss: 0.1408866047859192, Memory (GB): 7\n",
      "Epoch 17, Batch 87, Train Loss: 0.15315857529640198, Memory (GB): 7\n",
      "Epoch 17, Batch 88, Train Loss: 0.14562633633613586, Memory (GB): 7\n",
      "Epoch 17, Batch 89, Train Loss: 0.14716674387454987, Memory (GB): 7\n",
      "Epoch 17, Batch 90, Train Loss: 0.14894996583461761, Memory (GB): 7\n",
      "Epoch 17, Batch 91, Train Loss: 0.1521410048007965, Memory (GB): 7\n",
      "Epoch 17, Batch 92, Train Loss: 0.1603264957666397, Memory (GB): 7\n",
      "Epoch 17, Batch 93, Train Loss: 0.15757033228874207, Memory (GB): 7\n",
      "Epoch 17, Batch 94, Train Loss: 0.1616336703300476, Memory (GB): 7\n",
      "Epoch 17, Batch 95, Train Loss: 0.14283183217048645, Memory (GB): 7\n",
      "Epoch 17, Batch 96, Train Loss: 0.1626340001821518, Memory (GB): 7\n",
      "Epoch 17, Batch 97, Train Loss: 0.16559016704559326, Memory (GB): 7\n",
      "Epoch 17, Batch 98, Train Loss: 0.16252930462360382, Memory (GB): 7\n",
      "Epoch 17, Batch 99, Train Loss: 0.13383476436138153, Memory (GB): 7\n",
      "Epoch 17, Batch 100, Train Loss: 0.14625206589698792, Memory (GB): 7\n",
      "Epoch 17, Batch 101, Train Loss: 0.15710671246051788, Memory (GB): 7\n",
      "Epoch 17, Batch 102, Train Loss: 0.13714025914669037, Memory (GB): 7\n",
      "Epoch 17, Batch 103, Train Loss: 0.13912968337535858, Memory (GB): 7\n",
      "Epoch 17, Batch 104, Train Loss: 0.13931459188461304, Memory (GB): 7\n",
      "Epoch 17, Batch 105, Train Loss: 0.1399628221988678, Memory (GB): 7\n",
      "Epoch 17, Batch 106, Train Loss: 0.13744838535785675, Memory (GB): 7\n",
      "Epoch 17, Batch 107, Train Loss: 0.14915911853313446, Memory (GB): 7\n",
      "Epoch 17, Batch 108, Train Loss: 0.15197311341762543, Memory (GB): 7\n",
      "Epoch 17, Batch 109, Train Loss: 0.14817653596401215, Memory (GB): 7\n",
      "Epoch 17, Batch 110, Train Loss: 0.14570724964141846, Memory (GB): 7\n",
      "Epoch 17, Batch 111, Train Loss: 0.14647458493709564, Memory (GB): 7\n",
      "Epoch 17, Batch 112, Train Loss: 0.14369723200798035, Memory (GB): 7\n",
      "Epoch 17, Batch 113, Train Loss: 0.1359255611896515, Memory (GB): 7\n",
      "Epoch 17, Batch 114, Train Loss: 0.14296439290046692, Memory (GB): 7\n",
      "Epoch 17, Batch 115, Train Loss: 0.18381457030773163, Memory (GB): 7\n",
      "Epoch 17, Batch 116, Train Loss: 0.18539273738861084, Memory (GB): 7\n",
      "Epoch 17, Batch 117, Train Loss: 0.15256068110466003, Memory (GB): 7\n",
      "Epoch 17, Batch 118, Train Loss: 0.15308329463005066, Memory (GB): 7\n",
      "Epoch 17, Batch 119, Train Loss: 0.15528327226638794, Memory (GB): 7\n",
      "Epoch 17, Batch 120, Train Loss: 0.16271507740020752, Memory (GB): 7\n",
      "Epoch 17, Batch 121, Train Loss: 0.15065567195415497, Memory (GB): 7\n",
      "Epoch 17, Batch 122, Train Loss: 0.1570475697517395, Memory (GB): 7\n",
      "Epoch 17, Batch 123, Train Loss: 0.1651308387517929, Memory (GB): 7\n",
      "Epoch 17, Batch 124, Train Loss: 0.15598109364509583, Memory (GB): 7\n",
      "Epoch 17, Batch 125, Train Loss: 0.14419770240783691, Memory (GB): 7\n",
      "Epoch 17, Batch 126, Train Loss: 0.16467620432376862, Memory (GB): 7\n",
      "Epoch 17, Batch 127, Train Loss: 0.15921978652477264, Memory (GB): 7\n",
      "Epoch 17, Batch 128, Train Loss: 0.1693810075521469, Memory (GB): 7\n",
      "Epoch 17, Batch 129, Train Loss: 0.14578621089458466, Memory (GB): 7\n",
      "Epoch 17, Batch 130, Train Loss: 0.16075293719768524, Memory (GB): 7\n",
      "Epoch 17, Batch 131, Train Loss: 0.15989048779010773, Memory (GB): 7\n",
      "Epoch 17, Batch 132, Train Loss: 0.14376293122768402, Memory (GB): 7\n",
      "Epoch 17, Batch 133, Train Loss: 0.16695764660835266, Memory (GB): 7\n",
      "Epoch 17, Batch 134, Train Loss: 0.17547477781772614, Memory (GB): 7\n",
      "Epoch 17, Batch 135, Train Loss: 0.16011159121990204, Memory (GB): 7\n",
      "Epoch 17, Batch 136, Train Loss: 0.1749679446220398, Memory (GB): 7\n",
      "Epoch 17, Batch 137, Train Loss: 0.1645844578742981, Memory (GB): 7\n",
      "Epoch 17, Batch 138, Train Loss: 0.16446667909622192, Memory (GB): 7\n",
      "Epoch 17, Batch 139, Train Loss: 0.18093359470367432, Memory (GB): 7\n",
      "Epoch 17, Batch 140, Train Loss: 0.16897240281105042, Memory (GB): 7\n",
      "Epoch 17, Batch 141, Train Loss: 0.16174452006816864, Memory (GB): 7\n",
      "Epoch 17, Batch 142, Train Loss: 0.17266535758972168, Memory (GB): 7\n",
      "Epoch 17, Batch 143, Train Loss: 0.16417306661605835, Memory (GB): 7\n",
      "Epoch 17, Batch 144, Train Loss: 0.18055696785449982, Memory (GB): 7\n",
      "Epoch 17, Batch 145, Train Loss: 0.16639988124370575, Memory (GB): 7\n",
      "Epoch 17, Batch 146, Train Loss: 0.16505271196365356, Memory (GB): 7\n",
      "Epoch 17, Batch 147, Train Loss: 0.18804803490638733, Memory (GB): 7\n",
      "Epoch 17, Batch 148, Train Loss: 0.152988001704216, Memory (GB): 7\n",
      "Epoch 17, Batch 149, Train Loss: 0.1572040319442749, Memory (GB): 7\n",
      "Epoch 17, Batch 150, Train Loss: 0.16899962723255157, Memory (GB): 7\n",
      "Epoch 17, Batch 151, Train Loss: 0.17292983829975128, Memory (GB): 7\n",
      "Epoch 17, Batch 152, Train Loss: 0.17814509570598602, Memory (GB): 7\n",
      "Epoch 17, Batch 153, Train Loss: 0.17924045026302338, Memory (GB): 7\n",
      "Epoch 17, Batch 154, Train Loss: 0.17062389850616455, Memory (GB): 7\n",
      "Epoch 17, Batch 155, Train Loss: 0.17951515316963196, Memory (GB): 7\n",
      "Epoch 17, Batch 156, Train Loss: 0.164829283952713, Memory (GB): 7\n",
      "Epoch 17, Batch 157, Train Loss: 0.17089059948921204, Memory (GB): 7\n",
      "Epoch 17, Batch 158, Train Loss: 0.1921502947807312, Memory (GB): 7\n",
      "Epoch 17, Batch 159, Train Loss: 0.20405685901641846, Memory (GB): 7\n",
      "Epoch 17, Batch 160, Train Loss: 0.16905368864536285, Memory (GB): 7\n",
      "Epoch 17, Batch 161, Train Loss: 0.17171379923820496, Memory (GB): 7\n",
      "Epoch 17, Batch 162, Train Loss: 0.16277018189430237, Memory (GB): 7\n",
      "Epoch 17, Batch 163, Train Loss: 0.1687619537115097, Memory (GB): 7\n",
      "Epoch 17, Batch 164, Train Loss: 0.16264382004737854, Memory (GB): 7\n",
      "Epoch 17, Batch 165, Train Loss: 0.17765486240386963, Memory (GB): 7\n",
      "Epoch 17, Batch 166, Train Loss: 0.16106614470481873, Memory (GB): 7\n",
      "Epoch 17, Batch 167, Train Loss: 0.1612900197505951, Memory (GB): 7\n",
      "Epoch 17, Batch 168, Train Loss: 0.1643415242433548, Memory (GB): 7\n",
      "Epoch 17, Batch 169, Train Loss: 0.16285383701324463, Memory (GB): 7\n",
      "Epoch 17, Batch 170, Train Loss: 0.16732415556907654, Memory (GB): 7\n",
      "Epoch 17, Batch 171, Train Loss: 0.1595388650894165, Memory (GB): 7\n",
      "Epoch 17, Batch 172, Train Loss: 0.1457904428243637, Memory (GB): 7\n",
      "Epoch 17, Batch 173, Train Loss: 0.14304901659488678, Memory (GB): 7\n",
      "Epoch 17, Batch 174, Train Loss: 0.15674473345279694, Memory (GB): 7\n",
      "Epoch 17, Batch 175, Train Loss: 0.17120401561260223, Memory (GB): 7\n",
      "Epoch 17, Batch 176, Train Loss: 0.15751348435878754, Memory (GB): 7\n",
      "Epoch 17, Batch 177, Train Loss: 0.1601860523223877, Memory (GB): 7\n",
      "Epoch 17, Batch 178, Train Loss: 0.16315071284770966, Memory (GB): 7\n",
      "Epoch 17, Batch 179, Train Loss: 0.1665036678314209, Memory (GB): 7\n",
      "[9.549150281252054e-05, 9.071692767189442e-05, 8.618108128829972e-05, 8.187202722388477e-05, 7.777842586269049e-05, 7.388950456955597e-05, 7.019502934107817e-05, 6.668527787402422e-05, 6.335101398032303e-05, 6.018346328130688e-05, 5.717429011724154e-05, 5.431557561137945e-05, 5.1599796830810466e-05, 4.901980698926994e-05, 4.656881663980643e-05, 4.424037580781609e-05, 4.202835701742532e-05, 3.992693916655401e-05, 3.793059220822633e-05, 3.603406259781504e-05, 3.423235946792427e-05, 3.2520741494528064e-05, 3.089470441980165e-05, 2.9349969198811545e-05, 2.788247073887098e-05, 2.648834720192744e-05, 2.516392984183104e-05, 2.39057333497395e-05, 2.2710446682252524e-05, 2.1574924348139904e-05, 2.0496178130732897e-05, 1.947136922419625e-05, 1.8497800762986443e-05, 1.757291072483712e-05, 1.669426518859526e-05, 1.5859551929165506e-05, 1.5066574332707216e-05, 1.4313245616071857e-05, 1.3597583335268272e-05, 1.2917704168504854e-05, 1.2271818960079612e-05, 1.1658228012075628e-05, 1.107531661147185e-05, 1.0521550780898247e-05, 9.99547324185334e-06, 9.495699579760676e-06, 9.020914600772631e-06, 8.569868870734e-06, 8.141375427197309e-06, 7.734306655837443e-06, 7.3475913230455676e-06, 6.980211756893289e-06, 6.631201169048629e-06, 6.2996411105961884e-06, 5.984659055066391e-06, 5.685426102313063e-06, 5.401154797197412e-06, 5.131097057337539e-06, 4.874542204470662e-06, 4.6308150942471275e-06, 4.399274339534771e-06, 4.179310622558033e-06, 3.970345091430131e-06, 3.7718278368586223e-06, 3.58323644501569e-06, 3.4040746227649082e-06, 3.233870891626663e-06, 3.072177347045328e-06, 2.9185684796930635e-06, 2.7726400557084093e-06, 2.6340080529229873e-06, 2.50230765027684e-06, 2.3771922677629974e-06, 2.2583326543748486e-06, 2.1454160216561043e-06, 2.038145220573299e-06, 1.9362379595446345e-06, 1.8394260615674012e-06, 1.747454758489033e-06, 1.660082020564581e-06, 1.5770779195363527e-06, 1.4982240235595353e-06, 1.4233128223815567e-06, 1.352147181262479e-06, 1.2845398221993551e-06, 1.220312831089388e-06, 1.1592971895349173e-06, 1.1013323300581726e-06, 1.0462657135552636e-06, 9.939524278775008e-07, 9.442548064836245e-07, 8.970420661594429e-07, 8.521899628514723e-07, 8.09580464708898e-07, 7.691014414734531e-07, 7.306463693997806e-07, 6.941140509297913e-07, 6.594083483833017e-07, 6.264379309641369e-07, 5.951160344159301e-07, 5.653602326951329e-07, 5.370922210603768e-07, 5.102376100073578e-07, 4.847257295069902e-07, 4.6048944303164045e-07, 4.374649708800585e-07, 4.155917223360557e-07, 3.948121362192525e-07, 3.7507152940828976e-07, 3.563179529378755e-07, 3.385020552909817e-07, 3.215769525264325e-07, 3.0549810490011087e-07, 2.9022319965510536e-07, 2.7571203967235015e-07, 2.6192643768873256e-07, 2.488301158042958e-07, 2.363886100140811e-07, 2.2456917951337705e-07, 2.1334072053770805e-07, 2.0267368451082286e-07, 1.925400002852817e-07, 1.829130002710175e-07, 1.7376735025746665e-07, 1.6507898274459337e-07, 1.5682503360736362e-07, 1.489837819269954e-07, 1.415345928306458e-07, 1.3445786318911337e-07, 1.2773497002965766e-07, 1.2134822152817485e-07, 1.1528081045176609e-07, 1.0951676992917772e-07, 1.0404093143271886e-07, 9.8838884861083e-08, 9.389694061802875e-08, 8.920209358712735e-08, 8.474198890777098e-08, 8.050488946238242e-08, 7.647964498926329e-08, 7.265566273980012e-08, 6.902287960281014e-08, 6.55717356226696e-08, 6.229314884153617e-08, 5.917849139945931e-08, 5.621956682948637e-08, 5.340858848801201e-08, 5.073815906361144e-08, 4.820125111043083e-08, 4.579118855490929e-08, 4.350162912716384e-08, 4.132654767080562e-08, 3.926022028726534e-08, 3.7297209272902094e-08, 3.5432348809256986e-08, 3.366073136879413e-08, 3.197769480035442e-08, 3.037881006033669e-08, 2.8859869557319853e-08, 2.7416876079453868e-08, 2.6046032275481165e-08, 2.4743730661707118e-08, 2.3506544128621765e-08, 2.2331216922190656e-08, 2.1214656076081113e-08, 2.0153923272277068e-08, 1.9146227108663224e-08, 1.8188915753230045e-08, 1.7279469965568557e-08, 1.6415496467290112e-08, 1.5594721643925592e-08, 1.4814985561729348e-08, 1.4074236283642857e-08, 1.3370524469460718e-08, 1.2701998245987683e-08, 1.2066898333688296e-08, 1.1463553417003885e-08, 1.0890375746153691e-08, 1.0345856958846005e-08, 9.828564110903703e-09, 9.33713590535853e-09, 8.870279110090592e-09, 8.426765154586063e-09, 8.00542689685676e-09, 7.605155552013918e-09, 7.224897774413221e-09, 6.86365288569256e-09, 6.5204702414079335e-09, 6.1944467293375395e-09, 5.88472439287066e-09, 5.590488173227124e-09, 5.310963764565772e-09, 5.04541557633748e-09, 4.793144797520604e-09, 4.553487557644575e-09, 4.325813179762344e-09, 4.109522520774226e-09, 3.90404639473552e-09, 3.7088440749987433e-09]\n",
      "Epoch 18, Batch 0, Train Loss: 0.14936764538288116, Memory (GB): 7\n",
      "Epoch 18, Batch 1, Train Loss: 0.14652284979820251, Memory (GB): 7\n",
      "Epoch 18, Batch 2, Train Loss: 0.12545545399188995, Memory (GB): 7\n",
      "Epoch 18, Batch 3, Train Loss: 0.14045226573944092, Memory (GB): 7\n",
      "Epoch 18, Batch 4, Train Loss: 0.15386036038398743, Memory (GB): 7\n",
      "Epoch 18, Batch 5, Train Loss: 0.1314324289560318, Memory (GB): 7\n",
      "Epoch 18, Batch 6, Train Loss: 0.1543246954679489, Memory (GB): 7\n",
      "Epoch 18, Batch 7, Train Loss: 0.1287202090024948, Memory (GB): 7\n",
      "Epoch 18, Batch 8, Train Loss: 0.14888565242290497, Memory (GB): 7\n",
      "Epoch 18, Batch 9, Train Loss: 0.131370410323143, Memory (GB): 7\n",
      "Epoch 18, Batch 10, Train Loss: 0.13370783627033234, Memory (GB): 7\n",
      "Epoch 18, Batch 11, Train Loss: 0.1309785395860672, Memory (GB): 7\n",
      "Epoch 18, Batch 12, Train Loss: 0.12665483355522156, Memory (GB): 7\n",
      "Epoch 18, Batch 13, Train Loss: 0.1374034434556961, Memory (GB): 7\n",
      "Epoch 18, Batch 14, Train Loss: 0.1312243640422821, Memory (GB): 7\n",
      "Epoch 18, Batch 15, Train Loss: 0.14631950855255127, Memory (GB): 7\n",
      "Epoch 18, Batch 16, Train Loss: 0.13913285732269287, Memory (GB): 7\n",
      "Epoch 18, Batch 17, Train Loss: 0.15632307529449463, Memory (GB): 7\n",
      "Epoch 18, Batch 18, Train Loss: 0.145796000957489, Memory (GB): 7\n",
      "Epoch 18, Batch 19, Train Loss: 0.14762040972709656, Memory (GB): 7\n",
      "Epoch 18, Batch 20, Train Loss: 0.13540293276309967, Memory (GB): 7\n",
      "Epoch 18, Batch 21, Train Loss: 0.1291559338569641, Memory (GB): 7\n",
      "Epoch 18, Batch 22, Train Loss: 0.1436343938112259, Memory (GB): 7\n",
      "Epoch 18, Batch 23, Train Loss: 0.13175319135189056, Memory (GB): 7\n",
      "Epoch 18, Batch 24, Train Loss: 0.12900525331497192, Memory (GB): 7\n",
      "Epoch 18, Batch 25, Train Loss: 0.1452697366476059, Memory (GB): 7\n",
      "Epoch 18, Batch 26, Train Loss: 0.13245555758476257, Memory (GB): 7\n",
      "Epoch 18, Batch 27, Train Loss: 0.1398550271987915, Memory (GB): 7\n",
      "Epoch 18, Batch 28, Train Loss: 0.13948163390159607, Memory (GB): 7\n",
      "Epoch 18, Batch 29, Train Loss: 0.1546880602836609, Memory (GB): 7\n",
      "Epoch 18, Batch 30, Train Loss: 0.11698690801858902, Memory (GB): 7\n",
      "Epoch 18, Batch 31, Train Loss: 0.1504567414522171, Memory (GB): 7\n",
      "Epoch 18, Batch 32, Train Loss: 0.13988932967185974, Memory (GB): 7\n",
      "Epoch 18, Batch 33, Train Loss: 0.1331077367067337, Memory (GB): 7\n",
      "Epoch 18, Batch 34, Train Loss: 0.1259966790676117, Memory (GB): 7\n",
      "Epoch 18, Batch 35, Train Loss: 0.13191018998622894, Memory (GB): 7\n",
      "Epoch 18, Batch 36, Train Loss: 0.15443985164165497, Memory (GB): 7\n",
      "Epoch 18, Batch 37, Train Loss: 0.1608588546514511, Memory (GB): 7\n",
      "Epoch 18, Batch 38, Train Loss: 0.14442169666290283, Memory (GB): 7\n",
      "Epoch 18, Batch 39, Train Loss: 0.12485074996948242, Memory (GB): 7\n",
      "Epoch 18, Batch 40, Train Loss: 0.16155438125133514, Memory (GB): 7\n",
      "Epoch 18, Batch 41, Train Loss: 0.13193000853061676, Memory (GB): 7\n",
      "Epoch 18, Batch 42, Train Loss: 0.13809846341609955, Memory (GB): 7\n",
      "Epoch 18, Batch 43, Train Loss: 0.140905499458313, Memory (GB): 7\n",
      "Epoch 18, Batch 44, Train Loss: 0.13864614069461823, Memory (GB): 7\n",
      "Epoch 18, Batch 45, Train Loss: 0.14699304103851318, Memory (GB): 7\n",
      "Epoch 18, Batch 46, Train Loss: 0.14968779683113098, Memory (GB): 7\n",
      "Epoch 18, Batch 47, Train Loss: 0.1583562195301056, Memory (GB): 7\n",
      "Epoch 18, Batch 48, Train Loss: 0.1491299867630005, Memory (GB): 7\n",
      "Epoch 18, Batch 49, Train Loss: 0.14149314165115356, Memory (GB): 7\n",
      "Epoch 18, Batch 50, Train Loss: 0.1564028114080429, Memory (GB): 7\n",
      "Epoch 18, Batch 51, Train Loss: 0.1667664796113968, Memory (GB): 7\n",
      "Epoch 18, Batch 52, Train Loss: 0.1414846032857895, Memory (GB): 7\n",
      "Epoch 18, Batch 53, Train Loss: 0.16466309130191803, Memory (GB): 7\n",
      "Epoch 18, Batch 54, Train Loss: 0.15407592058181763, Memory (GB): 7\n",
      "Epoch 18, Batch 55, Train Loss: 0.18434317409992218, Memory (GB): 7\n",
      "Epoch 18, Batch 56, Train Loss: 0.15595951676368713, Memory (GB): 7\n",
      "Epoch 18, Batch 57, Train Loss: 0.14173933863639832, Memory (GB): 7\n",
      "Epoch 18, Batch 58, Train Loss: 0.157310351729393, Memory (GB): 7\n",
      "Epoch 18, Batch 59, Train Loss: 0.15733346343040466, Memory (GB): 7\n",
      "Epoch 18, Batch 60, Train Loss: 0.15069659054279327, Memory (GB): 7\n",
      "Epoch 18, Batch 61, Train Loss: 0.15978103876113892, Memory (GB): 7\n",
      "Epoch 18, Batch 62, Train Loss: 0.15448696911334991, Memory (GB): 7\n",
      "Epoch 18, Batch 63, Train Loss: 0.17510299384593964, Memory (GB): 7\n",
      "Epoch 18, Batch 64, Train Loss: 0.16121062636375427, Memory (GB): 7\n",
      "Epoch 18, Batch 65, Train Loss: 0.16872523725032806, Memory (GB): 7\n",
      "Epoch 18, Batch 66, Train Loss: 0.17168636620044708, Memory (GB): 7\n",
      "Epoch 18, Batch 67, Train Loss: 0.16782736778259277, Memory (GB): 7\n",
      "Epoch 18, Batch 68, Train Loss: 0.14234478771686554, Memory (GB): 7\n",
      "Epoch 18, Batch 69, Train Loss: 0.15664446353912354, Memory (GB): 7\n",
      "Epoch 18, Batch 70, Train Loss: 0.13289737701416016, Memory (GB): 7\n",
      "Epoch 18, Batch 71, Train Loss: 0.14165903627872467, Memory (GB): 7\n",
      "Epoch 18, Batch 72, Train Loss: 0.1573142260313034, Memory (GB): 7\n",
      "Epoch 18, Batch 73, Train Loss: 0.15700292587280273, Memory (GB): 7\n",
      "Epoch 18, Batch 74, Train Loss: 0.1401604562997818, Memory (GB): 7\n",
      "Epoch 18, Batch 75, Train Loss: 0.14682765305042267, Memory (GB): 7\n",
      "Epoch 18, Batch 76, Train Loss: 0.15879414975643158, Memory (GB): 7\n",
      "Epoch 18, Batch 77, Train Loss: 0.14800873398780823, Memory (GB): 7\n",
      "Epoch 18, Batch 78, Train Loss: 0.17873653769493103, Memory (GB): 7\n",
      "Epoch 18, Batch 79, Train Loss: 0.137674018740654, Memory (GB): 7\n",
      "Epoch 18, Batch 80, Train Loss: 0.1638903170824051, Memory (GB): 7\n",
      "Epoch 18, Batch 81, Train Loss: 0.15350614488124847, Memory (GB): 7\n",
      "Epoch 18, Batch 82, Train Loss: 0.1559380739927292, Memory (GB): 7\n",
      "Epoch 18, Batch 83, Train Loss: 0.14866913855075836, Memory (GB): 7\n",
      "Epoch 18, Batch 84, Train Loss: 0.141808420419693, Memory (GB): 7\n",
      "Epoch 18, Batch 85, Train Loss: 0.13352975249290466, Memory (GB): 7\n",
      "Epoch 18, Batch 86, Train Loss: 0.14600197970867157, Memory (GB): 7\n",
      "Epoch 18, Batch 87, Train Loss: 0.15033753216266632, Memory (GB): 7\n",
      "Epoch 18, Batch 88, Train Loss: 0.15514032542705536, Memory (GB): 7\n",
      "Epoch 18, Batch 89, Train Loss: 0.15435735881328583, Memory (GB): 7\n",
      "Epoch 18, Batch 90, Train Loss: 0.14288055896759033, Memory (GB): 7\n",
      "Epoch 18, Batch 91, Train Loss: 0.1683201789855957, Memory (GB): 7\n",
      "Epoch 18, Batch 92, Train Loss: 0.1819203794002533, Memory (GB): 7\n",
      "Epoch 18, Batch 93, Train Loss: 0.13546247780323029, Memory (GB): 7\n",
      "Epoch 18, Batch 94, Train Loss: 0.15358895063400269, Memory (GB): 7\n",
      "Epoch 18, Batch 95, Train Loss: 0.13633155822753906, Memory (GB): 7\n",
      "Epoch 18, Batch 96, Train Loss: 0.13509352505207062, Memory (GB): 7\n",
      "Epoch 18, Batch 97, Train Loss: 0.13605812191963196, Memory (GB): 7\n",
      "Epoch 18, Batch 98, Train Loss: 0.13732042908668518, Memory (GB): 7\n",
      "Epoch 18, Batch 99, Train Loss: 0.1311495304107666, Memory (GB): 7\n",
      "Epoch 18, Batch 100, Train Loss: 0.1408616155385971, Memory (GB): 7\n",
      "Epoch 18, Batch 101, Train Loss: 0.15444599092006683, Memory (GB): 7\n",
      "Epoch 18, Batch 102, Train Loss: 0.15176263451576233, Memory (GB): 7\n",
      "Epoch 18, Batch 103, Train Loss: 0.12988857924938202, Memory (GB): 7\n",
      "Epoch 18, Batch 104, Train Loss: 0.15077504515647888, Memory (GB): 7\n",
      "Epoch 18, Batch 105, Train Loss: 0.1447620689868927, Memory (GB): 7\n",
      "Epoch 18, Batch 106, Train Loss: 0.1637144386768341, Memory (GB): 7\n",
      "Epoch 18, Batch 107, Train Loss: 0.1638796627521515, Memory (GB): 7\n",
      "Epoch 18, Batch 108, Train Loss: 0.1394587904214859, Memory (GB): 7\n",
      "Epoch 18, Batch 109, Train Loss: 0.12191101163625717, Memory (GB): 7\n",
      "Epoch 18, Batch 110, Train Loss: 0.14168168604373932, Memory (GB): 7\n",
      "Epoch 18, Batch 111, Train Loss: 0.15027374029159546, Memory (GB): 7\n",
      "Epoch 18, Batch 112, Train Loss: 0.1458907127380371, Memory (GB): 7\n",
      "Epoch 18, Batch 113, Train Loss: 0.1339193433523178, Memory (GB): 7\n",
      "Epoch 18, Batch 114, Train Loss: 0.1501806080341339, Memory (GB): 7\n",
      "Epoch 18, Batch 115, Train Loss: 0.1339363157749176, Memory (GB): 7\n",
      "Epoch 18, Batch 116, Train Loss: 0.1376100331544876, Memory (GB): 7\n",
      "Epoch 18, Batch 117, Train Loss: 0.15154148638248444, Memory (GB): 7\n",
      "Epoch 18, Batch 118, Train Loss: 0.12989214062690735, Memory (GB): 7\n",
      "Epoch 18, Batch 119, Train Loss: 0.13189436495304108, Memory (GB): 7\n",
      "Epoch 18, Batch 120, Train Loss: 0.16406981647014618, Memory (GB): 7\n",
      "Epoch 18, Batch 121, Train Loss: 0.14082542061805725, Memory (GB): 7\n",
      "Epoch 18, Batch 122, Train Loss: 0.16031888127326965, Memory (GB): 7\n",
      "Epoch 18, Batch 123, Train Loss: 0.13773247599601746, Memory (GB): 7\n",
      "Epoch 18, Batch 124, Train Loss: 0.16890934109687805, Memory (GB): 7\n",
      "Epoch 18, Batch 125, Train Loss: 0.1394290030002594, Memory (GB): 7\n",
      "Epoch 18, Batch 126, Train Loss: 0.14639179408550262, Memory (GB): 7\n",
      "Epoch 18, Batch 127, Train Loss: 0.13700897991657257, Memory (GB): 7\n",
      "Epoch 18, Batch 128, Train Loss: 0.144863098859787, Memory (GB): 7\n",
      "Epoch 18, Batch 129, Train Loss: 0.1399800032377243, Memory (GB): 7\n",
      "Epoch 18, Batch 130, Train Loss: 0.14190314710140228, Memory (GB): 7\n",
      "Epoch 18, Batch 131, Train Loss: 0.13625292479991913, Memory (GB): 7\n",
      "Epoch 18, Batch 132, Train Loss: 0.13556310534477234, Memory (GB): 7\n",
      "Epoch 18, Batch 133, Train Loss: 0.13981303572654724, Memory (GB): 7\n",
      "Epoch 18, Batch 134, Train Loss: 0.1293601095676422, Memory (GB): 7\n",
      "Epoch 18, Batch 135, Train Loss: 0.15040138363838196, Memory (GB): 7\n",
      "Epoch 18, Batch 136, Train Loss: 0.1354040652513504, Memory (GB): 7\n",
      "Epoch 18, Batch 137, Train Loss: 0.13079799711704254, Memory (GB): 7\n",
      "Epoch 18, Batch 138, Train Loss: 0.1376737654209137, Memory (GB): 7\n",
      "Epoch 18, Batch 139, Train Loss: 0.13281984627246857, Memory (GB): 7\n",
      "Epoch 18, Batch 140, Train Loss: 0.14208944141864777, Memory (GB): 7\n",
      "Epoch 18, Batch 141, Train Loss: 0.17579469084739685, Memory (GB): 7\n",
      "Epoch 18, Batch 142, Train Loss: 0.15972697734832764, Memory (GB): 7\n",
      "Epoch 18, Batch 143, Train Loss: 0.14145906269550323, Memory (GB): 7\n",
      "Epoch 18, Batch 144, Train Loss: 0.1520075649023056, Memory (GB): 7\n",
      "Epoch 18, Batch 145, Train Loss: 0.13313056528568268, Memory (GB): 7\n",
      "Epoch 18, Batch 146, Train Loss: 0.1430967152118683, Memory (GB): 7\n",
      "Epoch 18, Batch 147, Train Loss: 0.135545015335083, Memory (GB): 7\n",
      "Epoch 18, Batch 148, Train Loss: 0.13617360591888428, Memory (GB): 7\n",
      "Epoch 18, Batch 149, Train Loss: 0.14602558314800262, Memory (GB): 7\n",
      "Epoch 18, Batch 150, Train Loss: 0.14250849187374115, Memory (GB): 7\n",
      "Epoch 18, Batch 151, Train Loss: 0.13602429628372192, Memory (GB): 7\n",
      "Epoch 18, Batch 152, Train Loss: 0.15462586283683777, Memory (GB): 7\n",
      "Epoch 18, Batch 153, Train Loss: 0.1424833983182907, Memory (GB): 7\n",
      "Epoch 18, Batch 154, Train Loss: 0.15679818391799927, Memory (GB): 7\n",
      "Epoch 18, Batch 155, Train Loss: 0.14543434977531433, Memory (GB): 7\n",
      "Epoch 18, Batch 156, Train Loss: 0.15676811337471008, Memory (GB): 7\n",
      "Epoch 18, Batch 157, Train Loss: 0.14803825318813324, Memory (GB): 7\n",
      "Epoch 18, Batch 158, Train Loss: 0.15541943907737732, Memory (GB): 7\n",
      "Epoch 18, Batch 159, Train Loss: 0.144173264503479, Memory (GB): 7\n",
      "Epoch 18, Batch 160, Train Loss: 0.1400037705898285, Memory (GB): 7\n",
      "Epoch 18, Batch 161, Train Loss: 0.1439080536365509, Memory (GB): 7\n",
      "Epoch 18, Batch 162, Train Loss: 0.15881173312664032, Memory (GB): 7\n",
      "Epoch 18, Batch 163, Train Loss: 0.14237885177135468, Memory (GB): 7\n",
      "Epoch 18, Batch 164, Train Loss: 0.15484623610973358, Memory (GB): 7\n",
      "Epoch 18, Batch 165, Train Loss: 0.15199415385723114, Memory (GB): 7\n",
      "Epoch 18, Batch 166, Train Loss: 0.1499512791633606, Memory (GB): 7\n",
      "Epoch 18, Batch 167, Train Loss: 0.17649638652801514, Memory (GB): 7\n",
      "Epoch 18, Batch 168, Train Loss: 0.14236879348754883, Memory (GB): 7\n",
      "Epoch 18, Batch 169, Train Loss: 0.16793930530548096, Memory (GB): 7\n",
      "Epoch 18, Batch 170, Train Loss: 0.1716758906841278, Memory (GB): 7\n",
      "Epoch 18, Batch 171, Train Loss: 0.15390713512897491, Memory (GB): 7\n",
      "Epoch 18, Batch 172, Train Loss: 0.16168639063835144, Memory (GB): 7\n",
      "Epoch 18, Batch 173, Train Loss: 0.16694359481334686, Memory (GB): 7\n",
      "Epoch 18, Batch 174, Train Loss: 0.1513201892375946, Memory (GB): 7\n",
      "Epoch 18, Batch 175, Train Loss: 0.152788907289505, Memory (GB): 7\n",
      "Epoch 18, Batch 176, Train Loss: 0.13885845243930817, Memory (GB): 7\n",
      "Epoch 18, Batch 177, Train Loss: 0.14979755878448486, Memory (GB): 7\n",
      "Epoch 18, Batch 178, Train Loss: 0.15568597614765167, Memory (GB): 7\n",
      "Epoch 18, Batch 179, Train Loss: 0.12357323616743088, Memory (GB): 7\n",
      "[0.0006545084971871749, 0.000621783072327816, 0.0005906939187114252, 0.0005611592227758544, 0.0005331012616370621, 0.0005064461985552078, 0.0004811238886274478, 0.00045706769419607537, 0.0004342143094862714, 0.00041250359401195794, 0.0003918784143113601, 0.0003722844935957919, 0.00035367026891600247, 0.0003359867554702024, 0.00031918741769669203, 0.0003032280468118574, 0.0002880666444712647, 0.00027366331224770144, 0.0002599801466353164, 0.00024698113930355037, 0.00023463208233837294, 0.00022290047822145438, 0.00021175545431038138, 0.00020116768159486245, 0.00019110929751511925, 0.00018155383263936337, 0.00017247614100739513, 0.00016385233395702537, 0.0001556597172591741, 0.0001478767313962153, 0.00014048289482640464, 0.00013345875008508433, 0.00012678581258083012, 0.00012044652195178861, 0.00011442419585419914, 0.00010870298606148919, 0.00010326783675841478, 9.81044449204941e-05, 9.319922267446932e-05, 8.853926154074583e-05, 8.411229846370851e-05, 7.990668354052312e-05, 7.591134936349688e-05, 7.211578189532204e-05, 6.8509992800556e-05, 6.508449316052818e-05, 6.18302685025018e-05, 5.87387550773767e-05, 5.580181732350784e-05, 5.3011726457332465e-05, 5.036114013446579e-05, 4.784308312774253e-05, 4.5450928971355394e-05, 4.317838252278758e-05, 4.101946339664826e-05, 3.896849022681584e-05, 3.7020065715475035e-05, 3.516906242970128e-05, 3.3410609308216214e-05, 3.174007884280542e-05, 3.0153074900665132e-05, 2.8645421155631858e-05, 2.721315009785025e-05, 2.5852492592957756e-05, 2.455986796330987e-05, 2.3331874565144385e-05, 2.2165280836887157e-05, 2.1057016795042813e-05, 2.0004165955290658e-05, 1.900395765752613e-05, 1.8053759774649812e-05, 1.7151071785917318e-05, 1.6293518196621458e-05, 1.5478842286790397e-05, 1.4704900172450857e-05, 1.3969655163828323e-05, 1.32711724056369e-05, 1.2607613785355067e-05, 1.19772330960873e-05, 1.1378371441282937e-05, 1.0809452869218792e-05, 1.0268980225757853e-05, 9.755531214469958e-06, 9.26775465374646e-06, 8.804366921059134e-06, 8.364148575006183e-06, 7.94594114625587e-06, 7.548644088943075e-06, 7.171211884495926e-06, 6.812651290271129e-06, 6.472018725757574e-06, 6.148417789469691e-06, 5.840996899996211e-06, 5.548947054996399e-06, 5.271499702246577e-06, 5.007924717134247e-06, 4.757528481277534e-06, 4.51965205721366e-06, 4.293669454352977e-06, 4.078985981635328e-06, 3.875036682553559e-06, 3.681284848425884e-06, 3.4972206060045847e-06, 3.3223595757043576e-06, 3.1562415969191405e-06, 2.998429517073182e-06, 2.848508041219523e-06, 2.706082639158546e-06, 2.5707785072006194e-06, 2.4422395818405887e-06, 2.3201276027485603e-06, 2.2041212226111308e-06, 2.093915161480574e-06, 1.989219403406545e-06, 1.889758433236218e-06, 1.795270511574407e-06, 1.7055069859956877e-06, 1.6202316366959027e-06, 1.5392200548611086e-06, 1.4622590521180517e-06, 1.3891460995121487e-06, 1.3196887945365418e-06, 1.2537043548097153e-06, 1.1910191370692282e-06, 1.1314681802157671e-06, 1.0748947712049784e-06, 1.02115003264473e-06, 9.700925310124935e-07, 9.215879044618686e-07, 8.755085092387746e-07, 8.317330837768363e-07, 7.901464295879943e-07, 7.506391081085952e-07, 7.131071527031649e-07, 6.774517950680069e-07, 6.435792053146068e-07, 6.114002450488754e-07, 5.808302327964321e-07, 5.517887211566108e-07, 5.241992850987799e-07, 4.979893208438408e-07, 4.730898548016487e-07, 4.4943536206156653e-07, 4.269635939584882e-07, 4.056154142605633e-07, 3.853346435475354e-07, 3.6606791137015863e-07, 3.4776451580165083e-07, 3.303762900115679e-07, 3.1385747551098965e-07, 2.981646017354403e-07, 2.832563716486681e-07, 2.6909355306623486e-07, 2.5563887541292287e-07, 2.428569316422768e-07, 2.307140850601629e-07, 2.1917838080715493e-07, 2.0821946176679706e-07, 1.9780848867845718e-07, 1.8791806424453446e-07, 1.7852216103230752e-07, 1.6959605298069232e-07, 1.611162503316575e-07, 1.5306043781507465e-07, 1.4540741592432097e-07, 1.3813704512810486e-07, 1.312301928716996e-07, 1.2466868322811469e-07, 1.1843524906670896e-07, 1.1251348661337348e-07, 1.0688781228270471e-07, 1.0154342166856949e-07, 9.646625058514109e-08, 9.1642938055884e-08, 8.706079115308974e-08, 8.27077515954353e-08, 7.857236401566355e-08, 7.464374581488039e-08, 7.091155852413632e-08, 6.736598059792955e-08, 6.399768156803306e-08, 6.07977974896314e-08, 5.775790761514981e-08, 5.4870012234392365e-08, 5.212651162267271e-08, 4.952018604153908e-08, 4.704417673946211e-08, 4.469196790248903e-08, 4.245736950736454e-08, 4.0334501031996306e-08, 3.8317775980396475e-08, 3.640188718137666e-08, 3.458179282230782e-08, 3.285270318119244e-08, 3.121006802213281e-08, 2.9649564621026165e-08, 2.8167086389974893e-08, 2.6758732070476108e-08, 2.542079546695231e-08]\n",
      "Epoch 19, Batch 0, Train Loss: 0.1320618838071823, Memory (GB): 7\n",
      "Epoch 19, Batch 1, Train Loss: 0.13634970784187317, Memory (GB): 7\n",
      "Epoch 19, Batch 2, Train Loss: 0.14899954199790955, Memory (GB): 7\n",
      "Epoch 19, Batch 3, Train Loss: 0.14773476123809814, Memory (GB): 7\n",
      "Epoch 19, Batch 4, Train Loss: 0.1485958993434906, Memory (GB): 7\n",
      "Epoch 19, Batch 5, Train Loss: 0.1492539793252945, Memory (GB): 7\n",
      "Epoch 19, Batch 6, Train Loss: 0.14500738680362701, Memory (GB): 7\n",
      "Epoch 19, Batch 7, Train Loss: 0.1586698591709137, Memory (GB): 7\n",
      "Epoch 19, Batch 8, Train Loss: 0.14288446307182312, Memory (GB): 7\n",
      "Epoch 19, Batch 9, Train Loss: 0.13052484393119812, Memory (GB): 7\n",
      "Epoch 19, Batch 10, Train Loss: 0.13994558155536652, Memory (GB): 7\n",
      "Epoch 19, Batch 11, Train Loss: 0.12277553975582123, Memory (GB): 7\n",
      "Epoch 19, Batch 12, Train Loss: 0.14835834503173828, Memory (GB): 7\n",
      "Epoch 19, Batch 13, Train Loss: 0.1298784464597702, Memory (GB): 7\n",
      "Epoch 19, Batch 14, Train Loss: 0.1378372460603714, Memory (GB): 7\n",
      "Epoch 19, Batch 15, Train Loss: 0.14376956224441528, Memory (GB): 7\n",
      "Epoch 19, Batch 16, Train Loss: 0.13139177858829498, Memory (GB): 7\n",
      "Epoch 19, Batch 17, Train Loss: 0.12401162832975388, Memory (GB): 7\n",
      "Epoch 19, Batch 18, Train Loss: 0.12468300759792328, Memory (GB): 7\n",
      "Epoch 19, Batch 19, Train Loss: 0.14046967029571533, Memory (GB): 7\n",
      "Epoch 19, Batch 20, Train Loss: 0.12655188143253326, Memory (GB): 7\n",
      "Epoch 19, Batch 21, Train Loss: 0.1463714987039566, Memory (GB): 7\n",
      "Epoch 19, Batch 22, Train Loss: 0.12793318927288055, Memory (GB): 7\n",
      "Epoch 19, Batch 23, Train Loss: 0.15171530842781067, Memory (GB): 7\n",
      "Epoch 19, Batch 24, Train Loss: 0.140325665473938, Memory (GB): 7\n",
      "Epoch 19, Batch 25, Train Loss: 0.13570746779441833, Memory (GB): 7\n",
      "Epoch 19, Batch 26, Train Loss: 0.12870575487613678, Memory (GB): 7\n",
      "Epoch 19, Batch 27, Train Loss: 0.12152988463640213, Memory (GB): 7\n",
      "Epoch 19, Batch 28, Train Loss: 0.13084173202514648, Memory (GB): 7\n",
      "Epoch 19, Batch 29, Train Loss: 0.14229904115200043, Memory (GB): 7\n",
      "Epoch 19, Batch 30, Train Loss: 0.1159648448228836, Memory (GB): 7\n",
      "Epoch 19, Batch 31, Train Loss: 0.13577456772327423, Memory (GB): 7\n",
      "Epoch 19, Batch 32, Train Loss: 0.12528352439403534, Memory (GB): 7\n",
      "Epoch 19, Batch 33, Train Loss: 0.13253264129161835, Memory (GB): 7\n",
      "Epoch 19, Batch 34, Train Loss: 0.13969120383262634, Memory (GB): 7\n",
      "Epoch 19, Batch 35, Train Loss: 0.1263556033372879, Memory (GB): 7\n",
      "Epoch 19, Batch 36, Train Loss: 0.13707661628723145, Memory (GB): 7\n",
      "Epoch 19, Batch 37, Train Loss: 0.1289968341588974, Memory (GB): 7\n",
      "Epoch 19, Batch 38, Train Loss: 0.15143278241157532, Memory (GB): 7\n",
      "Epoch 19, Batch 39, Train Loss: 0.1180235743522644, Memory (GB): 7\n",
      "Epoch 19, Batch 40, Train Loss: 0.13033363223075867, Memory (GB): 7\n",
      "Epoch 19, Batch 41, Train Loss: 0.1225607618689537, Memory (GB): 7\n",
      "Epoch 19, Batch 42, Train Loss: 0.13002368807792664, Memory (GB): 7\n",
      "Epoch 19, Batch 43, Train Loss: 0.10773498564958572, Memory (GB): 7\n",
      "Epoch 19, Batch 44, Train Loss: 0.13765880465507507, Memory (GB): 7\n",
      "Epoch 19, Batch 45, Train Loss: 0.14563500881195068, Memory (GB): 7\n",
      "Epoch 19, Batch 46, Train Loss: 0.150207057595253, Memory (GB): 7\n",
      "Epoch 19, Batch 47, Train Loss: 0.11827196180820465, Memory (GB): 7\n",
      "Epoch 19, Batch 48, Train Loss: 0.14671476185321808, Memory (GB): 7\n",
      "Epoch 19, Batch 49, Train Loss: 0.11869952827692032, Memory (GB): 7\n",
      "Epoch 19, Batch 50, Train Loss: 0.1339094489812851, Memory (GB): 7\n",
      "Epoch 19, Batch 51, Train Loss: 0.13752838969230652, Memory (GB): 7\n",
      "Epoch 19, Batch 52, Train Loss: 0.13189302384853363, Memory (GB): 7\n",
      "Epoch 19, Batch 53, Train Loss: 0.12444047629833221, Memory (GB): 7\n",
      "Epoch 19, Batch 54, Train Loss: 0.13051122426986694, Memory (GB): 7\n",
      "Epoch 19, Batch 55, Train Loss: 0.12599416077136993, Memory (GB): 7\n",
      "Epoch 19, Batch 56, Train Loss: 0.12596793472766876, Memory (GB): 7\n",
      "Epoch 19, Batch 57, Train Loss: 0.13590314984321594, Memory (GB): 7\n",
      "Epoch 19, Batch 58, Train Loss: 0.15710340440273285, Memory (GB): 7\n",
      "Epoch 19, Batch 59, Train Loss: 0.1161777526140213, Memory (GB): 7\n",
      "Epoch 19, Batch 60, Train Loss: 0.13049066066741943, Memory (GB): 7\n",
      "Epoch 19, Batch 61, Train Loss: 0.1210469976067543, Memory (GB): 7\n",
      "Epoch 19, Batch 62, Train Loss: 0.1449931263923645, Memory (GB): 7\n",
      "Epoch 19, Batch 63, Train Loss: 0.12222442030906677, Memory (GB): 7\n",
      "Epoch 19, Batch 64, Train Loss: 0.14628903567790985, Memory (GB): 7\n",
      "Epoch 19, Batch 65, Train Loss: 0.12565010786056519, Memory (GB): 7\n",
      "Epoch 19, Batch 66, Train Loss: 0.1278502643108368, Memory (GB): 7\n",
      "Epoch 19, Batch 67, Train Loss: 0.12294695526361465, Memory (GB): 7\n",
      "Epoch 19, Batch 68, Train Loss: 0.1121457889676094, Memory (GB): 7\n",
      "Epoch 19, Batch 69, Train Loss: 0.13659298419952393, Memory (GB): 7\n",
      "Epoch 19, Batch 70, Train Loss: 0.11797879636287689, Memory (GB): 7\n",
      "Epoch 19, Batch 71, Train Loss: 0.13984616100788116, Memory (GB): 7\n",
      "Epoch 19, Batch 72, Train Loss: 0.13148027658462524, Memory (GB): 7\n",
      "Epoch 19, Batch 73, Train Loss: 0.12951795756816864, Memory (GB): 7\n",
      "Epoch 19, Batch 74, Train Loss: 0.13410043716430664, Memory (GB): 7\n",
      "Epoch 19, Batch 75, Train Loss: 0.12028849869966507, Memory (GB): 7\n",
      "Epoch 19, Batch 76, Train Loss: 0.13142159581184387, Memory (GB): 7\n",
      "Epoch 19, Batch 77, Train Loss: 0.14090535044670105, Memory (GB): 7\n",
      "Epoch 19, Batch 78, Train Loss: 0.1407162994146347, Memory (GB): 7\n",
      "Epoch 19, Batch 79, Train Loss: 0.11353427171707153, Memory (GB): 7\n",
      "Epoch 19, Batch 80, Train Loss: 0.13352589309215546, Memory (GB): 7\n",
      "Epoch 19, Batch 81, Train Loss: 0.14954634010791779, Memory (GB): 7\n",
      "Epoch 19, Batch 82, Train Loss: 0.1417907178401947, Memory (GB): 7\n",
      "Epoch 19, Batch 83, Train Loss: 0.15284347534179688, Memory (GB): 7\n",
      "Epoch 19, Batch 84, Train Loss: 0.14316096901893616, Memory (GB): 7\n",
      "Epoch 19, Batch 85, Train Loss: 0.16713666915893555, Memory (GB): 7\n",
      "Epoch 19, Batch 86, Train Loss: 0.1275714635848999, Memory (GB): 7\n",
      "Epoch 19, Batch 87, Train Loss: 0.1311797797679901, Memory (GB): 7\n",
      "Epoch 19, Batch 88, Train Loss: 0.13400135934352875, Memory (GB): 7\n",
      "Epoch 19, Batch 89, Train Loss: 0.12283413857221603, Memory (GB): 7\n",
      "Epoch 19, Batch 90, Train Loss: 0.12765108048915863, Memory (GB): 7\n",
      "Epoch 19, Batch 91, Train Loss: 0.1412314772605896, Memory (GB): 7\n",
      "Epoch 19, Batch 92, Train Loss: 0.13448965549468994, Memory (GB): 7\n",
      "Epoch 19, Batch 93, Train Loss: 0.1466907411813736, Memory (GB): 7\n",
      "Epoch 19, Batch 94, Train Loss: 0.1587093025445938, Memory (GB): 7\n",
      "Epoch 19, Batch 95, Train Loss: 0.12264687567949295, Memory (GB): 7\n",
      "Epoch 19, Batch 96, Train Loss: 0.14536264538764954, Memory (GB): 7\n",
      "Epoch 19, Batch 97, Train Loss: 0.14478926360607147, Memory (GB): 7\n",
      "Epoch 19, Batch 98, Train Loss: 0.14701025187969208, Memory (GB): 7\n",
      "Epoch 19, Batch 99, Train Loss: 0.13496899604797363, Memory (GB): 7\n",
      "Epoch 19, Batch 100, Train Loss: 0.12811243534088135, Memory (GB): 7\n",
      "Epoch 19, Batch 101, Train Loss: 0.13425661623477936, Memory (GB): 7\n",
      "Epoch 19, Batch 102, Train Loss: 0.1333269625902176, Memory (GB): 7\n",
      "Epoch 19, Batch 103, Train Loss: 0.14216530323028564, Memory (GB): 7\n",
      "Epoch 19, Batch 104, Train Loss: 0.13154719769954681, Memory (GB): 7\n",
      "Epoch 19, Batch 105, Train Loss: 0.14537450671195984, Memory (GB): 7\n",
      "Epoch 19, Batch 106, Train Loss: 0.1330210417509079, Memory (GB): 7\n",
      "Epoch 19, Batch 107, Train Loss: 0.13047383725643158, Memory (GB): 7\n",
      "Epoch 19, Batch 108, Train Loss: 0.1205151304602623, Memory (GB): 7\n",
      "Epoch 19, Batch 109, Train Loss: 0.12444523721933365, Memory (GB): 7\n",
      "Epoch 19, Batch 110, Train Loss: 0.12775124609470367, Memory (GB): 7\n",
      "Epoch 19, Batch 111, Train Loss: 0.1326601356267929, Memory (GB): 7\n",
      "Epoch 19, Batch 112, Train Loss: 0.12655825912952423, Memory (GB): 7\n",
      "Epoch 19, Batch 113, Train Loss: 0.13469301164150238, Memory (GB): 7\n",
      "Epoch 19, Batch 114, Train Loss: 0.13781994581222534, Memory (GB): 7\n",
      "Epoch 19, Batch 115, Train Loss: 0.14071786403656006, Memory (GB): 7\n",
      "Epoch 19, Batch 116, Train Loss: 0.12978844344615936, Memory (GB): 7\n",
      "Epoch 19, Batch 117, Train Loss: 0.1287757158279419, Memory (GB): 7\n",
      "Epoch 19, Batch 118, Train Loss: 0.12490207701921463, Memory (GB): 7\n",
      "Epoch 19, Batch 119, Train Loss: 0.14448301494121552, Memory (GB): 7\n",
      "Epoch 19, Batch 120, Train Loss: 0.1390717774629593, Memory (GB): 7\n",
      "Epoch 19, Batch 121, Train Loss: 0.12645462155342102, Memory (GB): 7\n",
      "Epoch 19, Batch 122, Train Loss: 0.13000211119651794, Memory (GB): 7\n",
      "Epoch 19, Batch 123, Train Loss: 0.12994477152824402, Memory (GB): 7\n",
      "Epoch 19, Batch 124, Train Loss: 0.12382089346647263, Memory (GB): 7\n",
      "Epoch 19, Batch 125, Train Loss: 0.1405739039182663, Memory (GB): 7\n",
      "Epoch 19, Batch 126, Train Loss: 0.12400566041469574, Memory (GB): 7\n",
      "Epoch 19, Batch 127, Train Loss: 0.14395591616630554, Memory (GB): 7\n",
      "Epoch 19, Batch 128, Train Loss: 0.12847959995269775, Memory (GB): 7\n",
      "Epoch 19, Batch 129, Train Loss: 0.14238716661930084, Memory (GB): 7\n",
      "Epoch 19, Batch 130, Train Loss: 0.131799578666687, Memory (GB): 7\n",
      "Epoch 19, Batch 131, Train Loss: 0.11896083503961563, Memory (GB): 7\n",
      "Epoch 19, Batch 132, Train Loss: 0.14074328541755676, Memory (GB): 7\n",
      "Epoch 19, Batch 133, Train Loss: 0.12752480804920197, Memory (GB): 7\n",
      "Epoch 19, Batch 134, Train Loss: 0.13052667677402496, Memory (GB): 7\n",
      "Epoch 19, Batch 135, Train Loss: 0.13714268803596497, Memory (GB): 7\n",
      "Epoch 19, Batch 136, Train Loss: 0.11248942464590073, Memory (GB): 7\n",
      "Epoch 19, Batch 137, Train Loss: 0.13593938946723938, Memory (GB): 7\n",
      "Epoch 19, Batch 138, Train Loss: 0.13418281078338623, Memory (GB): 7\n",
      "Epoch 19, Batch 139, Train Loss: 0.13053233921527863, Memory (GB): 7\n",
      "Epoch 19, Batch 140, Train Loss: 0.14512372016906738, Memory (GB): 7\n",
      "Epoch 19, Batch 141, Train Loss: 0.1286858320236206, Memory (GB): 7\n",
      "Epoch 19, Batch 142, Train Loss: 0.11152619123458862, Memory (GB): 7\n",
      "Epoch 19, Batch 143, Train Loss: 0.12902094423770905, Memory (GB): 7\n",
      "Epoch 19, Batch 144, Train Loss: 0.1564669907093048, Memory (GB): 7\n",
      "Epoch 19, Batch 145, Train Loss: 0.14409101009368896, Memory (GB): 7\n",
      "Epoch 19, Batch 146, Train Loss: 0.12339980155229568, Memory (GB): 7\n",
      "Epoch 19, Batch 147, Train Loss: 0.10972408950328827, Memory (GB): 7\n",
      "Epoch 19, Batch 148, Train Loss: 0.11352679878473282, Memory (GB): 7\n",
      "Epoch 19, Batch 149, Train Loss: 0.17137902975082397, Memory (GB): 7\n",
      "Epoch 19, Batch 150, Train Loss: 0.1500445306301117, Memory (GB): 7\n",
      "Epoch 19, Batch 151, Train Loss: 0.12861168384552002, Memory (GB): 7\n",
      "Epoch 19, Batch 152, Train Loss: 0.13356022536754608, Memory (GB): 7\n",
      "Epoch 19, Batch 153, Train Loss: 0.13951879739761353, Memory (GB): 7\n",
      "Epoch 19, Batch 154, Train Loss: 0.12693224847316742, Memory (GB): 7\n",
      "Epoch 19, Batch 155, Train Loss: 0.1324205845594406, Memory (GB): 7\n",
      "Epoch 19, Batch 156, Train Loss: 0.1516852080821991, Memory (GB): 7\n",
      "Epoch 19, Batch 157, Train Loss: 0.14598773419857025, Memory (GB): 7\n",
      "Epoch 19, Batch 158, Train Loss: 0.13101109862327576, Memory (GB): 7\n",
      "Epoch 19, Batch 159, Train Loss: 0.13788564503192902, Memory (GB): 7\n",
      "Epoch 19, Batch 160, Train Loss: 0.13275517523288727, Memory (GB): 7\n",
      "Epoch 19, Batch 161, Train Loss: 0.11770588159561157, Memory (GB): 7\n",
      "Epoch 19, Batch 162, Train Loss: 0.1376533806324005, Memory (GB): 7\n",
      "Epoch 19, Batch 163, Train Loss: 0.1292811930179596, Memory (GB): 7\n",
      "Epoch 19, Batch 164, Train Loss: 0.12070484459400177, Memory (GB): 7\n",
      "Epoch 19, Batch 165, Train Loss: 0.1298903524875641, Memory (GB): 7\n",
      "Epoch 19, Batch 166, Train Loss: 0.13337597250938416, Memory (GB): 7\n",
      "Epoch 19, Batch 167, Train Loss: 0.14523212611675262, Memory (GB): 7\n",
      "Epoch 19, Batch 168, Train Loss: 0.1335289627313614, Memory (GB): 7\n",
      "Epoch 19, Batch 169, Train Loss: 0.13356514275074005, Memory (GB): 7\n",
      "Epoch 19, Batch 170, Train Loss: 0.14102321863174438, Memory (GB): 7\n",
      "Epoch 19, Batch 171, Train Loss: 0.13133695721626282, Memory (GB): 7\n",
      "Epoch 19, Batch 172, Train Loss: 0.12942716479301453, Memory (GB): 7\n",
      "Epoch 19, Batch 173, Train Loss: 0.1261216253042221, Memory (GB): 7\n",
      "Epoch 19, Batch 174, Train Loss: 0.1399426907300949, Memory (GB): 7\n",
      "Epoch 19, Batch 175, Train Loss: 0.13327285647392273, Memory (GB): 7\n",
      "Epoch 19, Batch 176, Train Loss: 0.1430177390575409, Memory (GB): 7\n",
      "Epoch 19, Batch 177, Train Loss: 0.12661130726337433, Memory (GB): 7\n",
      "Epoch 19, Batch 178, Train Loss: 0.14568740129470825, Memory (GB): 7\n",
      "Epoch 19, Batch 179, Train Loss: 0.1550702303647995, Memory (GB): 7\n",
      "[0.0010000000000009, 0.0009500000000008551, 0.0009025000000008128, 0.0008573750000007723, 0.0008145062500007333, 0.0007737809375006969, 0.0007350918906256618, 0.0006983372960943786, 0.0006634204312896594, 0.0006302494097251762, 0.0005987369392389176, 0.0005688000922769721, 0.0005403600876631231, 0.0005133420832799671, 0.00048767497911596886, 0.00046329123016017033, 0.000440126668652162, 0.0004181203352195535, 0.00039721431845857603, 0.00037735360253564717, 0.0003584859224088649, 0.00034056162628842126, 0.00032353354497400037, 0.00030735686772530014, 0.0002919890243390353, 0.00027738957312208344, 0.00026352009446597936, 0.00025034408974268014, 0.00023782688525554623, 0.00022593554099276894, 0.00021463876394313042, 0.0002039068257459739, 0.00019371148445867535, 0.00018402591023574148, 0.00017482461472395438, 0.00016608338398775655, 0.00015777921478836869, 0.00014989025404895035, 0.00014239574134650276, 0.00013527595427917767, 0.00012851215656521875, 0.00012208654873695795, 0.00011598222130011, 0.0001101831102351044, 0.00010467395472334924, 9.944025698718172e-05, 9.446824413782265e-05, 8.974483193093149e-05, 8.525759033438493e-05, 8.099471081766565e-05, 7.69449752767824e-05, 7.309772651294328e-05, 6.944284018729611e-05, 6.597069817793126e-05, 6.267216326903477e-05, 5.953855510558298e-05, 5.656162735030383e-05, 5.3733545982788645e-05, 5.104686868364924e-05, 4.849452524946674e-05, 4.606979898699339e-05, 4.376630903764376e-05, 4.1577993585761537e-05, 3.949909390647345e-05, 3.752413921114981e-05, 3.5647932250592294e-05, 3.386553563806267e-05, 3.2172258856159546e-05, 3.056364591335156e-05, 2.9035463617683997e-05, 2.7583690436799767e-05, 2.620450591495979e-05, 2.4894280619211803e-05, 2.36495665882512e-05, 2.2467088258838657e-05, 2.1343733845896707e-05, 2.0276547153601876e-05, 1.9262719795921776e-05, 1.829958380612569e-05, 1.738460461581941e-05, 1.6515374385028442e-05, 1.5689605665777007e-05, 1.4905125382488165e-05, 1.4159869113363759e-05, 1.3451875657695563e-05, 1.2779281874810789e-05, 1.2140317781070246e-05, 1.153330189201673e-05, 1.0956636797415896e-05, 1.0408804957545104e-05, 9.888364709667847e-06, 9.393946474184459e-06, 8.924249150475226e-06, 8.478036692951472e-06, 8.0541348583039e-06, 7.651428115388699e-06, 7.268856709619266e-06, 6.9054138741383e-06, 6.560143180431388e-06, 6.232136021409817e-06, 5.920529220339326e-06, 5.624502759322361e-06, 5.34327762135624e-06, 5.0761137402884315e-06, 4.82230805327401e-06, 4.581192650610308e-06, 4.352133018079791e-06, 4.134526367175802e-06, 3.927800048817012e-06, 3.731410046376162e-06, 3.5448395440573532e-06, 3.3675975668544843e-06, 3.1992176885117604e-06, 3.0392568040861736e-06, 2.8872939638818635e-06, 2.7429292656877698e-06, 2.6057828024033827e-06, 2.4754936622832134e-06, 2.3517189791690506e-06, 2.234133030210599e-06, 2.1224263787000706e-06, 2.016305059765067e-06, 1.9154898067768124e-06, 1.819715316437971e-06, 1.7287295506160736e-06, 1.6422930730852688e-06, 1.5601784194310054e-06, 1.4821694984594549e-06, 1.4080610235364822e-06, 1.337657972359658e-06, 1.2707750737416748e-06, 1.2072363200545914e-06, 1.1468745040518613e-06, 1.0895307788492684e-06, 1.035054239906805e-06, 9.833015279114654e-07, 9.341364515158914e-07, 8.874296289400972e-07, 8.430581474930923e-07, 8.009052401184372e-07, 7.608599781125155e-07, 7.228169792068897e-07, 6.866761302465456e-07, 6.523423237342182e-07, 6.197252075475069e-07, 5.887389471701317e-07, 5.593019998116246e-07, 5.313368998210439e-07, 5.047700548299911e-07, 4.795315520884917e-07, 4.555549744840672e-07, 4.3277722575986397e-07, 4.111383644718707e-07, 3.9058144624827705e-07, 3.710523739358632e-07, 3.5249975523906994e-07, 3.348747674771165e-07, 3.181310291032606e-07, 3.022244776480975e-07, 2.871132537656927e-07, 2.727575910774079e-07, 2.5911971152353764e-07, 2.4616372594736076e-07, 2.3385553964999272e-07, 2.2216276266749302e-07, 2.1105462453411843e-07, 2.0050189330741238e-07, 1.90476798642042e-07, 1.8095295870993968e-07, 1.7190531077444277e-07, 1.633100452357206e-07, 1.551445429739346e-07, 1.4738731582523786e-07, 1.400179500339759e-07, 1.3301705253227713e-07, 1.2636619990566323e-07, 1.2004788991038012e-07, 1.1404549541486108e-07, 1.0834322064411808e-07, 1.0292605961191213e-07, 9.77797566313165e-08, 9.289076879975068e-08, 8.824623035976316e-08, 8.383391884177493e-08, 7.964222289968624e-08, 7.566011175470193e-08, 7.187710616696683e-08, 6.82832508586185e-08, 6.486908831568751e-08, 6.162563389990313e-08, 5.8544352204907975e-08, 5.56171345946626e-08, 5.283627786492948e-08, 5.019446397168299e-08, 4.768474077309883e-08, 4.5300503734443866e-08, 4.303547854772168e-08, 4.088370462033562e-08, 3.8839519389318813e-08]\n",
      "Epoch 20, Batch 0, Train Loss: 0.1408047378063202, Memory (GB): 7\n",
      "Epoch 20, Batch 1, Train Loss: 0.11976581811904907, Memory (GB): 7\n",
      "Epoch 20, Batch 2, Train Loss: 0.13702283799648285, Memory (GB): 7\n",
      "Epoch 20, Batch 3, Train Loss: 0.1255997270345688, Memory (GB): 7\n",
      "Epoch 20, Batch 4, Train Loss: 0.13754883408546448, Memory (GB): 7\n",
      "Epoch 20, Batch 5, Train Loss: 0.12084369361400604, Memory (GB): 7\n",
      "Epoch 20, Batch 6, Train Loss: 0.126607283949852, Memory (GB): 7\n",
      "Epoch 20, Batch 7, Train Loss: 0.12987764179706573, Memory (GB): 7\n",
      "Epoch 20, Batch 8, Train Loss: 0.12365024536848068, Memory (GB): 7\n",
      "Epoch 20, Batch 9, Train Loss: 0.11528538167476654, Memory (GB): 7\n",
      "Epoch 20, Batch 10, Train Loss: 0.12605664134025574, Memory (GB): 7\n",
      "Epoch 20, Batch 11, Train Loss: 0.12192385643720627, Memory (GB): 7\n",
      "Epoch 20, Batch 12, Train Loss: 0.12706908583641052, Memory (GB): 7\n",
      "Epoch 20, Batch 13, Train Loss: 0.1287303864955902, Memory (GB): 7\n",
      "Epoch 20, Batch 14, Train Loss: 0.12298683822154999, Memory (GB): 7\n",
      "Epoch 20, Batch 15, Train Loss: 0.1186835914850235, Memory (GB): 7\n",
      "Epoch 20, Batch 16, Train Loss: 0.13023154437541962, Memory (GB): 7\n",
      "Epoch 20, Batch 17, Train Loss: 0.13771016895771027, Memory (GB): 7\n",
      "Epoch 20, Batch 18, Train Loss: 0.1292150318622589, Memory (GB): 7\n",
      "Epoch 20, Batch 19, Train Loss: 0.12479090690612793, Memory (GB): 7\n",
      "Epoch 20, Batch 20, Train Loss: 0.1258988380432129, Memory (GB): 7\n",
      "Epoch 20, Batch 21, Train Loss: 0.13464505970478058, Memory (GB): 7\n",
      "Epoch 20, Batch 22, Train Loss: 0.11659654229879379, Memory (GB): 7\n",
      "Epoch 20, Batch 23, Train Loss: 0.12483056634664536, Memory (GB): 7\n",
      "Epoch 20, Batch 24, Train Loss: 0.12913790345191956, Memory (GB): 7\n",
      "Epoch 20, Batch 25, Train Loss: 0.1156649962067604, Memory (GB): 7\n",
      "Epoch 20, Batch 26, Train Loss: 0.12022965401411057, Memory (GB): 7\n",
      "Epoch 20, Batch 27, Train Loss: 0.12302248924970627, Memory (GB): 7\n",
      "Epoch 20, Batch 28, Train Loss: 0.13497275114059448, Memory (GB): 7\n",
      "Epoch 20, Batch 29, Train Loss: 0.11905203759670258, Memory (GB): 7\n",
      "Epoch 20, Batch 30, Train Loss: 0.12726210057735443, Memory (GB): 7\n",
      "Epoch 20, Batch 31, Train Loss: 0.11249536275863647, Memory (GB): 7\n",
      "Epoch 20, Batch 32, Train Loss: 0.1240491196513176, Memory (GB): 7\n",
      "Epoch 20, Batch 33, Train Loss: 0.14028142392635345, Memory (GB): 7\n",
      "Epoch 20, Batch 34, Train Loss: 0.11432460695505142, Memory (GB): 7\n",
      "Epoch 20, Batch 35, Train Loss: 0.13620978593826294, Memory (GB): 7\n",
      "Epoch 20, Batch 36, Train Loss: 0.12224968522787094, Memory (GB): 7\n",
      "Epoch 20, Batch 37, Train Loss: 0.11594733595848083, Memory (GB): 7\n",
      "Epoch 20, Batch 38, Train Loss: 0.12241663038730621, Memory (GB): 7\n",
      "Epoch 20, Batch 39, Train Loss: 0.12545877695083618, Memory (GB): 7\n",
      "Epoch 20, Batch 40, Train Loss: 0.12084479629993439, Memory (GB): 7\n",
      "Epoch 20, Batch 41, Train Loss: 0.12077727913856506, Memory (GB): 7\n",
      "Epoch 20, Batch 42, Train Loss: 0.10817454755306244, Memory (GB): 7\n",
      "Epoch 20, Batch 43, Train Loss: 0.10816887766122818, Memory (GB): 7\n",
      "Epoch 20, Batch 44, Train Loss: 0.11712212860584259, Memory (GB): 7\n",
      "Epoch 20, Batch 45, Train Loss: 0.13437983393669128, Memory (GB): 7\n",
      "Epoch 20, Batch 46, Train Loss: 0.11378008872270584, Memory (GB): 7\n",
      "Epoch 20, Batch 47, Train Loss: 0.12877033650875092, Memory (GB): 7\n",
      "Epoch 20, Batch 48, Train Loss: 0.11090783029794693, Memory (GB): 7\n",
      "Epoch 20, Batch 49, Train Loss: 0.11690818518400192, Memory (GB): 7\n",
      "Epoch 20, Batch 50, Train Loss: 0.11283740401268005, Memory (GB): 7\n",
      "Epoch 20, Batch 51, Train Loss: 0.12362296134233475, Memory (GB): 7\n",
      "Epoch 20, Batch 52, Train Loss: 0.10698791593313217, Memory (GB): 7\n",
      "Epoch 20, Batch 53, Train Loss: 0.14829175174236298, Memory (GB): 7\n",
      "Epoch 20, Batch 54, Train Loss: 0.10236376523971558, Memory (GB): 7\n",
      "Epoch 20, Batch 55, Train Loss: 0.10045554488897324, Memory (GB): 7\n",
      "Epoch 20, Batch 56, Train Loss: 0.11241143941879272, Memory (GB): 7\n",
      "Epoch 20, Batch 57, Train Loss: 0.11503015458583832, Memory (GB): 7\n",
      "Epoch 20, Batch 58, Train Loss: 0.12742577493190765, Memory (GB): 7\n",
      "Epoch 20, Batch 59, Train Loss: 0.12454412132501602, Memory (GB): 7\n",
      "Epoch 20, Batch 60, Train Loss: 0.11987999826669693, Memory (GB): 7\n",
      "Epoch 20, Batch 61, Train Loss: 0.12128353118896484, Memory (GB): 7\n",
      "Epoch 20, Batch 62, Train Loss: 0.11687617003917694, Memory (GB): 7\n",
      "Epoch 20, Batch 63, Train Loss: 0.11074694991111755, Memory (GB): 7\n",
      "Epoch 20, Batch 64, Train Loss: 0.11701706796884537, Memory (GB): 7\n",
      "Epoch 20, Batch 65, Train Loss: 0.11077339202165604, Memory (GB): 7\n",
      "Epoch 20, Batch 66, Train Loss: 0.11943483352661133, Memory (GB): 7\n",
      "Epoch 20, Batch 67, Train Loss: 0.10862217843532562, Memory (GB): 7\n",
      "Epoch 20, Batch 68, Train Loss: 0.11660086363554001, Memory (GB): 7\n",
      "Epoch 20, Batch 69, Train Loss: 0.1079695001244545, Memory (GB): 7\n",
      "Epoch 20, Batch 70, Train Loss: 0.10183225572109222, Memory (GB): 7\n",
      "Epoch 20, Batch 71, Train Loss: 0.11705242842435837, Memory (GB): 7\n",
      "Epoch 20, Batch 72, Train Loss: 0.1290617734193802, Memory (GB): 7\n",
      "Epoch 20, Batch 73, Train Loss: 0.12172757834196091, Memory (GB): 7\n",
      "Epoch 20, Batch 74, Train Loss: 0.10495199263095856, Memory (GB): 7\n",
      "Epoch 20, Batch 75, Train Loss: 0.11948879808187485, Memory (GB): 7\n",
      "Epoch 20, Batch 76, Train Loss: 0.11964336782693863, Memory (GB): 7\n",
      "Epoch 20, Batch 77, Train Loss: 0.10702092200517654, Memory (GB): 7\n",
      "Epoch 20, Batch 78, Train Loss: 0.09538502991199493, Memory (GB): 7\n",
      "Epoch 20, Batch 79, Train Loss: 0.12723131477832794, Memory (GB): 7\n",
      "Epoch 20, Batch 80, Train Loss: 0.11051904410123825, Memory (GB): 7\n",
      "Epoch 20, Batch 81, Train Loss: 0.13389654457569122, Memory (GB): 7\n",
      "Epoch 20, Batch 82, Train Loss: 0.13034111261367798, Memory (GB): 7\n",
      "Epoch 20, Batch 83, Train Loss: 0.11821144819259644, Memory (GB): 7\n",
      "Epoch 20, Batch 84, Train Loss: 0.12877818942070007, Memory (GB): 7\n",
      "Epoch 20, Batch 85, Train Loss: 0.10994940996170044, Memory (GB): 7\n",
      "Epoch 20, Batch 86, Train Loss: 0.12486857175827026, Memory (GB): 7\n",
      "Epoch 20, Batch 87, Train Loss: 0.11534745246171951, Memory (GB): 7\n",
      "Epoch 20, Batch 88, Train Loss: 0.12674462795257568, Memory (GB): 7\n",
      "Epoch 20, Batch 89, Train Loss: 0.1136142760515213, Memory (GB): 7\n",
      "Epoch 20, Batch 90, Train Loss: 0.1252984255552292, Memory (GB): 7\n",
      "Epoch 20, Batch 91, Train Loss: 0.12770172953605652, Memory (GB): 7\n",
      "Epoch 20, Batch 92, Train Loss: 0.12781967222690582, Memory (GB): 7\n",
      "Epoch 20, Batch 93, Train Loss: 0.12062972038984299, Memory (GB): 7\n",
      "Epoch 20, Batch 94, Train Loss: 0.13274233043193817, Memory (GB): 7\n",
      "Epoch 20, Batch 95, Train Loss: 0.13255652785301208, Memory (GB): 7\n",
      "Epoch 20, Batch 96, Train Loss: 0.1448802649974823, Memory (GB): 7\n",
      "Epoch 20, Batch 97, Train Loss: 0.13740015029907227, Memory (GB): 7\n",
      "Epoch 20, Batch 98, Train Loss: 0.12935422360897064, Memory (GB): 7\n",
      "Epoch 20, Batch 99, Train Loss: 0.1173715591430664, Memory (GB): 7\n",
      "Epoch 20, Batch 100, Train Loss: 0.12162687629461288, Memory (GB): 7\n",
      "Epoch 20, Batch 101, Train Loss: 0.14234265685081482, Memory (GB): 7\n",
      "Epoch 20, Batch 102, Train Loss: 0.12178787589073181, Memory (GB): 7\n",
      "Epoch 20, Batch 103, Train Loss: 0.13241952657699585, Memory (GB): 7\n",
      "Epoch 20, Batch 104, Train Loss: 0.11659108847379684, Memory (GB): 7\n",
      "Epoch 20, Batch 105, Train Loss: 0.14698483049869537, Memory (GB): 7\n",
      "Epoch 20, Batch 106, Train Loss: 0.12320379912853241, Memory (GB): 7\n",
      "Epoch 20, Batch 107, Train Loss: 0.13059356808662415, Memory (GB): 7\n",
      "Epoch 20, Batch 108, Train Loss: 0.13410302996635437, Memory (GB): 7\n",
      "Epoch 20, Batch 109, Train Loss: 0.12449166923761368, Memory (GB): 7\n",
      "Epoch 20, Batch 110, Train Loss: 0.14217586815357208, Memory (GB): 7\n",
      "Epoch 20, Batch 111, Train Loss: 0.13124917447566986, Memory (GB): 7\n",
      "Epoch 20, Batch 112, Train Loss: 0.1618438959121704, Memory (GB): 7\n",
      "Epoch 20, Batch 113, Train Loss: 0.15407244861125946, Memory (GB): 7\n",
      "Epoch 20, Batch 114, Train Loss: 0.13068287074565887, Memory (GB): 7\n",
      "Epoch 20, Batch 115, Train Loss: 0.13176566362380981, Memory (GB): 7\n",
      "Epoch 20, Batch 116, Train Loss: 0.12700024247169495, Memory (GB): 7\n",
      "Epoch 20, Batch 117, Train Loss: 0.13775041699409485, Memory (GB): 7\n",
      "Epoch 20, Batch 118, Train Loss: 0.1377059668302536, Memory (GB): 7\n",
      "Epoch 20, Batch 119, Train Loss: 0.12904852628707886, Memory (GB): 7\n",
      "Epoch 20, Batch 120, Train Loss: 0.11190607398748398, Memory (GB): 7\n",
      "Epoch 20, Batch 121, Train Loss: 0.12145785242319107, Memory (GB): 7\n",
      "Epoch 20, Batch 122, Train Loss: 0.13199906051158905, Memory (GB): 7\n",
      "Epoch 20, Batch 123, Train Loss: 0.14378622174263, Memory (GB): 7\n",
      "Epoch 20, Batch 124, Train Loss: 0.11672767251729965, Memory (GB): 7\n",
      "Epoch 20, Batch 125, Train Loss: 0.14152976870536804, Memory (GB): 7\n",
      "Epoch 20, Batch 126, Train Loss: 0.13657322525978088, Memory (GB): 7\n",
      "Epoch 20, Batch 127, Train Loss: 0.13288307189941406, Memory (GB): 7\n",
      "Epoch 20, Batch 128, Train Loss: 0.12180891633033752, Memory (GB): 7\n",
      "Epoch 20, Batch 129, Train Loss: 0.13731631636619568, Memory (GB): 7\n",
      "Epoch 20, Batch 130, Train Loss: 0.13433386385440826, Memory (GB): 7\n",
      "Epoch 20, Batch 131, Train Loss: 0.11293093115091324, Memory (GB): 7\n",
      "Epoch 20, Batch 132, Train Loss: 0.11202548444271088, Memory (GB): 7\n",
      "Epoch 20, Batch 133, Train Loss: 0.1270594596862793, Memory (GB): 7\n",
      "Epoch 20, Batch 134, Train Loss: 0.11416834592819214, Memory (GB): 7\n",
      "Epoch 20, Batch 135, Train Loss: 0.13358072936534882, Memory (GB): 7\n",
      "Epoch 20, Batch 136, Train Loss: 0.14689791202545166, Memory (GB): 7\n",
      "Epoch 20, Batch 137, Train Loss: 0.13378676772117615, Memory (GB): 7\n",
      "Epoch 20, Batch 138, Train Loss: 0.13436122238636017, Memory (GB): 7\n",
      "Epoch 20, Batch 139, Train Loss: 0.11822135001420975, Memory (GB): 7\n",
      "Epoch 20, Batch 140, Train Loss: 0.11617972701787949, Memory (GB): 7\n",
      "Epoch 20, Batch 141, Train Loss: 0.1171579584479332, Memory (GB): 7\n",
      "Epoch 20, Batch 142, Train Loss: 0.11252647638320923, Memory (GB): 7\n",
      "Epoch 20, Batch 143, Train Loss: 0.11062274873256683, Memory (GB): 7\n",
      "Epoch 20, Batch 144, Train Loss: 0.11837204545736313, Memory (GB): 7\n",
      "Epoch 20, Batch 145, Train Loss: 0.124592624604702, Memory (GB): 7\n",
      "Epoch 20, Batch 146, Train Loss: 0.13517963886260986, Memory (GB): 7\n",
      "Epoch 20, Batch 147, Train Loss: 0.1299753487110138, Memory (GB): 7\n",
      "Epoch 20, Batch 148, Train Loss: 0.12089590728282928, Memory (GB): 7\n",
      "Epoch 20, Batch 149, Train Loss: 0.11905143409967422, Memory (GB): 7\n",
      "Epoch 20, Batch 150, Train Loss: 0.11514442414045334, Memory (GB): 7\n",
      "Epoch 20, Batch 151, Train Loss: 0.11731211096048355, Memory (GB): 7\n",
      "Epoch 20, Batch 152, Train Loss: 0.1224885806441307, Memory (GB): 7\n",
      "Epoch 20, Batch 153, Train Loss: 0.14093074202537537, Memory (GB): 7\n",
      "Epoch 20, Batch 154, Train Loss: 0.12002034485340118, Memory (GB): 7\n",
      "Epoch 20, Batch 155, Train Loss: 0.11576550453901291, Memory (GB): 7\n",
      "Epoch 20, Batch 156, Train Loss: 0.10872776061296463, Memory (GB): 7\n",
      "Epoch 20, Batch 157, Train Loss: 0.11670903116464615, Memory (GB): 7\n",
      "Epoch 20, Batch 158, Train Loss: 0.10917538404464722, Memory (GB): 7\n",
      "Epoch 20, Batch 159, Train Loss: 0.12276013940572739, Memory (GB): 7\n",
      "Epoch 20, Batch 160, Train Loss: 0.11358826607465744, Memory (GB): 7\n",
      "Epoch 20, Batch 161, Train Loss: 0.11787622421979904, Memory (GB): 7\n",
      "Epoch 20, Batch 162, Train Loss: 0.1131172850728035, Memory (GB): 7\n",
      "Epoch 20, Batch 163, Train Loss: 0.11028200387954712, Memory (GB): 7\n",
      "Epoch 20, Batch 164, Train Loss: 0.11428185552358627, Memory (GB): 7\n",
      "Epoch 20, Batch 165, Train Loss: 0.1162717342376709, Memory (GB): 7\n",
      "Epoch 20, Batch 166, Train Loss: 0.11507951468229294, Memory (GB): 7\n",
      "Epoch 20, Batch 167, Train Loss: 0.11714254319667816, Memory (GB): 7\n",
      "Epoch 20, Batch 168, Train Loss: 0.11766979843378067, Memory (GB): 7\n",
      "Epoch 20, Batch 169, Train Loss: 0.11494766175746918, Memory (GB): 7\n",
      "Epoch 20, Batch 170, Train Loss: 0.10846253484487534, Memory (GB): 7\n",
      "Epoch 20, Batch 171, Train Loss: 0.10224512964487076, Memory (GB): 7\n",
      "Epoch 20, Batch 172, Train Loss: 0.12245073914527893, Memory (GB): 7\n",
      "Epoch 20, Batch 173, Train Loss: 0.12231027334928513, Memory (GB): 7\n",
      "Epoch 20, Batch 174, Train Loss: 0.1126791387796402, Memory (GB): 7\n",
      "Epoch 20, Batch 175, Train Loss: 0.12806352972984314, Memory (GB): 7\n",
      "Epoch 20, Batch 176, Train Loss: 0.11524742841720581, Memory (GB): 7\n",
      "Epoch 20, Batch 177, Train Loss: 0.1145486831665039, Memory (GB): 7\n",
      "Epoch 20, Batch 178, Train Loss: 0.11207278817892075, Memory (GB): 7\n",
      "Epoch 20, Batch 179, Train Loss: 0.11875922977924347, Memory (GB): 7\n",
      "[0.0006545084971877651, 0.0006217830723283767, 0.0005906939187119577, 0.0005611592227763598, 0.0005331012616375419, 0.0005064461985556649, 0.0004811238886278814, 0.0004570676941964874, 0.000434214309486663, 0.0004125035940123299, 0.0003918784143117133, 0.00037228449359612764, 0.0003536702689163213, 0.0003359867554705051, 0.0003191874176969799, 0.0003032280468121309, 0.0002880666444715243, 0.0002736633122479481, 0.0002599801466355506, 0.00024698113930377307, 0.00023463208233858452, 0.00022290047822165525, 0.00021175545431057242, 0.0002011676815950438, 0.00019110929751529155, 0.0001815538326395269, 0.00017247614100755068, 0.0001638523339571731, 0.0001556597172593144, 0.00014787673139634872, 0.00014048289482653135, 0.0001334587500852047, 0.00012678581258094445, 0.00012044652195189724, 0.00011442419585430231, 0.00010870298606158722, 0.00010326783675850787, 9.810444492058245e-05, 9.319922267455333e-05, 8.853926154082562e-05, 8.411229846378438e-05, 7.990668354059518e-05, 7.591134936356537e-05, 7.211578189538706e-05, 6.850999280061774e-05, 6.508449316058686e-05, 6.183026850255748e-05, 5.873875507742963e-05, 5.580181732355813e-05, 5.301172645738024e-05, 5.036114013451122e-05, 4.784308312778565e-05, 4.545092897139638e-05, 4.317838252282656e-05, 4.101946339668523e-05, 3.896849022685097e-05, 3.702006571550841e-05, 3.516906242973297e-05, 3.341060930824635e-05, 3.174007884283402e-05, 3.0153074900692308e-05, 2.86454211556577e-05, 2.7213150097874805e-05, 2.585249259298107e-05, 2.4559867963332023e-05, 2.333187456516541e-05, 2.216528083690713e-05, 2.1057016795061796e-05, 2.0004165955308693e-05, 1.9003957657543253e-05, 1.805375977466609e-05, 1.7151071785932784e-05, 1.629351819663615e-05, 1.5478842286804333e-05, 1.4704900172464123e-05, 1.3969655163840908e-05, 1.3271172405648867e-05, 1.2607613785366428e-05, 1.1977233096098103e-05, 1.1378371441293191e-05, 1.0809452869228537e-05, 1.0268980225767113e-05, 9.755531214478754e-06, 9.267754653754815e-06, 8.804366921067076e-06, 8.364148575013722e-06, 7.945941146263035e-06, 7.548644088949884e-06, 7.171211884502388e-06, 6.8126512902772645e-06, 6.472018725763405e-06, 6.148417789475232e-06, 5.840996900001474e-06, 5.548947055001402e-06, 5.27149970225133e-06, 5.007924717138763e-06, 4.757528481281825e-06, 4.5196520572177325e-06, 4.293669454356847e-06, 4.0789859816390035e-06, 3.875036682557053e-06, 3.6812848484292012e-06, 3.4972206060077387e-06, 3.3223595757073527e-06, 3.1562415969219865e-06, 2.9984295170758863e-06, 2.848508041222091e-06, 2.7060826391609863e-06, 2.570778507202937e-06, 2.4422395818427897e-06, 2.320127602750651e-06, 2.2041212226131175e-06, 2.093915161482462e-06, 1.989219403408339e-06, 1.8897584332379212e-06, 1.795270511576026e-06, 1.7055069859972244e-06, 1.6202316366973634e-06, 1.5392200548624946e-06, 1.4622590521193703e-06, 1.389146099513401e-06, 1.3196887945377315e-06, 1.2537043548108452e-06, 1.1910191370703026e-06, 1.1314681802167871e-06, 1.0748947712059477e-06, 1.0211500326456505e-06, 9.700925310133674e-07, 9.21587904462699e-07, 8.755085092395643e-07, 8.317330837775858e-07, 7.901464295887068e-07, 7.506391081092715e-07, 7.13107152703808e-07, 6.774517950686173e-07, 6.435792053151866e-07, 6.114002450494274e-07, 5.808302327969557e-07, 5.517887211571079e-07, 5.241992850992527e-07, 4.979893208442898e-07, 4.7308985480207515e-07, 4.494353620619717e-07, 4.26963593958873e-07, 4.056154142609293e-07, 3.853346435478829e-07, 3.6606791137048876e-07, 3.477645158019642e-07, 3.3037629001186594e-07, 3.1385747551127267e-07, 2.981646017357091e-07, 2.8325637164892356e-07, 2.690935530664774e-07, 2.5563887541315337e-07, 2.4285693164249575e-07, 2.3071408506037103e-07, 2.1917838080735237e-07, 2.082194617669847e-07, 1.9780848867863556e-07, 1.8791806424470376e-07, 1.785221610324686e-07, 1.695960529808451e-07, 1.6111625033180285e-07, 1.530604378152126e-07, 1.454074159244521e-07, 1.3813704512822943e-07, 1.3123019287181795e-07, 1.2466868322822705e-07, 1.1843524906681562e-07, 1.1251348661347489e-07, 1.0688781228280105e-07, 1.0154342166866107e-07, 9.646625058522807e-08, 9.164293805596668e-08, 8.706079115316829e-08, 8.270775159550985e-08, 7.85723640157344e-08, 7.464374581494765e-08, 7.091155852420025e-08, 6.736598059799027e-08, 6.399768156809073e-08, 6.079779748968619e-08, 5.775790761520186e-08, 5.48700122344418e-08, 5.212651162271968e-08, 4.9520186041583715e-08, 4.704417673950451e-08, 4.4691967902529315e-08, 4.2457369507402834e-08, 4.033450103203267e-08, 3.831777598043103e-08, 3.640188718140948e-08, 3.4581792822339e-08, 3.285270318122207e-08, 3.121006802216095e-08, 2.9649564621052906e-08, 2.8167086390000258e-08, 2.675873207050026e-08, 2.542079546697524e-08]\n",
      "Epoch 21, Batch 0, Train Loss: 0.10943133383989334, Memory (GB): 7\n",
      "Epoch 21, Batch 1, Train Loss: 0.10218912363052368, Memory (GB): 7\n",
      "Epoch 21, Batch 2, Train Loss: 0.10767172276973724, Memory (GB): 7\n",
      "Epoch 21, Batch 3, Train Loss: 0.10555011034011841, Memory (GB): 7\n",
      "Epoch 21, Batch 4, Train Loss: 0.11380599439144135, Memory (GB): 7\n",
      "Epoch 21, Batch 5, Train Loss: 0.12259987741708755, Memory (GB): 7\n",
      "Epoch 21, Batch 6, Train Loss: 0.1038554310798645, Memory (GB): 7\n",
      "Epoch 21, Batch 7, Train Loss: 0.10502136498689651, Memory (GB): 7\n",
      "Epoch 21, Batch 8, Train Loss: 0.10622235387563705, Memory (GB): 7\n",
      "Epoch 21, Batch 9, Train Loss: 0.11438868939876556, Memory (GB): 7\n",
      "Epoch 21, Batch 10, Train Loss: 0.09932553023099899, Memory (GB): 7\n",
      "Epoch 21, Batch 11, Train Loss: 0.10641729086637497, Memory (GB): 7\n",
      "Epoch 21, Batch 12, Train Loss: 0.11035492271184921, Memory (GB): 7\n",
      "Epoch 21, Batch 13, Train Loss: 0.09548405557870865, Memory (GB): 7\n",
      "Epoch 21, Batch 14, Train Loss: 0.11167328804731369, Memory (GB): 7\n",
      "Epoch 21, Batch 15, Train Loss: 0.10351914167404175, Memory (GB): 7\n",
      "Epoch 21, Batch 16, Train Loss: 0.11456342041492462, Memory (GB): 7\n",
      "Epoch 21, Batch 17, Train Loss: 0.11782006174325943, Memory (GB): 7\n",
      "Epoch 21, Batch 18, Train Loss: 0.10698843002319336, Memory (GB): 7\n",
      "Epoch 21, Batch 19, Train Loss: 0.10658300668001175, Memory (GB): 7\n",
      "Epoch 21, Batch 20, Train Loss: 0.12822362780570984, Memory (GB): 7\n",
      "Epoch 21, Batch 21, Train Loss: 0.10238868743181229, Memory (GB): 7\n",
      "Epoch 21, Batch 22, Train Loss: 0.11761841177940369, Memory (GB): 7\n",
      "Epoch 21, Batch 23, Train Loss: 0.1256619691848755, Memory (GB): 7\n",
      "Epoch 21, Batch 24, Train Loss: 0.12490987777709961, Memory (GB): 7\n",
      "Epoch 21, Batch 25, Train Loss: 0.12198740988969803, Memory (GB): 7\n",
      "Epoch 21, Batch 26, Train Loss: 0.11819928139448166, Memory (GB): 7\n",
      "Epoch 21, Batch 27, Train Loss: 0.11920474469661713, Memory (GB): 7\n",
      "Epoch 21, Batch 28, Train Loss: 0.10831522941589355, Memory (GB): 7\n",
      "Epoch 21, Batch 29, Train Loss: 0.11372920125722885, Memory (GB): 7\n",
      "Epoch 21, Batch 30, Train Loss: 0.10079354047775269, Memory (GB): 7\n",
      "Epoch 21, Batch 31, Train Loss: 0.12115029245615005, Memory (GB): 7\n",
      "Epoch 21, Batch 32, Train Loss: 0.11093146353960037, Memory (GB): 7\n",
      "Epoch 21, Batch 33, Train Loss: 0.12178193777799606, Memory (GB): 7\n",
      "Epoch 21, Batch 34, Train Loss: 0.12240730971097946, Memory (GB): 7\n",
      "Epoch 21, Batch 35, Train Loss: 0.11896064132452011, Memory (GB): 7\n",
      "Epoch 21, Batch 36, Train Loss: 0.13209453225135803, Memory (GB): 7\n",
      "Epoch 21, Batch 37, Train Loss: 0.10367504507303238, Memory (GB): 7\n",
      "Epoch 21, Batch 38, Train Loss: 0.12212365865707397, Memory (GB): 7\n",
      "Epoch 21, Batch 39, Train Loss: 0.12013151496648788, Memory (GB): 7\n",
      "Epoch 21, Batch 40, Train Loss: 0.1292722076177597, Memory (GB): 7\n",
      "Epoch 21, Batch 41, Train Loss: 0.10709812492132187, Memory (GB): 7\n",
      "Epoch 21, Batch 42, Train Loss: 0.12585803866386414, Memory (GB): 7\n",
      "Epoch 21, Batch 43, Train Loss: 0.11105266958475113, Memory (GB): 7\n",
      "Epoch 21, Batch 44, Train Loss: 0.10699811577796936, Memory (GB): 7\n",
      "Epoch 21, Batch 45, Train Loss: 0.10806931555271149, Memory (GB): 7\n",
      "Epoch 21, Batch 46, Train Loss: 0.09977386146783829, Memory (GB): 7\n",
      "Epoch 21, Batch 47, Train Loss: 0.11083260178565979, Memory (GB): 7\n",
      "Epoch 21, Batch 48, Train Loss: 0.12229230254888535, Memory (GB): 7\n",
      "Epoch 21, Batch 49, Train Loss: 0.09778811037540436, Memory (GB): 7\n",
      "Epoch 21, Batch 50, Train Loss: 0.10470537096261978, Memory (GB): 7\n",
      "Epoch 21, Batch 51, Train Loss: 0.10740602761507034, Memory (GB): 7\n",
      "Epoch 21, Batch 52, Train Loss: 0.11468690633773804, Memory (GB): 7\n",
      "Epoch 21, Batch 53, Train Loss: 0.112550750374794, Memory (GB): 7\n",
      "Epoch 21, Batch 54, Train Loss: 0.10080184042453766, Memory (GB): 7\n",
      "Epoch 21, Batch 55, Train Loss: 0.11079304665327072, Memory (GB): 7\n",
      "Epoch 21, Batch 56, Train Loss: 0.11696776002645493, Memory (GB): 7\n",
      "Epoch 21, Batch 57, Train Loss: 0.12328032404184341, Memory (GB): 7\n",
      "Epoch 21, Batch 58, Train Loss: 0.11511100083589554, Memory (GB): 7\n",
      "Epoch 21, Batch 59, Train Loss: 0.10840066522359848, Memory (GB): 7\n",
      "Epoch 21, Batch 60, Train Loss: 0.10722502321004868, Memory (GB): 7\n",
      "Epoch 21, Batch 61, Train Loss: 0.11201217025518417, Memory (GB): 7\n",
      "Epoch 21, Batch 62, Train Loss: 0.12403303384780884, Memory (GB): 7\n",
      "Epoch 21, Batch 63, Train Loss: 0.11885318160057068, Memory (GB): 7\n",
      "Epoch 21, Batch 64, Train Loss: 0.09462874382734299, Memory (GB): 7\n",
      "Epoch 21, Batch 65, Train Loss: 0.10096096992492676, Memory (GB): 7\n",
      "Epoch 21, Batch 66, Train Loss: 0.1161174327135086, Memory (GB): 7\n",
      "Epoch 21, Batch 67, Train Loss: 0.11029042303562164, Memory (GB): 7\n",
      "Epoch 21, Batch 68, Train Loss: 0.10977692902088165, Memory (GB): 7\n",
      "Epoch 21, Batch 69, Train Loss: 0.10638746619224548, Memory (GB): 7\n",
      "Epoch 21, Batch 70, Train Loss: 0.13195377588272095, Memory (GB): 7\n",
      "Epoch 21, Batch 71, Train Loss: 0.12473267316818237, Memory (GB): 7\n",
      "Epoch 21, Batch 72, Train Loss: 0.11320141702890396, Memory (GB): 7\n",
      "Epoch 21, Batch 73, Train Loss: 0.10483662039041519, Memory (GB): 7\n",
      "Epoch 21, Batch 74, Train Loss: 0.09814009070396423, Memory (GB): 7\n",
      "Epoch 21, Batch 75, Train Loss: 0.11831101775169373, Memory (GB): 7\n",
      "Epoch 21, Batch 76, Train Loss: 0.10250980406999588, Memory (GB): 7\n",
      "Epoch 21, Batch 77, Train Loss: 0.1212894544005394, Memory (GB): 7\n",
      "Epoch 21, Batch 78, Train Loss: 0.09080301970243454, Memory (GB): 7\n",
      "Epoch 21, Batch 79, Train Loss: 0.0970771536231041, Memory (GB): 7\n",
      "Epoch 21, Batch 80, Train Loss: 0.10567056387662888, Memory (GB): 7\n",
      "Epoch 21, Batch 81, Train Loss: 0.12497206032276154, Memory (GB): 7\n",
      "Epoch 21, Batch 82, Train Loss: 0.115406833589077, Memory (GB): 7\n",
      "Epoch 21, Batch 83, Train Loss: 0.11432311683893204, Memory (GB): 7\n",
      "Epoch 21, Batch 84, Train Loss: 0.09863131493330002, Memory (GB): 7\n",
      "Epoch 21, Batch 85, Train Loss: 0.11264092475175858, Memory (GB): 7\n",
      "Epoch 21, Batch 86, Train Loss: 0.1123262345790863, Memory (GB): 7\n",
      "Epoch 21, Batch 87, Train Loss: 0.0939396470785141, Memory (GB): 7\n",
      "Epoch 21, Batch 88, Train Loss: 0.09736447036266327, Memory (GB): 7\n",
      "Epoch 21, Batch 89, Train Loss: 0.10861679166555405, Memory (GB): 7\n",
      "Epoch 21, Batch 90, Train Loss: 0.1025281772017479, Memory (GB): 7\n",
      "Epoch 21, Batch 91, Train Loss: 0.11285413801670074, Memory (GB): 7\n",
      "Epoch 21, Batch 92, Train Loss: 0.11085464060306549, Memory (GB): 7\n",
      "Epoch 21, Batch 93, Train Loss: 0.09181666374206543, Memory (GB): 7\n",
      "Epoch 21, Batch 94, Train Loss: 0.10721198469400406, Memory (GB): 7\n",
      "Epoch 21, Batch 95, Train Loss: 0.09551836550235748, Memory (GB): 7\n",
      "Epoch 21, Batch 96, Train Loss: 0.10899244993925095, Memory (GB): 7\n",
      "Epoch 21, Batch 97, Train Loss: 0.0948554202914238, Memory (GB): 7\n",
      "Epoch 21, Batch 98, Train Loss: 0.10395853966474533, Memory (GB): 7\n",
      "Epoch 21, Batch 99, Train Loss: 0.10711349546909332, Memory (GB): 7\n",
      "Epoch 21, Batch 100, Train Loss: 0.099916473031044, Memory (GB): 7\n",
      "Epoch 21, Batch 101, Train Loss: 0.11292269080877304, Memory (GB): 7\n",
      "Epoch 21, Batch 102, Train Loss: 0.10363356024026871, Memory (GB): 7\n",
      "Epoch 21, Batch 103, Train Loss: 0.08787931501865387, Memory (GB): 7\n",
      "Epoch 21, Batch 104, Train Loss: 0.1156490221619606, Memory (GB): 7\n",
      "Epoch 21, Batch 105, Train Loss: 0.09861553460359573, Memory (GB): 7\n",
      "Epoch 21, Batch 106, Train Loss: 0.11056309938430786, Memory (GB): 7\n",
      "Epoch 21, Batch 107, Train Loss: 0.09368917346000671, Memory (GB): 7\n",
      "Epoch 21, Batch 108, Train Loss: 0.11057551950216293, Memory (GB): 7\n",
      "Epoch 21, Batch 109, Train Loss: 0.11017482727766037, Memory (GB): 7\n",
      "Epoch 21, Batch 110, Train Loss: 0.10197574645280838, Memory (GB): 7\n",
      "Epoch 21, Batch 111, Train Loss: 0.11041554808616638, Memory (GB): 7\n",
      "Epoch 21, Batch 112, Train Loss: 0.14049923419952393, Memory (GB): 7\n",
      "Epoch 21, Batch 113, Train Loss: 0.1114775538444519, Memory (GB): 7\n",
      "Epoch 21, Batch 114, Train Loss: 0.10839139670133591, Memory (GB): 7\n",
      "Epoch 21, Batch 115, Train Loss: 0.12310628592967987, Memory (GB): 7\n",
      "Epoch 21, Batch 116, Train Loss: 0.12894606590270996, Memory (GB): 7\n",
      "Epoch 21, Batch 117, Train Loss: 0.11341298371553421, Memory (GB): 7\n",
      "Epoch 21, Batch 118, Train Loss: 0.12879657745361328, Memory (GB): 7\n",
      "Epoch 21, Batch 119, Train Loss: 0.11837297677993774, Memory (GB): 7\n",
      "Epoch 21, Batch 120, Train Loss: 0.11679483950138092, Memory (GB): 7\n",
      "Epoch 21, Batch 121, Train Loss: 0.1306626945734024, Memory (GB): 7\n",
      "Epoch 21, Batch 122, Train Loss: 0.11101286858320236, Memory (GB): 7\n",
      "Epoch 21, Batch 123, Train Loss: 0.13219277560710907, Memory (GB): 7\n",
      "Epoch 21, Batch 124, Train Loss: 0.12565597891807556, Memory (GB): 7\n",
      "Epoch 21, Batch 125, Train Loss: 0.11868075281381607, Memory (GB): 7\n",
      "Epoch 21, Batch 126, Train Loss: 0.13924212753772736, Memory (GB): 7\n",
      "Epoch 21, Batch 127, Train Loss: 0.11981069296598434, Memory (GB): 7\n",
      "Epoch 21, Batch 128, Train Loss: 0.11076339334249496, Memory (GB): 7\n",
      "Epoch 21, Batch 129, Train Loss: 0.12960073351860046, Memory (GB): 7\n",
      "Epoch 21, Batch 130, Train Loss: 0.11356379836797714, Memory (GB): 7\n",
      "Epoch 21, Batch 131, Train Loss: 0.12271197885274887, Memory (GB): 7\n",
      "Epoch 21, Batch 132, Train Loss: 0.14622806012630463, Memory (GB): 7\n",
      "Epoch 21, Batch 133, Train Loss: 0.11759322136640549, Memory (GB): 7\n",
      "Epoch 21, Batch 134, Train Loss: 0.12234847992658615, Memory (GB): 7\n",
      "Epoch 21, Batch 135, Train Loss: 0.13333432376384735, Memory (GB): 7\n",
      "Epoch 21, Batch 136, Train Loss: 0.12660615146160126, Memory (GB): 7\n",
      "Epoch 21, Batch 137, Train Loss: 0.12227226793766022, Memory (GB): 7\n",
      "Epoch 21, Batch 138, Train Loss: 0.11312495917081833, Memory (GB): 7\n",
      "Epoch 21, Batch 139, Train Loss: 0.1253819465637207, Memory (GB): 7\n",
      "Epoch 21, Batch 140, Train Loss: 0.12111123651266098, Memory (GB): 7\n",
      "Epoch 21, Batch 141, Train Loss: 0.10761712491512299, Memory (GB): 7\n",
      "Epoch 21, Batch 142, Train Loss: 0.1324096918106079, Memory (GB): 7\n",
      "Epoch 21, Batch 143, Train Loss: 0.12350738793611526, Memory (GB): 7\n",
      "Epoch 21, Batch 144, Train Loss: 0.13159425556659698, Memory (GB): 7\n",
      "Epoch 21, Batch 145, Train Loss: 0.12975187599658966, Memory (GB): 7\n",
      "Epoch 21, Batch 146, Train Loss: 0.12015806138515472, Memory (GB): 7\n",
      "Epoch 21, Batch 147, Train Loss: 0.11154662817716599, Memory (GB): 7\n",
      "Epoch 21, Batch 148, Train Loss: 0.11405511200428009, Memory (GB): 7\n",
      "Epoch 21, Batch 149, Train Loss: 0.10851164907217026, Memory (GB): 7\n",
      "Epoch 21, Batch 150, Train Loss: 0.09881137311458588, Memory (GB): 7\n",
      "Epoch 21, Batch 151, Train Loss: 0.12439902871847153, Memory (GB): 7\n",
      "Epoch 21, Batch 152, Train Loss: 0.10352776944637299, Memory (GB): 7\n",
      "Epoch 21, Batch 153, Train Loss: 0.12052646279335022, Memory (GB): 7\n",
      "Epoch 21, Batch 154, Train Loss: 0.11091922223567963, Memory (GB): 7\n",
      "Epoch 21, Batch 155, Train Loss: 0.11881090700626373, Memory (GB): 7\n",
      "Epoch 21, Batch 156, Train Loss: 0.11519791185855865, Memory (GB): 7\n",
      "Epoch 21, Batch 157, Train Loss: 0.11361104995012283, Memory (GB): 7\n",
      "Epoch 21, Batch 158, Train Loss: 0.1067650318145752, Memory (GB): 7\n",
      "Epoch 21, Batch 159, Train Loss: 0.10583759844303131, Memory (GB): 7\n",
      "Epoch 21, Batch 160, Train Loss: 0.1168387308716774, Memory (GB): 7\n",
      "Epoch 21, Batch 161, Train Loss: 0.10271394997835159, Memory (GB): 7\n",
      "Epoch 21, Batch 162, Train Loss: 0.11565357446670532, Memory (GB): 7\n",
      "Epoch 21, Batch 163, Train Loss: 0.10193781554698944, Memory (GB): 7\n",
      "Epoch 21, Batch 164, Train Loss: 0.10540080070495605, Memory (GB): 7\n",
      "Epoch 21, Batch 165, Train Loss: 0.11133148521184921, Memory (GB): 7\n",
      "Epoch 21, Batch 166, Train Loss: 0.11188040673732758, Memory (GB): 7\n",
      "Epoch 21, Batch 167, Train Loss: 0.11876528710126877, Memory (GB): 7\n",
      "Epoch 21, Batch 168, Train Loss: 0.09683788567781448, Memory (GB): 7\n",
      "Epoch 21, Batch 169, Train Loss: 0.10191354155540466, Memory (GB): 7\n",
      "Epoch 21, Batch 170, Train Loss: 0.10451857000589371, Memory (GB): 7\n",
      "Epoch 21, Batch 171, Train Loss: 0.10438190400600433, Memory (GB): 7\n",
      "Epoch 21, Batch 172, Train Loss: 0.12821659445762634, Memory (GB): 7\n",
      "Epoch 21, Batch 173, Train Loss: 0.10614007711410522, Memory (GB): 7\n",
      "Epoch 21, Batch 174, Train Loss: 0.11447141319513321, Memory (GB): 7\n",
      "Epoch 21, Batch 175, Train Loss: 0.10410661995410919, Memory (GB): 7\n",
      "Epoch 21, Batch 176, Train Loss: 0.10566416382789612, Memory (GB): 7\n",
      "Epoch 21, Batch 177, Train Loss: 0.09838297218084335, Memory (GB): 7\n",
      "Epoch 21, Batch 178, Train Loss: 0.10022802650928497, Memory (GB): 7\n",
      "Epoch 21, Batch 179, Train Loss: 0.11022817343473434, Memory (GB): 7\n",
      "[9.549150281252618e-05, 9.071692767189982e-05, 8.618108128830484e-05, 8.18720272238896e-05, 7.77784258626951e-05, 7.388950456956034e-05, 7.019502934108233e-05, 6.668527787402819e-05, 6.335101398032678e-05, 6.018346328131044e-05, 5.7174290117244917e-05, 5.431557561138268e-05, 5.1599796830813516e-05, 4.901980698927284e-05, 4.6568816639809214e-05, 4.424037580781875e-05, 4.2028357017427806e-05, 3.992693916655642e-05, 3.7930592208228593e-05, 3.603406259781716e-05, 3.423235946792629e-05, 3.2520741494529975e-05, 3.089470441980348e-05, 2.934996919881331e-05, 2.7882470738872634e-05, 2.6488347201929003e-05, 2.5163929841832552e-05, 2.3905733349740918e-05, 2.271044668225387e-05, 2.1574924348141178e-05, 2.0496178130734123e-05, 1.9471369224197414e-05, 1.849780076298754e-05, 1.757291072483816e-05, 1.6694265188596254e-05, 1.585955192916644e-05, 1.5066574332708117e-05, 1.4313245616072711e-05, 1.3597583335269073e-05, 1.291770416850562e-05, 1.2271818960080339e-05, 1.1658228012076321e-05, 1.1075316611472501e-05, 1.0521550780898876e-05, 9.995473241853931e-06, 9.495699579761235e-06, 9.020914600773173e-06, 8.569868870734513e-06, 8.14137542719779e-06, 7.734306655837897e-06, 7.347591323046005e-06, 6.980211756893701e-06, 6.63120116904902e-06, 6.299641110596565e-06, 5.984659055066738e-06, 5.685426102313402e-06, 5.401154797197731e-06, 5.131097057337845e-06, 4.87454220447095e-06, 4.6308150942474045e-06, 4.399274339535032e-06, 4.179310622558281e-06, 3.970345091430366e-06, 3.7718278368588467e-06, 3.583236445015906e-06, 3.4040746227651102e-06, 3.2338708916268536e-06, 3.0721773470455107e-06, 2.9185684796932342e-06, 2.7726400557085736e-06, 2.6340080529231453e-06, 2.502307650276987e-06, 2.377192267763138e-06, 2.2583326543749803e-06, 2.1454160216562314e-06, 2.0381452205734194e-06, 1.9362379595447484e-06, 1.8394260615675113e-06, 1.747454758489136e-06, 1.6600820205646792e-06, 1.5770779195364452e-06, 1.4982240235596225e-06, 1.4233128223816414e-06, 1.352147181262559e-06, 1.2845398221994316e-06, 1.22031283108946e-06, 1.159297189534987e-06, 1.1013323300582372e-06, 1.0462657135553257e-06, 9.93952427877559e-07, 9.442548064836811e-07, 8.97042066159497e-07, 8.521899628515221e-07, 8.095804647089462e-07, 7.691014414734986e-07, 7.306463693998238e-07, 6.941140509298326e-07, 6.59408348383341e-07, 6.264379309641739e-07, 5.951160344159652e-07, 5.653602326951669e-07, 5.370922210604084e-07, 5.102376100073881e-07, 4.847257295070187e-07, 4.6048944303166766e-07, 4.3746497088008436e-07, 4.1559172233608005e-07, 3.9481213621927607e-07, 3.750715294083123e-07, 3.5631795293789665e-07, 3.385020552910018e-07, 3.2157695252645163e-07, 3.054981049001291e-07, 2.902231996551226e-07, 2.7571203967236645e-07, 2.619264376887481e-07, 2.4883011580431076e-07, 2.363886100140951e-07, 2.245691795133904e-07, 2.133407205377209e-07, 2.0267368451083482e-07, 1.92540000285293e-07, 1.8291300027102838e-07, 1.7376735025747692e-07, 1.6507898274460309e-07, 1.5682503360737293e-07, 1.489837819270043e-07, 1.4153459283065404e-07, 1.3445786318912134e-07, 1.2773497002966526e-07, 1.2134822152818202e-07, 1.1528081045177291e-07, 1.0951676992918426e-07, 1.0404093143272506e-07, 9.883888486108878e-08, 9.389694061803433e-08, 8.920209358713261e-08, 8.474198890777601e-08, 8.050488946238716e-08, 7.647964498926782e-08, 7.265566273980443e-08, 6.902287960281422e-08, 6.557173562267349e-08, 6.229314884153981e-08, 5.9178491399462826e-08, 5.621956682948968e-08, 5.340858848801519e-08, 5.073815906361443e-08, 4.82012511104337e-08, 4.5791188554912006e-08, 4.350162912716641e-08, 4.1326547670808075e-08, 3.926022028726767e-08, 3.729720927290429e-08, 3.543234880925908e-08, 3.3660731368796123e-08, 3.1977694800356315e-08, 3.03788100603385e-08, 2.8859869557321566e-08, 2.7416876079455492e-08, 2.6046032275482717e-08, 2.4743730661708574e-08, 2.3506544128623144e-08, 2.2331216922191987e-08, 2.1214656076082377e-08, 2.0153923272278266e-08, 1.914622710866435e-08, 1.818891575323113e-08, 1.727946996556957e-08, 1.641549646729109e-08, 1.5594721643926535e-08, 1.4814985561730212e-08, 1.4074236283643703e-08, 1.337052446946152e-08, 1.2701998245988442e-08, 1.2066898333689019e-08, 1.1463553417004564e-08, 1.089037574615434e-08, 1.0345856958846621e-08, 9.828564110904292e-09, 9.337135905359073e-09, 8.870279110091118e-09, 8.426765154586565e-09, 8.005426896857233e-09, 7.605155552014373e-09, 7.224897774413653e-09, 6.86365288569297e-09, 6.5204702414083215e-09, 6.194446729337907e-09, 5.884724392871009e-09, 5.5904881732274574e-09, 5.310963764566085e-09, 5.045415576337781e-09, 4.79314479752089e-09, 4.5534875576448464e-09, 4.325813179762603e-09, 4.109522520774474e-09, 3.90404639473575e-09, 3.7088440749989626e-09]\n",
      "Epoch 22, Batch 0, Train Loss: 0.09522827714681625, Memory (GB): 7\n",
      "Epoch 22, Batch 1, Train Loss: 0.11359656602144241, Memory (GB): 7\n",
      "Epoch 22, Batch 2, Train Loss: 0.10285774618387222, Memory (GB): 7\n",
      "Epoch 22, Batch 3, Train Loss: 0.09257395565509796, Memory (GB): 7\n",
      "Epoch 22, Batch 4, Train Loss: 0.09266242384910583, Memory (GB): 7\n",
      "Epoch 22, Batch 5, Train Loss: 0.10831115394830704, Memory (GB): 7\n",
      "Epoch 22, Batch 6, Train Loss: 0.09024461358785629, Memory (GB): 7\n",
      "Epoch 22, Batch 7, Train Loss: 0.09499111771583557, Memory (GB): 7\n",
      "Epoch 22, Batch 8, Train Loss: 0.08980719745159149, Memory (GB): 7\n",
      "Epoch 22, Batch 9, Train Loss: 0.09360288828611374, Memory (GB): 7\n",
      "Epoch 22, Batch 10, Train Loss: 0.08621124178171158, Memory (GB): 7\n",
      "Epoch 22, Batch 11, Train Loss: 0.0900384709239006, Memory (GB): 7\n",
      "Epoch 22, Batch 12, Train Loss: 0.10126306861639023, Memory (GB): 7\n",
      "Epoch 22, Batch 13, Train Loss: 0.11043526232242584, Memory (GB): 7\n",
      "Epoch 22, Batch 14, Train Loss: 0.0900193303823471, Memory (GB): 7\n",
      "Epoch 22, Batch 15, Train Loss: 0.09909029304981232, Memory (GB): 7\n",
      "Epoch 22, Batch 16, Train Loss: 0.10784877091646194, Memory (GB): 7\n",
      "Epoch 22, Batch 17, Train Loss: 0.10646429657936096, Memory (GB): 7\n",
      "Epoch 22, Batch 18, Train Loss: 0.10213645547628403, Memory (GB): 7\n",
      "Epoch 22, Batch 19, Train Loss: 0.0958954393863678, Memory (GB): 7\n",
      "Epoch 22, Batch 20, Train Loss: 0.08946871757507324, Memory (GB): 7\n",
      "Epoch 22, Batch 21, Train Loss: 0.09799989312887192, Memory (GB): 7\n",
      "Epoch 22, Batch 22, Train Loss: 0.10881983488798141, Memory (GB): 7\n",
      "Epoch 22, Batch 23, Train Loss: 0.09639447182416916, Memory (GB): 7\n",
      "Epoch 22, Batch 24, Train Loss: 0.09804996103048325, Memory (GB): 7\n",
      "Epoch 22, Batch 25, Train Loss: 0.09670764207839966, Memory (GB): 7\n",
      "Epoch 22, Batch 26, Train Loss: 0.09725210815668106, Memory (GB): 7\n",
      "Epoch 22, Batch 27, Train Loss: 0.09401551634073257, Memory (GB): 7\n",
      "Epoch 22, Batch 28, Train Loss: 0.12454566359519958, Memory (GB): 7\n",
      "Epoch 22, Batch 29, Train Loss: 0.1124192401766777, Memory (GB): 7\n",
      "Epoch 22, Batch 30, Train Loss: 0.10555001348257065, Memory (GB): 7\n",
      "Epoch 22, Batch 31, Train Loss: 0.11114136129617691, Memory (GB): 7\n",
      "Epoch 22, Batch 32, Train Loss: 0.09998559951782227, Memory (GB): 7\n",
      "Epoch 22, Batch 33, Train Loss: 0.1163899227976799, Memory (GB): 7\n",
      "Epoch 22, Batch 34, Train Loss: 0.10878988355398178, Memory (GB): 7\n",
      "Epoch 22, Batch 35, Train Loss: 0.11166615784168243, Memory (GB): 7\n",
      "Epoch 22, Batch 36, Train Loss: 0.10386727750301361, Memory (GB): 7\n",
      "Epoch 22, Batch 37, Train Loss: 0.10181549936532974, Memory (GB): 7\n",
      "Epoch 22, Batch 38, Train Loss: 0.11454668641090393, Memory (GB): 7\n",
      "Epoch 22, Batch 39, Train Loss: 0.12276224792003632, Memory (GB): 7\n",
      "Epoch 22, Batch 40, Train Loss: 0.09618894010782242, Memory (GB): 7\n",
      "Epoch 22, Batch 41, Train Loss: 0.09688635915517807, Memory (GB): 7\n",
      "Epoch 22, Batch 42, Train Loss: 0.10482272505760193, Memory (GB): 7\n",
      "Epoch 22, Batch 43, Train Loss: 0.11510183662176132, Memory (GB): 7\n",
      "Epoch 22, Batch 44, Train Loss: 0.10584427416324615, Memory (GB): 7\n",
      "Epoch 22, Batch 45, Train Loss: 0.10513812303543091, Memory (GB): 7\n",
      "Epoch 22, Batch 46, Train Loss: 0.0978294238448143, Memory (GB): 7\n",
      "Epoch 22, Batch 47, Train Loss: 0.1257658451795578, Memory (GB): 7\n",
      "Epoch 22, Batch 48, Train Loss: 0.10377690941095352, Memory (GB): 7\n",
      "Epoch 22, Batch 49, Train Loss: 0.10263155400753021, Memory (GB): 7\n",
      "Epoch 22, Batch 50, Train Loss: 0.11819789558649063, Memory (GB): 7\n",
      "Epoch 22, Batch 51, Train Loss: 0.11648377776145935, Memory (GB): 7\n",
      "Epoch 22, Batch 52, Train Loss: 0.11167532205581665, Memory (GB): 7\n",
      "Epoch 22, Batch 53, Train Loss: 0.10012472420930862, Memory (GB): 7\n",
      "Epoch 22, Batch 54, Train Loss: 0.0962970107793808, Memory (GB): 7\n",
      "Epoch 22, Batch 55, Train Loss: 0.1069023460149765, Memory (GB): 7\n",
      "Epoch 22, Batch 56, Train Loss: 0.11875675618648529, Memory (GB): 7\n",
      "Epoch 22, Batch 57, Train Loss: 0.11589197814464569, Memory (GB): 7\n",
      "Epoch 22, Batch 58, Train Loss: 0.09269827604293823, Memory (GB): 7\n",
      "Epoch 22, Batch 59, Train Loss: 0.12123133987188339, Memory (GB): 7\n",
      "Epoch 22, Batch 60, Train Loss: 0.09931956231594086, Memory (GB): 7\n",
      "Epoch 22, Batch 61, Train Loss: 0.10474463552236557, Memory (GB): 7\n",
      "Epoch 22, Batch 62, Train Loss: 0.11053486168384552, Memory (GB): 7\n",
      "Epoch 22, Batch 63, Train Loss: 0.1066393256187439, Memory (GB): 7\n",
      "Epoch 22, Batch 64, Train Loss: 0.09531752020120621, Memory (GB): 7\n",
      "Epoch 22, Batch 65, Train Loss: 0.11391115188598633, Memory (GB): 7\n",
      "Epoch 22, Batch 66, Train Loss: 0.09537332504987717, Memory (GB): 7\n",
      "Epoch 22, Batch 67, Train Loss: 0.10691647231578827, Memory (GB): 7\n",
      "Epoch 22, Batch 68, Train Loss: 0.09412702918052673, Memory (GB): 7\n",
      "Epoch 22, Batch 69, Train Loss: 0.1034083142876625, Memory (GB): 7\n",
      "Epoch 22, Batch 70, Train Loss: 0.10113133490085602, Memory (GB): 7\n",
      "Epoch 22, Batch 71, Train Loss: 0.1106446385383606, Memory (GB): 7\n",
      "Epoch 22, Batch 72, Train Loss: 0.09307298064231873, Memory (GB): 7\n",
      "Epoch 22, Batch 73, Train Loss: 0.11879019439220428, Memory (GB): 7\n",
      "Epoch 22, Batch 74, Train Loss: 0.09865216165781021, Memory (GB): 7\n",
      "Epoch 22, Batch 75, Train Loss: 0.09334029257297516, Memory (GB): 7\n",
      "Epoch 22, Batch 76, Train Loss: 0.096805639564991, Memory (GB): 7\n",
      "Epoch 22, Batch 77, Train Loss: 0.10044849663972855, Memory (GB): 7\n",
      "Epoch 22, Batch 78, Train Loss: 0.09941010177135468, Memory (GB): 7\n",
      "Epoch 22, Batch 79, Train Loss: 0.10280756652355194, Memory (GB): 7\n",
      "Epoch 22, Batch 80, Train Loss: 0.10110145807266235, Memory (GB): 7\n",
      "Epoch 22, Batch 81, Train Loss: 0.09316039830446243, Memory (GB): 7\n",
      "Epoch 22, Batch 82, Train Loss: 0.09826783090829849, Memory (GB): 7\n",
      "Epoch 22, Batch 83, Train Loss: 0.09349185228347778, Memory (GB): 7\n",
      "Epoch 22, Batch 84, Train Loss: 0.09629937261343002, Memory (GB): 7\n",
      "Epoch 22, Batch 85, Train Loss: 0.11153323203325272, Memory (GB): 7\n",
      "Epoch 22, Batch 86, Train Loss: 0.09392371773719788, Memory (GB): 7\n",
      "Epoch 22, Batch 87, Train Loss: 0.09224562346935272, Memory (GB): 7\n",
      "Epoch 22, Batch 88, Train Loss: 0.09966006875038147, Memory (GB): 7\n",
      "Epoch 22, Batch 89, Train Loss: 0.10281608998775482, Memory (GB): 7\n",
      "Epoch 22, Batch 90, Train Loss: 0.09013497084379196, Memory (GB): 7\n",
      "Epoch 22, Batch 91, Train Loss: 0.09905440360307693, Memory (GB): 7\n",
      "Epoch 22, Batch 92, Train Loss: 0.10610261559486389, Memory (GB): 7\n",
      "Epoch 22, Batch 93, Train Loss: 0.10300324112176895, Memory (GB): 7\n",
      "Epoch 22, Batch 94, Train Loss: 0.10912910103797913, Memory (GB): 7\n",
      "Epoch 22, Batch 95, Train Loss: 0.10078848153352737, Memory (GB): 7\n",
      "Epoch 22, Batch 96, Train Loss: 0.11090551316738129, Memory (GB): 7\n",
      "Epoch 22, Batch 97, Train Loss: 0.09995442628860474, Memory (GB): 7\n",
      "Epoch 22, Batch 98, Train Loss: 0.10102096199989319, Memory (GB): 7\n",
      "Epoch 22, Batch 99, Train Loss: 0.09558141976594925, Memory (GB): 7\n",
      "Epoch 22, Batch 100, Train Loss: 0.09614387899637222, Memory (GB): 7\n",
      "Epoch 22, Batch 101, Train Loss: 0.08685111254453659, Memory (GB): 7\n",
      "Epoch 22, Batch 102, Train Loss: 0.08371578902006149, Memory (GB): 7\n",
      "Epoch 22, Batch 103, Train Loss: 0.08708465844392776, Memory (GB): 7\n",
      "Epoch 22, Batch 104, Train Loss: 0.0930122435092926, Memory (GB): 7\n",
      "Epoch 22, Batch 105, Train Loss: 0.10986505448818207, Memory (GB): 7\n",
      "Epoch 22, Batch 106, Train Loss: 0.09394308179616928, Memory (GB): 7\n",
      "Epoch 22, Batch 107, Train Loss: 0.09871319681406021, Memory (GB): 7\n",
      "Epoch 22, Batch 108, Train Loss: 0.10522501915693283, Memory (GB): 7\n",
      "Epoch 22, Batch 109, Train Loss: 0.12257177382707596, Memory (GB): 7\n",
      "Epoch 22, Batch 110, Train Loss: 0.0927373617887497, Memory (GB): 7\n",
      "Epoch 22, Batch 111, Train Loss: 0.09507175534963608, Memory (GB): 7\n",
      "Epoch 22, Batch 112, Train Loss: 0.10804596543312073, Memory (GB): 7\n",
      "Epoch 22, Batch 113, Train Loss: 0.094920314848423, Memory (GB): 7\n",
      "Epoch 22, Batch 114, Train Loss: 0.1001582145690918, Memory (GB): 7\n",
      "Epoch 22, Batch 115, Train Loss: 0.0957629531621933, Memory (GB): 7\n",
      "Epoch 22, Batch 116, Train Loss: 0.08701929450035095, Memory (GB): 7\n",
      "Epoch 22, Batch 117, Train Loss: 0.09430588781833649, Memory (GB): 7\n",
      "Epoch 22, Batch 118, Train Loss: 0.09757193922996521, Memory (GB): 7\n",
      "Epoch 22, Batch 119, Train Loss: 0.09641581773757935, Memory (GB): 7\n",
      "Epoch 22, Batch 120, Train Loss: 0.10227237641811371, Memory (GB): 7\n",
      "Epoch 22, Batch 121, Train Loss: 0.09324180334806442, Memory (GB): 7\n",
      "Epoch 22, Batch 122, Train Loss: 0.09037520736455917, Memory (GB): 7\n",
      "Epoch 22, Batch 123, Train Loss: 0.10153137147426605, Memory (GB): 7\n",
      "Epoch 22, Batch 124, Train Loss: 0.10752664506435394, Memory (GB): 7\n",
      "Epoch 22, Batch 125, Train Loss: 0.11297039687633514, Memory (GB): 7\n",
      "Epoch 22, Batch 126, Train Loss: 0.0999336913228035, Memory (GB): 7\n",
      "Epoch 22, Batch 127, Train Loss: 0.1179257482290268, Memory (GB): 7\n",
      "Epoch 22, Batch 128, Train Loss: 0.09931884706020355, Memory (GB): 7\n",
      "Epoch 22, Batch 129, Train Loss: 0.10311201959848404, Memory (GB): 7\n",
      "Epoch 22, Batch 130, Train Loss: 0.09996990114450455, Memory (GB): 7\n",
      "Epoch 22, Batch 131, Train Loss: 0.10235752165317535, Memory (GB): 7\n",
      "Epoch 22, Batch 132, Train Loss: 0.09884892404079437, Memory (GB): 7\n",
      "Epoch 22, Batch 133, Train Loss: 0.09822316467761993, Memory (GB): 7\n",
      "Epoch 22, Batch 134, Train Loss: 0.10239848494529724, Memory (GB): 7\n",
      "Epoch 22, Batch 135, Train Loss: 0.10954932868480682, Memory (GB): 7\n",
      "Epoch 22, Batch 136, Train Loss: 0.10856060683727264, Memory (GB): 7\n",
      "Epoch 22, Batch 137, Train Loss: 0.11463629454374313, Memory (GB): 7\n",
      "Epoch 22, Batch 138, Train Loss: 0.11479123681783676, Memory (GB): 7\n",
      "Epoch 22, Batch 139, Train Loss: 0.11165197938680649, Memory (GB): 7\n",
      "Epoch 22, Batch 140, Train Loss: 0.10852920264005661, Memory (GB): 7\n",
      "Epoch 22, Batch 141, Train Loss: 0.10421677678823471, Memory (GB): 7\n",
      "Epoch 22, Batch 142, Train Loss: 0.12681181728839874, Memory (GB): 7\n",
      "Epoch 22, Batch 143, Train Loss: 0.1051681637763977, Memory (GB): 7\n",
      "Epoch 22, Batch 144, Train Loss: 0.11143134534358978, Memory (GB): 7\n",
      "Epoch 22, Batch 145, Train Loss: 0.12074997276067734, Memory (GB): 7\n",
      "Epoch 22, Batch 146, Train Loss: 0.12080101668834686, Memory (GB): 7\n",
      "Epoch 22, Batch 147, Train Loss: 0.10014086961746216, Memory (GB): 7\n",
      "Epoch 22, Batch 148, Train Loss: 0.12688042223453522, Memory (GB): 7\n",
      "Epoch 22, Batch 149, Train Loss: 0.11382018774747849, Memory (GB): 7\n",
      "Epoch 22, Batch 150, Train Loss: 0.09760048240423203, Memory (GB): 7\n",
      "Epoch 22, Batch 151, Train Loss: 0.11799795180559158, Memory (GB): 7\n",
      "Epoch 22, Batch 152, Train Loss: 0.12308486551046371, Memory (GB): 7\n",
      "Epoch 22, Batch 153, Train Loss: 0.10201722383499146, Memory (GB): 7\n",
      "Epoch 22, Batch 154, Train Loss: 0.11321739107370377, Memory (GB): 7\n",
      "Epoch 22, Batch 155, Train Loss: 0.10600502043962479, Memory (GB): 7\n",
      "Epoch 22, Batch 156, Train Loss: 0.09431443363428116, Memory (GB): 7\n",
      "Epoch 22, Batch 157, Train Loss: 0.0969868153333664, Memory (GB): 7\n",
      "Epoch 22, Batch 158, Train Loss: 0.12247360497713089, Memory (GB): 7\n",
      "Epoch 22, Batch 159, Train Loss: 0.1317661702632904, Memory (GB): 7\n",
      "Epoch 22, Batch 160, Train Loss: 0.0993773490190506, Memory (GB): 7\n",
      "Epoch 22, Batch 161, Train Loss: 0.11559447646141052, Memory (GB): 7\n",
      "Epoch 22, Batch 162, Train Loss: 0.10709403455257416, Memory (GB): 7\n",
      "Epoch 22, Batch 163, Train Loss: 0.11228682845830917, Memory (GB): 7\n",
      "Epoch 22, Batch 164, Train Loss: 0.09834937751293182, Memory (GB): 7\n",
      "Epoch 22, Batch 165, Train Loss: 0.09590677917003632, Memory (GB): 7\n",
      "Epoch 22, Batch 166, Train Loss: 0.11569051444530487, Memory (GB): 7\n",
      "Epoch 22, Batch 167, Train Loss: 0.11387582123279572, Memory (GB): 7\n",
      "Epoch 22, Batch 168, Train Loss: 0.10742434114217758, Memory (GB): 7\n",
      "Epoch 22, Batch 169, Train Loss: 0.09751957654953003, Memory (GB): 7\n",
      "Epoch 22, Batch 170, Train Loss: 0.11620792746543884, Memory (GB): 7\n",
      "Epoch 22, Batch 171, Train Loss: 0.09521272778511047, Memory (GB): 7\n",
      "Epoch 22, Batch 172, Train Loss: 0.10597182810306549, Memory (GB): 7\n",
      "Epoch 22, Batch 173, Train Loss: 0.10469824820756912, Memory (GB): 7\n",
      "Epoch 22, Batch 174, Train Loss: 0.11718448251485825, Memory (GB): 7\n",
      "Epoch 22, Batch 175, Train Loss: 0.10849439352750778, Memory (GB): 7\n",
      "Epoch 22, Batch 176, Train Loss: 0.11357863992452621, Memory (GB): 7\n",
      "Epoch 22, Batch 177, Train Loss: 0.10623613744974136, Memory (GB): 7\n",
      "Epoch 22, Batch 178, Train Loss: 0.10030841827392578, Memory (GB): 7\n",
      "Epoch 22, Batch 179, Train Loss: 0.11092458665370941, Memory (GB): 7\n",
      "[9.549150281251664e-05, 9.071692767189082e-05, 8.618108128829621e-05, 8.187202722388147e-05, 7.77784258626874e-05, 7.388950456955303e-05, 7.019502934107536e-05, 6.66852778740216e-05, 6.335101398032052e-05, 6.0183463281304456e-05, 5.717429011723927e-05, 5.431557561137726e-05, 5.1599796830808406e-05, 4.9019806989267984e-05, 4.6568816639804606e-05, 4.424037580781436e-05, 4.202835701742362e-05, 3.992693916655244e-05, 3.793059220822482e-05, 3.6034062597813585e-05, 3.4232359467922896e-05, 3.252074149452673e-05, 3.089470441980039e-05, 2.9349969198810393e-05, 2.7882470738869883e-05, 2.6488347201926384e-05, 2.516392984183005e-05, 2.3905733349738543e-05, 2.2710446682251623e-05, 2.1574924348139033e-05, 2.0496178130732104e-05, 1.9471369224195476e-05, 1.849780076298571e-05, 1.7572910724836413e-05, 1.6694265188594594e-05, 1.5859551929164866e-05, 1.5066574332706625e-05, 1.4313245616071278e-05, 1.3597583335267728e-05, 1.2917704168504333e-05, 1.2271818960079119e-05, 1.1658228012075164e-05, 1.1075316611471403e-05, 1.052155078089783e-05, 9.99547324185294e-06, 9.495699579760286e-06, 9.02091460077227e-06, 8.569868870733659e-06, 8.141375427196984e-06, 7.734306655837134e-06, 7.347591323045276e-06, 6.980211756893005e-06, 6.631201169048358e-06, 6.299641110595938e-06, 5.984659055066141e-06, 5.68542610231283e-06, 5.401154797197193e-06, 5.131097057337336e-06, 4.874542204470469e-06, 4.630815094246945e-06, 4.399274339534596e-06, 4.179310622557862e-06, 3.97034509142997e-06, 3.7718278368584715e-06, 3.5832364450155496e-06, 3.404074622764771e-06, 3.2338708916265342e-06, 3.072177347045206e-06, 2.918568479692945e-06, 2.772640055708297e-06, 2.634008052922883e-06, 2.5023076502767364e-06, 2.3771922677629017e-06, 2.258332654374756e-06, 2.1454160216560175e-06, 2.038145220573219e-06, 1.9362379595445557e-06, 1.8394260615673286e-06, 1.7474547584889623e-06, 1.6600820205645136e-06, 1.5770779195362876e-06, 1.4982240235594734e-06, 1.4233128223815002e-06, 1.3521471812624249e-06, 1.2845398221993039e-06, 1.2203128310893392e-06, 1.1592971895348707e-06, 1.101332330058129e-06, 1.0462657135552209e-06, 9.939524278774612e-07, 9.442548064835867e-07, 8.970420661594075e-07, 8.521899628514382e-07, 8.095804647088652e-07, 7.691014414734219e-07, 7.30646369399751e-07, 6.941140509297633e-07, 6.594083483832751e-07, 6.264379309641109e-07, 5.951160344159059e-07, 5.653602326951106e-07, 5.370922210603547e-07, 5.102376100073372e-07, 4.847257295069705e-07, 4.6048944303162186e-07, 4.374649708800409e-07, 4.1559172233603875e-07, 3.9481213621923684e-07, 3.7507152940827525e-07, 3.563179529378611e-07, 3.3850205529096806e-07, 3.2157695252641955e-07, 3.0549810490009854e-07, 2.902231996550935e-07, 2.7571203967233914e-07, 2.6192643768872186e-07, 2.488301158042856e-07, 2.3638861001407157e-07, 2.24569179513368e-07, 2.133407205376996e-07, 2.0267368451081468e-07, 1.9254000028527382e-07, 1.8291300027101022e-07, 1.7376735025745984e-07, 1.6507898274458673e-07, 1.5682503360735734e-07, 1.489837819269895e-07, 1.4153459283064009e-07, 1.3445786318910795e-07, 1.2773497002965253e-07, 1.2134822152816998e-07, 1.1528081045176143e-07, 1.0951676992917349e-07, 1.0404093143271468e-07, 9.883888486107894e-08, 9.389694061802505e-08, 8.920209358712372e-08, 8.474198890776754e-08, 8.050488946237917e-08, 7.647964498926023e-08, 7.265566273979716e-08, 6.902287960280732e-08, 6.557173562266704e-08, 6.229314884153365e-08, 5.917849139945689e-08, 5.6219566829484096e-08, 5.340858848800986e-08, 5.0738159063609364e-08, 4.820125111042892e-08, 4.5791188554907434e-08, 4.350162912716208e-08, 4.1326547670803985e-08, 3.92602202872638e-08, 3.729720927290057e-08, 3.543234880925557e-08, 3.366073136879279e-08, 3.197769480035315e-08, 3.037881006033548e-08, 2.8859869557318678e-08, 2.741687607945273e-08, 2.6046032275480122e-08, 2.4743730661706112e-08, 2.350654412862081e-08, 2.2331216922189776e-08, 2.1214656076080266e-08, 2.015392327227626e-08, 1.9146227108662437e-08, 1.818891575322933e-08, 1.7279469965567856e-08, 1.641549646728946e-08, 1.5594721643924987e-08, 1.4814985561728746e-08, 1.4074236283642303e-08, 1.3370524469460186e-08, 1.2701998245987175e-08, 1.206689833368782e-08, 1.1463553417003421e-08, 1.0890375746153255e-08, 1.0345856958845595e-08, 9.828564110903312e-09, 9.33713590535815e-09, 8.87027911009024e-09, 8.426765154585723e-09, 8.005426896856436e-09, 7.605155552013616e-09, 7.224897774412936e-09, 6.8636528856922856e-09, 6.520470241407674e-09, 6.194446729337285e-09, 5.884724392870422e-09, 5.5904881732269e-09, 5.310963764565559e-09, 5.045415576337277e-09, 4.793144797520415e-09, 4.553487557644394e-09, 4.325813179762176e-09, 4.109522520774067e-09, 3.904046394735362e-09, 3.7088440749985932e-09]\n",
      "Epoch 23, Batch 0, Train Loss: 0.09588778764009476, Memory (GB): 7\n",
      "Epoch 23, Batch 1, Train Loss: 0.08959680050611496, Memory (GB): 7\n",
      "Epoch 23, Batch 2, Train Loss: 0.08779080212116241, Memory (GB): 7\n",
      "Epoch 23, Batch 3, Train Loss: 0.10853642225265503, Memory (GB): 7\n",
      "Epoch 23, Batch 4, Train Loss: 0.10300933569669724, Memory (GB): 7\n",
      "Epoch 23, Batch 5, Train Loss: 0.10113538801670074, Memory (GB): 7\n",
      "Epoch 23, Batch 6, Train Loss: 0.08841240406036377, Memory (GB): 7\n",
      "Epoch 23, Batch 7, Train Loss: 0.10349148511886597, Memory (GB): 7\n",
      "Epoch 23, Batch 8, Train Loss: 0.10144838690757751, Memory (GB): 7\n",
      "Epoch 23, Batch 9, Train Loss: 0.09595736861228943, Memory (GB): 7\n",
      "Epoch 23, Batch 10, Train Loss: 0.10145300626754761, Memory (GB): 7\n",
      "Epoch 23, Batch 11, Train Loss: 0.09307172149419785, Memory (GB): 7\n",
      "Epoch 23, Batch 12, Train Loss: 0.09453646093606949, Memory (GB): 7\n",
      "Epoch 23, Batch 13, Train Loss: 0.08974847197532654, Memory (GB): 7\n",
      "Epoch 23, Batch 14, Train Loss: 0.09494048357009888, Memory (GB): 7\n",
      "Epoch 23, Batch 15, Train Loss: 0.1006370261311531, Memory (GB): 7\n",
      "Epoch 23, Batch 16, Train Loss: 0.10454193502664566, Memory (GB): 7\n",
      "Epoch 23, Batch 17, Train Loss: 0.09910997748374939, Memory (GB): 7\n",
      "Epoch 23, Batch 18, Train Loss: 0.10064519941806793, Memory (GB): 7\n",
      "Epoch 23, Batch 19, Train Loss: 0.09122486412525177, Memory (GB): 7\n",
      "Epoch 23, Batch 20, Train Loss: 0.09484092146158218, Memory (GB): 7\n",
      "Epoch 23, Batch 21, Train Loss: 0.09418172389268875, Memory (GB): 7\n",
      "Epoch 23, Batch 22, Train Loss: 0.09449087828397751, Memory (GB): 7\n",
      "Epoch 23, Batch 23, Train Loss: 0.10318905115127563, Memory (GB): 7\n",
      "Epoch 23, Batch 24, Train Loss: 0.08948379755020142, Memory (GB): 7\n",
      "Epoch 23, Batch 25, Train Loss: 0.09682174026966095, Memory (GB): 7\n",
      "Epoch 23, Batch 26, Train Loss: 0.11520508676767349, Memory (GB): 7\n",
      "Epoch 23, Batch 27, Train Loss: 0.09697582572698593, Memory (GB): 7\n",
      "Epoch 23, Batch 28, Train Loss: 0.09622307121753693, Memory (GB): 7\n",
      "Epoch 23, Batch 29, Train Loss: 0.08253492414951324, Memory (GB): 7\n",
      "Epoch 23, Batch 30, Train Loss: 0.09260145574808121, Memory (GB): 7\n",
      "Epoch 23, Batch 31, Train Loss: 0.07791627943515778, Memory (GB): 7\n",
      "Epoch 23, Batch 32, Train Loss: 0.0865277498960495, Memory (GB): 7\n",
      "Epoch 23, Batch 33, Train Loss: 0.0877937451004982, Memory (GB): 7\n",
      "Epoch 23, Batch 34, Train Loss: 0.08260949701070786, Memory (GB): 7\n",
      "Epoch 23, Batch 35, Train Loss: 0.09880207479000092, Memory (GB): 7\n",
      "Epoch 23, Batch 36, Train Loss: 0.09890829026699066, Memory (GB): 7\n",
      "Epoch 23, Batch 37, Train Loss: 0.08605340868234634, Memory (GB): 7\n",
      "Epoch 23, Batch 38, Train Loss: 0.10453785210847855, Memory (GB): 7\n",
      "Epoch 23, Batch 39, Train Loss: 0.09054054319858551, Memory (GB): 7\n",
      "Epoch 23, Batch 40, Train Loss: 0.0850696787238121, Memory (GB): 7\n",
      "Epoch 23, Batch 41, Train Loss: 0.09383028745651245, Memory (GB): 7\n",
      "Epoch 23, Batch 42, Train Loss: 0.08354954421520233, Memory (GB): 7\n",
      "Epoch 23, Batch 43, Train Loss: 0.0871664360165596, Memory (GB): 7\n",
      "Epoch 23, Batch 44, Train Loss: 0.10959528386592865, Memory (GB): 7\n",
      "Epoch 23, Batch 45, Train Loss: 0.10404244065284729, Memory (GB): 7\n",
      "Epoch 23, Batch 46, Train Loss: 0.09917842596769333, Memory (GB): 7\n",
      "Epoch 23, Batch 47, Train Loss: 0.09258483350276947, Memory (GB): 7\n",
      "Epoch 23, Batch 48, Train Loss: 0.08861958235502243, Memory (GB): 7\n",
      "Epoch 23, Batch 49, Train Loss: 0.10323509573936462, Memory (GB): 7\n",
      "Epoch 23, Batch 50, Train Loss: 0.09279005229473114, Memory (GB): 7\n",
      "Epoch 23, Batch 51, Train Loss: 0.10056811571121216, Memory (GB): 7\n",
      "Epoch 23, Batch 52, Train Loss: 0.0949140191078186, Memory (GB): 7\n",
      "Epoch 23, Batch 53, Train Loss: 0.08330519497394562, Memory (GB): 7\n",
      "Epoch 23, Batch 54, Train Loss: 0.10410794615745544, Memory (GB): 7\n",
      "Epoch 23, Batch 55, Train Loss: 0.08933362364768982, Memory (GB): 7\n",
      "Epoch 23, Batch 56, Train Loss: 0.09958657622337341, Memory (GB): 7\n",
      "Epoch 23, Batch 57, Train Loss: 0.09223445504903793, Memory (GB): 7\n",
      "Epoch 23, Batch 58, Train Loss: 0.1139032393693924, Memory (GB): 7\n",
      "Epoch 23, Batch 59, Train Loss: 0.09209422022104263, Memory (GB): 7\n",
      "Epoch 23, Batch 60, Train Loss: 0.10104008764028549, Memory (GB): 7\n",
      "Epoch 23, Batch 61, Train Loss: 0.09217771887779236, Memory (GB): 7\n",
      "Epoch 23, Batch 62, Train Loss: 0.11043427139520645, Memory (GB): 7\n",
      "Epoch 23, Batch 63, Train Loss: 0.09725750982761383, Memory (GB): 7\n",
      "Epoch 23, Batch 64, Train Loss: 0.1112179085612297, Memory (GB): 7\n",
      "Epoch 23, Batch 65, Train Loss: 0.09801463037729263, Memory (GB): 7\n",
      "Epoch 23, Batch 66, Train Loss: 0.10966739058494568, Memory (GB): 7\n",
      "Epoch 23, Batch 67, Train Loss: 0.0992189571261406, Memory (GB): 7\n",
      "Epoch 23, Batch 68, Train Loss: 0.10524226725101471, Memory (GB): 7\n",
      "Epoch 23, Batch 69, Train Loss: 0.10758152604103088, Memory (GB): 7\n",
      "Epoch 23, Batch 70, Train Loss: 0.09378217905759811, Memory (GB): 7\n",
      "Epoch 23, Batch 71, Train Loss: 0.10147317498922348, Memory (GB): 7\n",
      "Epoch 23, Batch 72, Train Loss: 0.08658520132303238, Memory (GB): 7\n",
      "Epoch 23, Batch 73, Train Loss: 0.10479651391506195, Memory (GB): 7\n",
      "Epoch 23, Batch 74, Train Loss: 0.107338085770607, Memory (GB): 7\n",
      "Epoch 23, Batch 75, Train Loss: 0.09481018036603928, Memory (GB): 7\n",
      "Epoch 23, Batch 76, Train Loss: 0.09316424280405045, Memory (GB): 7\n",
      "Epoch 23, Batch 77, Train Loss: 0.08789510279893875, Memory (GB): 7\n",
      "Epoch 23, Batch 78, Train Loss: 0.10214408487081528, Memory (GB): 7\n",
      "Epoch 23, Batch 79, Train Loss: 0.10019765794277191, Memory (GB): 7\n",
      "Epoch 23, Batch 80, Train Loss: 0.10626395791769028, Memory (GB): 7\n",
      "Epoch 23, Batch 81, Train Loss: 0.11029652506113052, Memory (GB): 7\n",
      "Epoch 23, Batch 82, Train Loss: 0.09806178510189056, Memory (GB): 7\n",
      "Epoch 23, Batch 83, Train Loss: 0.09525507688522339, Memory (GB): 7\n",
      "Epoch 23, Batch 84, Train Loss: 0.10433398932218552, Memory (GB): 7\n",
      "Epoch 23, Batch 85, Train Loss: 0.10162351280450821, Memory (GB): 7\n",
      "Epoch 23, Batch 86, Train Loss: 0.09769798815250397, Memory (GB): 7\n",
      "Epoch 23, Batch 87, Train Loss: 0.10431505739688873, Memory (GB): 7\n",
      "Epoch 23, Batch 88, Train Loss: 0.10571234673261642, Memory (GB): 7\n",
      "Epoch 23, Batch 89, Train Loss: 0.10032512992620468, Memory (GB): 7\n",
      "Epoch 23, Batch 90, Train Loss: 0.10819288343191147, Memory (GB): 7\n",
      "Epoch 23, Batch 91, Train Loss: 0.09670645743608475, Memory (GB): 7\n",
      "Epoch 23, Batch 92, Train Loss: 0.10196968168020248, Memory (GB): 7\n",
      "Epoch 23, Batch 93, Train Loss: 0.11280695348978043, Memory (GB): 7\n",
      "Epoch 23, Batch 94, Train Loss: 0.0927610844373703, Memory (GB): 7\n",
      "Epoch 23, Batch 95, Train Loss: 0.08016455918550491, Memory (GB): 7\n",
      "Epoch 23, Batch 96, Train Loss: 0.08953157067298889, Memory (GB): 7\n",
      "Epoch 23, Batch 97, Train Loss: 0.10080836713314056, Memory (GB): 7\n",
      "Epoch 23, Batch 98, Train Loss: 0.10196688026189804, Memory (GB): 7\n",
      "Epoch 23, Batch 99, Train Loss: 0.09138880670070648, Memory (GB): 7\n",
      "Epoch 23, Batch 100, Train Loss: 0.09273721277713776, Memory (GB): 7\n",
      "Epoch 23, Batch 101, Train Loss: 0.08795399963855743, Memory (GB): 7\n",
      "Epoch 23, Batch 102, Train Loss: 0.09847184270620346, Memory (GB): 7\n",
      "Epoch 23, Batch 103, Train Loss: 0.10486119240522385, Memory (GB): 7\n",
      "Epoch 23, Batch 104, Train Loss: 0.08550075441598892, Memory (GB): 7\n",
      "Epoch 23, Batch 105, Train Loss: 0.09192617982625961, Memory (GB): 7\n",
      "Epoch 23, Batch 106, Train Loss: 0.09988423436880112, Memory (GB): 7\n",
      "Epoch 23, Batch 107, Train Loss: 0.1011231541633606, Memory (GB): 7\n",
      "Epoch 23, Batch 108, Train Loss: 0.09879675507545471, Memory (GB): 7\n",
      "Epoch 23, Batch 109, Train Loss: 0.08147908747196198, Memory (GB): 7\n",
      "Epoch 23, Batch 110, Train Loss: 0.08379941433668137, Memory (GB): 7\n",
      "Epoch 23, Batch 111, Train Loss: 0.0770304799079895, Memory (GB): 7\n",
      "Epoch 23, Batch 112, Train Loss: 0.08808111399412155, Memory (GB): 7\n",
      "Epoch 23, Batch 113, Train Loss: 0.08985387533903122, Memory (GB): 7\n",
      "Epoch 23, Batch 114, Train Loss: 0.09246628731489182, Memory (GB): 7\n",
      "Epoch 23, Batch 115, Train Loss: 0.08977770060300827, Memory (GB): 7\n",
      "Epoch 23, Batch 116, Train Loss: 0.09110499918460846, Memory (GB): 7\n",
      "Epoch 23, Batch 117, Train Loss: 0.10276411473751068, Memory (GB): 7\n",
      "Epoch 23, Batch 118, Train Loss: 0.09932296723127365, Memory (GB): 7\n",
      "Epoch 23, Batch 119, Train Loss: 0.09582830965518951, Memory (GB): 7\n",
      "Epoch 23, Batch 120, Train Loss: 0.08077281713485718, Memory (GB): 7\n",
      "Epoch 23, Batch 121, Train Loss: 0.09927558898925781, Memory (GB): 7\n",
      "Epoch 23, Batch 122, Train Loss: 0.09257758408784866, Memory (GB): 7\n",
      "Epoch 23, Batch 123, Train Loss: 0.08654190599918365, Memory (GB): 7\n",
      "Epoch 23, Batch 124, Train Loss: 0.08510567247867584, Memory (GB): 7\n",
      "Epoch 23, Batch 125, Train Loss: 0.10140908509492874, Memory (GB): 7\n",
      "Epoch 23, Batch 126, Train Loss: 0.09361974149942398, Memory (GB): 7\n",
      "Epoch 23, Batch 127, Train Loss: 0.09961685538291931, Memory (GB): 7\n",
      "Epoch 23, Batch 128, Train Loss: 0.08824619650840759, Memory (GB): 7\n",
      "Epoch 23, Batch 129, Train Loss: 0.07760404050350189, Memory (GB): 7\n",
      "Epoch 23, Batch 130, Train Loss: 0.07823928445577621, Memory (GB): 7\n",
      "Epoch 23, Batch 131, Train Loss: 0.0928666740655899, Memory (GB): 7\n",
      "Epoch 23, Batch 132, Train Loss: 0.10039975494146347, Memory (GB): 7\n",
      "Epoch 23, Batch 133, Train Loss: 0.09351374953985214, Memory (GB): 7\n",
      "Epoch 23, Batch 134, Train Loss: 0.08943138271570206, Memory (GB): 7\n",
      "Epoch 23, Batch 135, Train Loss: 0.09718438237905502, Memory (GB): 7\n",
      "Epoch 23, Batch 136, Train Loss: 0.09191962331533432, Memory (GB): 7\n",
      "Epoch 23, Batch 137, Train Loss: 0.09355460107326508, Memory (GB): 7\n",
      "Epoch 23, Batch 138, Train Loss: 0.08682037889957428, Memory (GB): 7\n",
      "Epoch 23, Batch 139, Train Loss: 0.0938768982887268, Memory (GB): 7\n",
      "Epoch 23, Batch 140, Train Loss: 0.08973278105258942, Memory (GB): 7\n",
      "Epoch 23, Batch 141, Train Loss: 0.08297602087259293, Memory (GB): 7\n",
      "Epoch 23, Batch 142, Train Loss: 0.10026302933692932, Memory (GB): 7\n",
      "Epoch 23, Batch 143, Train Loss: 0.09124201536178589, Memory (GB): 7\n",
      "Epoch 23, Batch 144, Train Loss: 0.0901065319776535, Memory (GB): 7\n",
      "Epoch 23, Batch 145, Train Loss: 0.08894430845975876, Memory (GB): 7\n",
      "Epoch 23, Batch 146, Train Loss: 0.09304770827293396, Memory (GB): 7\n",
      "Epoch 23, Batch 147, Train Loss: 0.09959223121404648, Memory (GB): 7\n",
      "Epoch 23, Batch 148, Train Loss: 0.09754497557878494, Memory (GB): 7\n",
      "Epoch 23, Batch 149, Train Loss: 0.11498299986124039, Memory (GB): 7\n",
      "Epoch 23, Batch 150, Train Loss: 0.09508444368839264, Memory (GB): 7\n",
      "Epoch 23, Batch 151, Train Loss: 0.08739607781171799, Memory (GB): 7\n",
      "Epoch 23, Batch 152, Train Loss: 0.09361190348863602, Memory (GB): 7\n",
      "Epoch 23, Batch 153, Train Loss: 0.10548118501901627, Memory (GB): 7\n",
      "Epoch 23, Batch 154, Train Loss: 0.09535626322031021, Memory (GB): 7\n",
      "Epoch 23, Batch 155, Train Loss: 0.09984046220779419, Memory (GB): 7\n",
      "Epoch 23, Batch 156, Train Loss: 0.10297974944114685, Memory (GB): 7\n",
      "Epoch 23, Batch 157, Train Loss: 0.09929925203323364, Memory (GB): 7\n",
      "Epoch 23, Batch 158, Train Loss: 0.09240536391735077, Memory (GB): 7\n",
      "Epoch 23, Batch 159, Train Loss: 0.10233969986438751, Memory (GB): 7\n",
      "Epoch 23, Batch 160, Train Loss: 0.10014401376247406, Memory (GB): 7\n",
      "Epoch 23, Batch 161, Train Loss: 0.0861859992146492, Memory (GB): 7\n",
      "Epoch 23, Batch 162, Train Loss: 0.1084500253200531, Memory (GB): 7\n",
      "Epoch 23, Batch 163, Train Loss: 0.1051703542470932, Memory (GB): 7\n",
      "Epoch 23, Batch 164, Train Loss: 0.09828290343284607, Memory (GB): 7\n",
      "Epoch 23, Batch 165, Train Loss: 0.10101760923862457, Memory (GB): 7\n",
      "Epoch 23, Batch 166, Train Loss: 0.10385002195835114, Memory (GB): 7\n",
      "Epoch 23, Batch 167, Train Loss: 0.10175486654043198, Memory (GB): 7\n",
      "Epoch 23, Batch 168, Train Loss: 0.09713038802146912, Memory (GB): 7\n",
      "Epoch 23, Batch 169, Train Loss: 0.09557056427001953, Memory (GB): 7\n",
      "Epoch 23, Batch 170, Train Loss: 0.10271058976650238, Memory (GB): 7\n",
      "Epoch 23, Batch 171, Train Loss: 0.11083680391311646, Memory (GB): 7\n",
      "Epoch 23, Batch 172, Train Loss: 0.1105930358171463, Memory (GB): 7\n",
      "Epoch 23, Batch 173, Train Loss: 0.10538565367460251, Memory (GB): 7\n",
      "Epoch 23, Batch 174, Train Loss: 0.09762760251760483, Memory (GB): 7\n",
      "Epoch 23, Batch 175, Train Loss: 0.0949903354048729, Memory (GB): 7\n",
      "Epoch 23, Batch 176, Train Loss: 0.09678496420383453, Memory (GB): 7\n",
      "Epoch 23, Batch 177, Train Loss: 0.1057722344994545, Memory (GB): 7\n",
      "Epoch 23, Batch 178, Train Loss: 0.11426496505737305, Memory (GB): 7\n",
      "Epoch 23, Batch 179, Train Loss: 0.09268882870674133, Memory (GB): 7\n",
      "[0.0006545084971876519, 0.0006217830723282687, 0.0005906939187118555, 0.0005611592227762628, 0.0005331012616374494, 0.0005064461985555768, 0.000481123888627798, 0.00045706769419640806, 0.000434214309486588, 0.0004125035940122581, 0.0003918784143116456, 0.000372284493596063, 0.00035367026891626, 0.0003359867554704469, 0.0003191874176969245, 0.0003032280468120783, 0.00028806664447147436, 0.00027366331224790055, 0.00025998014663550554, 0.00024698113930373013, 0.0002346320823385436, 0.0002229004782216167, 0.0002117554543105357, 0.00020116768159500896, 0.00019110929751525838, 0.0001815538326394955, 0.00017247614100752076, 0.00016385233395714457, 0.00015565971725928744, 0.00014787673139632305, 0.0001404828948265069, 0.0001334587500851815, 0.00012678581258092253, 0.00012044652195187636, 0.00011442419585428253, 0.00010870298606156847, 0.00010326783675848993, 9.810444492056542e-05, 9.319922267453716e-05, 8.853926154081025e-05, 8.41122984637697e-05, 7.990668354058128e-05, 7.591134936355222e-05, 7.21157818953746e-05, 6.850999280060586e-05, 6.508449316057556e-05, 6.183026850254678e-05, 5.87387550774195e-05, 5.5801817323548464e-05, 5.301172645737101e-05, 5.0361140134502494e-05, 4.784308312777738e-05, 4.5450928971388516e-05, 4.3178382522819076e-05, 4.1019463396678114e-05, 3.8968490226844236e-05, 3.7020065715501984e-05, 3.5169062429726903e-05, 3.341060930824054e-05, 3.174007884282854e-05, 3.0153074900687077e-05, 2.8645421155652735e-05, 2.721315009787011e-05, 2.5852492592976594e-05, 2.4559867963327767e-05, 2.3331874565161377e-05, 2.21652808369033e-05, 2.1057016795058127e-05, 2.000416595530522e-05, 1.900395765753995e-05, 1.8053759774662948e-05, 1.7151071785929823e-05, 1.6293518196633327e-05, 1.5478842286801656e-05, 1.4704900172461565e-05, 1.3969655163838477e-05, 1.3271172405646565e-05, 1.2607613785364239e-05, 1.1977233096096026e-05, 1.1378371441291221e-05, 1.0809452869226663e-05, 1.0268980225765327e-05, 9.755531214477057e-06, 9.267754653753209e-06, 8.804366921065548e-06, 8.36414857501227e-06, 7.945941146261651e-06, 7.548644088948576e-06, 7.171211884501148e-06, 6.812651290276086e-06, 6.4720187257622815e-06, 6.148417789474168e-06, 5.840996900000456e-06, 5.548947055000433e-06, 5.271499702250413e-06, 5.007924717137896e-06, 4.757528481280997e-06, 4.519652057216949e-06, 4.2936694543561e-06, 4.078985981638296e-06, 3.875036682556382e-06, 3.6812848484285613e-06, 3.4972206060071318e-06, 3.3223595757067793e-06, 3.1562415969214406e-06, 2.998429517075366e-06, 2.8485080412215976e-06, 2.7060826391605187e-06, 2.570778507202491e-06, 2.442239581842367e-06, 2.3201276027502497e-06, 2.204121222612736e-06, 2.0939151614820988e-06, 1.989219403407994e-06, 1.8897584332375948e-06, 1.795270511575714e-06, 1.7055069859969284e-06, 1.6202316366970827e-06, 1.5392200548622274e-06, 1.4622590521191162e-06, 1.3891460995131599e-06, 1.3196887945375026e-06, 1.2537043548106282e-06, 1.1910191370700964e-06, 1.1314681802165902e-06, 1.0748947712057613e-06, 1.0211500326454737e-06, 9.700925310131993e-07, 9.215879044625393e-07, 8.755085092394126e-07, 8.317330837774426e-07, 7.901464295885696e-07, 7.50639108109141e-07, 7.13107152703684e-07, 6.774517950684995e-07, 6.435792053150745e-07, 6.114002450493208e-07, 5.808302327968547e-07, 5.517887211570121e-07, 5.241992850991611e-07, 4.979893208442038e-07, 4.730898548019933e-07, 4.494353620618936e-07, 4.2696359395879886e-07, 4.056154142608587e-07, 3.8533464354781605e-07, 3.66067911370425e-07, 3.477645158019038e-07, 3.3037629001180866e-07, 3.138574755112181e-07, 2.981646017356572e-07, 2.832563716488743e-07, 2.690935530664306e-07, 2.5563887541310927e-07, 2.428569316424538e-07, 2.307140850603309e-07, 2.1917838080731426e-07, 2.0821946176694876e-07, 1.9780848867860115e-07, 1.879180642446712e-07, 1.7852216103243767e-07, 1.6959605298081572e-07, 1.611162503317749e-07, 1.5306043781518614e-07, 1.4540741592442677e-07, 1.3813704512820544e-07, 1.3123019287179508e-07, 1.2466868322820545e-07, 1.1843524906679512e-07, 1.1251348661345546e-07, 1.0688781228278252e-07, 1.0154342166864354e-07, 9.64662505852113e-08, 9.164293805595072e-08, 8.706079115315324e-08, 8.270775159549549e-08, 7.85723640157207e-08, 7.464374581493473e-08, 7.091155852418799e-08, 6.736598059797857e-08, 6.399768156807968e-08, 6.079779748967563e-08, 5.775790761519183e-08, 5.487001223443229e-08, 5.212651162271067e-08, 4.95201860415751e-08, 4.704417673949634e-08, 4.469196790252154e-08, 4.2457369507395476e-08, 4.0334501032025694e-08, 3.831777598042441e-08, 3.640188718140318e-08, 3.4581792822333034e-08, 3.2852703181216364e-08, 3.121006802215552e-08, 2.9649564621047767e-08, 2.816708638999537e-08, 2.6758732070495626e-08, 2.542079546697083e-08]\n",
      "Epoch 24, Batch 0, Train Loss: 0.10614527016878128, Memory (GB): 7\n",
      "Epoch 24, Batch 1, Train Loss: 0.09981471300125122, Memory (GB): 7\n",
      "Epoch 24, Batch 2, Train Loss: 0.09314218163490295, Memory (GB): 7\n",
      "Epoch 24, Batch 3, Train Loss: 0.08588965982198715, Memory (GB): 7\n",
      "Epoch 24, Batch 4, Train Loss: 0.10198278725147247, Memory (GB): 7\n",
      "Epoch 24, Batch 5, Train Loss: 0.10448620468378067, Memory (GB): 7\n",
      "Epoch 24, Batch 6, Train Loss: 0.09379129111766815, Memory (GB): 7\n",
      "Epoch 24, Batch 7, Train Loss: 0.09087280929088593, Memory (GB): 7\n",
      "Epoch 24, Batch 8, Train Loss: 0.09731026738882065, Memory (GB): 7\n",
      "Epoch 24, Batch 9, Train Loss: 0.08845831453800201, Memory (GB): 7\n",
      "Epoch 24, Batch 10, Train Loss: 0.09181477129459381, Memory (GB): 7\n",
      "Epoch 24, Batch 11, Train Loss: 0.0929500162601471, Memory (GB): 7\n",
      "Epoch 24, Batch 12, Train Loss: 0.08224085718393326, Memory (GB): 7\n",
      "Epoch 24, Batch 13, Train Loss: 0.09125234931707382, Memory (GB): 7\n",
      "Epoch 24, Batch 14, Train Loss: 0.08303044736385345, Memory (GB): 7\n",
      "Epoch 24, Batch 15, Train Loss: 0.09954673796892166, Memory (GB): 7\n",
      "Epoch 24, Batch 16, Train Loss: 0.08735135942697525, Memory (GB): 7\n",
      "Epoch 24, Batch 17, Train Loss: 0.08783433586359024, Memory (GB): 7\n",
      "Epoch 24, Batch 18, Train Loss: 0.09554833173751831, Memory (GB): 7\n",
      "Epoch 24, Batch 19, Train Loss: 0.09482128173112869, Memory (GB): 7\n",
      "Epoch 24, Batch 20, Train Loss: 0.09050162881612778, Memory (GB): 7\n",
      "Epoch 24, Batch 21, Train Loss: 0.07855658233165741, Memory (GB): 7\n",
      "Epoch 24, Batch 22, Train Loss: 0.08716632425785065, Memory (GB): 7\n",
      "Epoch 24, Batch 23, Train Loss: 0.09591542184352875, Memory (GB): 7\n",
      "Epoch 24, Batch 24, Train Loss: 0.09044861793518066, Memory (GB): 7\n",
      "Epoch 24, Batch 25, Train Loss: 0.08921989053487778, Memory (GB): 7\n",
      "Epoch 24, Batch 26, Train Loss: 0.0833636000752449, Memory (GB): 7\n",
      "Epoch 24, Batch 27, Train Loss: 0.09225276857614517, Memory (GB): 7\n",
      "Epoch 24, Batch 28, Train Loss: 0.08727602660655975, Memory (GB): 7\n",
      "Epoch 24, Batch 29, Train Loss: 0.0899333581328392, Memory (GB): 7\n",
      "Epoch 24, Batch 30, Train Loss: 0.10569266229867935, Memory (GB): 7\n",
      "Epoch 24, Batch 31, Train Loss: 0.10871564596891403, Memory (GB): 7\n",
      "Epoch 24, Batch 32, Train Loss: 0.08431261777877808, Memory (GB): 7\n",
      "Epoch 24, Batch 33, Train Loss: 0.08085625618696213, Memory (GB): 7\n",
      "Epoch 24, Batch 34, Train Loss: 0.08312973380088806, Memory (GB): 7\n",
      "Epoch 24, Batch 35, Train Loss: 0.09832209348678589, Memory (GB): 7\n",
      "Epoch 24, Batch 36, Train Loss: 0.09134382009506226, Memory (GB): 7\n",
      "Epoch 24, Batch 37, Train Loss: 0.0861460343003273, Memory (GB): 7\n",
      "Epoch 24, Batch 38, Train Loss: 0.0995149239897728, Memory (GB): 7\n",
      "Epoch 24, Batch 39, Train Loss: 0.08671456575393677, Memory (GB): 7\n",
      "Epoch 24, Batch 40, Train Loss: 0.09528664499521255, Memory (GB): 7\n",
      "Epoch 24, Batch 41, Train Loss: 0.08036842942237854, Memory (GB): 7\n",
      "Epoch 24, Batch 42, Train Loss: 0.10041872411966324, Memory (GB): 7\n",
      "Epoch 24, Batch 43, Train Loss: 0.09230227023363113, Memory (GB): 7\n",
      "Epoch 24, Batch 44, Train Loss: 0.08438996970653534, Memory (GB): 7\n",
      "Epoch 24, Batch 45, Train Loss: 0.0904524028301239, Memory (GB): 7\n",
      "Epoch 24, Batch 46, Train Loss: 0.07860241830348969, Memory (GB): 7\n",
      "Epoch 24, Batch 47, Train Loss: 0.08172572404146194, Memory (GB): 7\n",
      "Epoch 24, Batch 48, Train Loss: 0.10521198809146881, Memory (GB): 7\n",
      "Epoch 24, Batch 49, Train Loss: 0.08638545125722885, Memory (GB): 7\n",
      "Epoch 24, Batch 50, Train Loss: 0.0829363614320755, Memory (GB): 7\n",
      "Epoch 24, Batch 51, Train Loss: 0.07752803713083267, Memory (GB): 7\n",
      "Epoch 24, Batch 52, Train Loss: 0.09003142267465591, Memory (GB): 7\n",
      "Epoch 24, Batch 53, Train Loss: 0.09391624480485916, Memory (GB): 7\n",
      "Epoch 24, Batch 54, Train Loss: 0.07888831198215485, Memory (GB): 7\n",
      "Epoch 24, Batch 55, Train Loss: 0.08856562525033951, Memory (GB): 7\n",
      "Epoch 24, Batch 56, Train Loss: 0.08641839772462845, Memory (GB): 7\n",
      "Epoch 24, Batch 57, Train Loss: 0.08611943572759628, Memory (GB): 7\n",
      "Epoch 24, Batch 58, Train Loss: 0.09082809090614319, Memory (GB): 7\n",
      "Epoch 24, Batch 59, Train Loss: 0.09377391636371613, Memory (GB): 7\n",
      "Epoch 24, Batch 60, Train Loss: 0.077809177339077, Memory (GB): 7\n",
      "Epoch 24, Batch 61, Train Loss: 0.087002232670784, Memory (GB): 7\n",
      "Epoch 24, Batch 62, Train Loss: 0.0938439592719078, Memory (GB): 7\n",
      "Epoch 24, Batch 63, Train Loss: 0.08512052148580551, Memory (GB): 7\n",
      "Epoch 24, Batch 64, Train Loss: 0.11343669891357422, Memory (GB): 7\n",
      "Epoch 24, Batch 65, Train Loss: 0.09181616455316544, Memory (GB): 7\n",
      "Epoch 24, Batch 66, Train Loss: 0.09005339443683624, Memory (GB): 7\n",
      "Epoch 24, Batch 67, Train Loss: 0.09019454568624496, Memory (GB): 7\n",
      "Epoch 24, Batch 68, Train Loss: 0.09264931082725525, Memory (GB): 7\n",
      "Epoch 24, Batch 69, Train Loss: 0.08920451253652573, Memory (GB): 7\n",
      "Epoch 24, Batch 70, Train Loss: 0.08803308755159378, Memory (GB): 7\n",
      "Epoch 24, Batch 71, Train Loss: 0.09295094013214111, Memory (GB): 7\n",
      "Epoch 24, Batch 72, Train Loss: 0.08222348242998123, Memory (GB): 7\n",
      "Epoch 24, Batch 73, Train Loss: 0.1001184806227684, Memory (GB): 7\n",
      "Epoch 24, Batch 74, Train Loss: 0.08409508317708969, Memory (GB): 7\n",
      "Epoch 24, Batch 75, Train Loss: 0.0951116681098938, Memory (GB): 7\n",
      "Epoch 24, Batch 76, Train Loss: 0.08908392488956451, Memory (GB): 7\n",
      "Epoch 24, Batch 77, Train Loss: 0.09661048650741577, Memory (GB): 7\n",
      "Epoch 24, Batch 78, Train Loss: 0.10746080428361893, Memory (GB): 7\n",
      "Epoch 24, Batch 79, Train Loss: 0.08344563096761703, Memory (GB): 7\n",
      "Epoch 24, Batch 80, Train Loss: 0.08993063867092133, Memory (GB): 7\n",
      "Epoch 24, Batch 81, Train Loss: 0.09893601387739182, Memory (GB): 7\n",
      "Epoch 24, Batch 82, Train Loss: 0.08732479810714722, Memory (GB): 7\n",
      "Epoch 24, Batch 83, Train Loss: 0.09563539177179337, Memory (GB): 7\n",
      "Epoch 24, Batch 84, Train Loss: 0.10553697496652603, Memory (GB): 7\n",
      "Epoch 24, Batch 85, Train Loss: 0.10126695036888123, Memory (GB): 7\n",
      "Epoch 24, Batch 86, Train Loss: 0.09241221845149994, Memory (GB): 7\n",
      "Epoch 24, Batch 87, Train Loss: 0.09184232354164124, Memory (GB): 7\n",
      "Epoch 24, Batch 88, Train Loss: 0.08751333504915237, Memory (GB): 7\n",
      "Epoch 24, Batch 89, Train Loss: 0.08816634863615036, Memory (GB): 7\n",
      "Epoch 24, Batch 90, Train Loss: 0.09823527932167053, Memory (GB): 7\n",
      "Epoch 24, Batch 91, Train Loss: 0.10658157616853714, Memory (GB): 7\n",
      "Epoch 24, Batch 92, Train Loss: 0.1080673560500145, Memory (GB): 7\n",
      "Epoch 24, Batch 93, Train Loss: 0.10103123635053635, Memory (GB): 7\n",
      "Epoch 24, Batch 94, Train Loss: 0.09813980013132095, Memory (GB): 7\n",
      "Epoch 24, Batch 95, Train Loss: 0.09535200148820877, Memory (GB): 7\n",
      "Epoch 24, Batch 96, Train Loss: 0.09050352871417999, Memory (GB): 7\n",
      "Epoch 24, Batch 97, Train Loss: 0.09293065965175629, Memory (GB): 7\n",
      "Epoch 24, Batch 98, Train Loss: 0.11745142936706543, Memory (GB): 7\n",
      "Epoch 24, Batch 99, Train Loss: 0.10544126480817795, Memory (GB): 7\n",
      "Epoch 24, Batch 100, Train Loss: 0.09982910752296448, Memory (GB): 7\n",
      "Epoch 24, Batch 101, Train Loss: 0.09666598588228226, Memory (GB): 7\n",
      "Epoch 24, Batch 102, Train Loss: 0.09266337007284164, Memory (GB): 7\n",
      "Epoch 24, Batch 103, Train Loss: 0.08844149857759476, Memory (GB): 7\n",
      "Epoch 24, Batch 104, Train Loss: 0.09011093527078629, Memory (GB): 7\n",
      "Epoch 24, Batch 105, Train Loss: 0.09286370873451233, Memory (GB): 7\n",
      "Epoch 24, Batch 106, Train Loss: 0.08966246247291565, Memory (GB): 7\n",
      "Epoch 24, Batch 107, Train Loss: 0.09533634781837463, Memory (GB): 7\n",
      "Epoch 24, Batch 108, Train Loss: 0.09387020766735077, Memory (GB): 7\n",
      "Epoch 24, Batch 109, Train Loss: 0.09107021987438202, Memory (GB): 7\n",
      "Epoch 24, Batch 110, Train Loss: 0.09855858236551285, Memory (GB): 7\n",
      "Epoch 24, Batch 111, Train Loss: 0.08821015059947968, Memory (GB): 7\n",
      "Epoch 24, Batch 112, Train Loss: 0.09432248026132584, Memory (GB): 7\n",
      "Epoch 24, Batch 113, Train Loss: 0.09302495419979095, Memory (GB): 7\n",
      "Epoch 24, Batch 114, Train Loss: 0.10363951325416565, Memory (GB): 7\n",
      "Epoch 24, Batch 115, Train Loss: 0.10423065721988678, Memory (GB): 7\n",
      "Epoch 24, Batch 116, Train Loss: 0.10514721274375916, Memory (GB): 7\n",
      "Epoch 24, Batch 117, Train Loss: 0.10045868903398514, Memory (GB): 7\n",
      "Epoch 24, Batch 118, Train Loss: 0.10859107971191406, Memory (GB): 7\n",
      "Epoch 24, Batch 119, Train Loss: 0.0977761372923851, Memory (GB): 7\n",
      "Epoch 24, Batch 120, Train Loss: 0.0996437668800354, Memory (GB): 7\n",
      "Epoch 24, Batch 121, Train Loss: 0.09261395037174225, Memory (GB): 7\n",
      "Epoch 24, Batch 122, Train Loss: 0.09737046808004379, Memory (GB): 7\n",
      "Epoch 24, Batch 123, Train Loss: 0.09767547249794006, Memory (GB): 7\n",
      "Epoch 24, Batch 124, Train Loss: 0.0949040949344635, Memory (GB): 7\n",
      "Epoch 24, Batch 125, Train Loss: 0.10133771598339081, Memory (GB): 7\n",
      "Epoch 24, Batch 126, Train Loss: 0.0966801643371582, Memory (GB): 7\n",
      "Epoch 24, Batch 127, Train Loss: 0.09394364804029465, Memory (GB): 7\n",
      "Epoch 24, Batch 128, Train Loss: 0.09137630462646484, Memory (GB): 7\n",
      "Epoch 24, Batch 129, Train Loss: 0.08764161169528961, Memory (GB): 7\n",
      "Epoch 24, Batch 130, Train Loss: 0.09537769854068756, Memory (GB): 7\n",
      "Epoch 24, Batch 131, Train Loss: 0.0896611362695694, Memory (GB): 7\n",
      "Epoch 24, Batch 132, Train Loss: 0.08815118670463562, Memory (GB): 7\n",
      "Epoch 24, Batch 133, Train Loss: 0.08242231607437134, Memory (GB): 7\n",
      "Epoch 24, Batch 134, Train Loss: 0.10143367201089859, Memory (GB): 7\n",
      "Epoch 24, Batch 135, Train Loss: 0.10459087044000626, Memory (GB): 7\n",
      "Epoch 24, Batch 136, Train Loss: 0.10159878432750702, Memory (GB): 7\n",
      "Epoch 24, Batch 137, Train Loss: 0.08769845217466354, Memory (GB): 7\n",
      "Epoch 24, Batch 138, Train Loss: 0.09097722917795181, Memory (GB): 7\n",
      "Epoch 24, Batch 139, Train Loss: 0.10989250242710114, Memory (GB): 7\n",
      "Epoch 24, Batch 140, Train Loss: 0.09514325112104416, Memory (GB): 7\n",
      "Epoch 24, Batch 141, Train Loss: 0.08470430970191956, Memory (GB): 7\n",
      "Epoch 24, Batch 142, Train Loss: 0.09323614090681076, Memory (GB): 7\n",
      "Epoch 24, Batch 143, Train Loss: 0.08203014731407166, Memory (GB): 7\n",
      "Epoch 24, Batch 144, Train Loss: 0.08079028129577637, Memory (GB): 7\n",
      "Epoch 24, Batch 145, Train Loss: 0.09442899376153946, Memory (GB): 7\n",
      "Epoch 24, Batch 146, Train Loss: 0.0900070071220398, Memory (GB): 7\n",
      "Epoch 24, Batch 147, Train Loss: 0.08513051271438599, Memory (GB): 7\n",
      "Epoch 24, Batch 148, Train Loss: 0.08020476251840591, Memory (GB): 7\n",
      "Epoch 24, Batch 149, Train Loss: 0.08844517171382904, Memory (GB): 7\n",
      "Epoch 24, Batch 150, Train Loss: 0.08369795978069305, Memory (GB): 7\n",
      "Epoch 24, Batch 151, Train Loss: 0.09258247911930084, Memory (GB): 7\n",
      "Epoch 24, Batch 152, Train Loss: 0.08935593068599701, Memory (GB): 7\n",
      "Epoch 24, Batch 153, Train Loss: 0.10391798615455627, Memory (GB): 7\n",
      "Epoch 24, Batch 154, Train Loss: 0.08378026634454727, Memory (GB): 7\n",
      "Epoch 24, Batch 155, Train Loss: 0.10797635465860367, Memory (GB): 7\n",
      "Epoch 24, Batch 156, Train Loss: 0.09447498619556427, Memory (GB): 7\n",
      "Epoch 24, Batch 157, Train Loss: 0.11491773277521133, Memory (GB): 7\n",
      "Epoch 24, Batch 158, Train Loss: 0.0938318595290184, Memory (GB): 7\n",
      "Epoch 24, Batch 159, Train Loss: 0.08428937941789627, Memory (GB): 7\n",
      "Epoch 24, Batch 160, Train Loss: 0.09386064857244492, Memory (GB): 7\n",
      "Epoch 24, Batch 161, Train Loss: 0.10033967345952988, Memory (GB): 7\n",
      "Epoch 24, Batch 162, Train Loss: 0.09090615808963776, Memory (GB): 7\n",
      "Epoch 24, Batch 163, Train Loss: 0.08559499680995941, Memory (GB): 7\n",
      "Epoch 24, Batch 164, Train Loss: 0.08595315366983414, Memory (GB): 7\n",
      "Epoch 24, Batch 165, Train Loss: 0.08709239214658737, Memory (GB): 7\n",
      "Epoch 24, Batch 166, Train Loss: 0.09876155853271484, Memory (GB): 7\n",
      "Epoch 24, Batch 167, Train Loss: 0.09378910064697266, Memory (GB): 7\n",
      "Epoch 24, Batch 168, Train Loss: 0.0965680181980133, Memory (GB): 7\n",
      "Epoch 24, Batch 169, Train Loss: 0.09125085920095444, Memory (GB): 7\n",
      "Epoch 24, Batch 170, Train Loss: 0.08911239355802536, Memory (GB): 7\n",
      "Epoch 24, Batch 171, Train Loss: 0.08253898471593857, Memory (GB): 7\n",
      "Epoch 24, Batch 172, Train Loss: 0.09195687621831894, Memory (GB): 7\n",
      "Epoch 24, Batch 173, Train Loss: 0.09706808626651764, Memory (GB): 7\n",
      "Epoch 24, Batch 174, Train Loss: 0.08771965652704239, Memory (GB): 7\n",
      "Epoch 24, Batch 175, Train Loss: 0.09425794333219528, Memory (GB): 7\n",
      "Epoch 24, Batch 176, Train Loss: 0.10499399900436401, Memory (GB): 7\n",
      "Epoch 24, Batch 177, Train Loss: 0.09717055410146713, Memory (GB): 7\n",
      "Epoch 24, Batch 178, Train Loss: 0.09956641495227814, Memory (GB): 7\n",
      "Epoch 24, Batch 179, Train Loss: 0.09902726113796234, Memory (GB): 7\n",
      "[0.0009999999999998318, 0.0009499999999998391, 0.0009024999999998473, 0.0008573749999998554, 0.0008145062499998622, 0.0007737809374998688, 0.0007350918906248758, 0.000698337296093632, 0.0006634204312889498, 0.0006302494097245026, 0.0005987369392382772, 0.0005688000922763637, 0.0005403600876625454, 0.000513342083279418, 0.0004876749791154469, 0.00046329123015967475, 0.0004401266686516907, 0.0004181203352191062, 0.0003972143184581509, 0.00037735360253524325, 0.0003584859224084812, 0.00034056162628805714, 0.00032353354497365424, 0.0003073568677249716, 0.00029198902433872297, 0.0002773895731217867, 0.00026352009446569747, 0.0002503440897424125, 0.00023782688525529196, 0.00022593554099252717, 0.000214638763942901, 0.00020390682574575585, 0.00019371148445846792, 0.0001840259102355446, 0.00017482461472376749, 0.00016608338398757899, 0.0001577792147882001, 0.00014989025404879008, 0.00014239574134635054, 0.00013527595427903299, 0.00012851215656508138, 0.0001220865487368273, 0.00011598222129998584, 0.00011018311023498656, 0.00010467395472323728, 9.944025698707539e-05, 9.446824413772159e-05, 8.974483193083549e-05, 8.525759033429376e-05, 8.099471081757905e-05, 7.694497527670013e-05, 7.30977265128651e-05, 6.944284018722184e-05, 6.597069817786072e-05, 6.267216326896771e-05, 5.953855510551934e-05, 5.6561627350243344e-05, 5.373354598273118e-05, 5.104686868359459e-05, 4.8494525249414906e-05, 4.606979898694414e-05, 4.376630903759692e-05, 4.157799358571704e-05, 3.94990939064312e-05, 3.752413921110963e-05, 3.564793225055418e-05, 3.3865535638026455e-05, 3.217225885612513e-05, 3.0563645913318876e-05, 2.9035463617652914e-05, 2.7583690436770263e-05, 2.620450591493176e-05, 2.4894280619185186e-05, 2.364956658822593e-05, 2.2467088258814625e-05, 2.1343733845873878e-05, 2.027654715358019e-05, 1.926271979590118e-05, 1.8299583806106128e-05, 1.7384604615800824e-05, 1.6515374385010783e-05, 1.5689605665760246e-05, 1.4905125382472224e-05, 1.4159869113348604e-05, 1.3451875657681174e-05, 1.2779281874797123e-05, 1.214031778105727e-05, 1.1533301892004396e-05, 1.095663679740418e-05, 1.0408804957533972e-05, 9.888364709657275e-06, 9.393946474174408e-06, 8.924249150465687e-06, 8.478036692942405e-06, 8.05413485829529e-06, 7.65142811538052e-06, 7.268856709611491e-06, 6.90541387413092e-06, 6.560143180424371e-06, 6.2321360214031505e-06, 5.920529220332995e-06, 5.624502759316344e-06, 5.3432776213505295e-06, 5.076113740283001e-06, 4.82230805326885e-06, 4.581192650605406e-06, 4.35213301807514e-06, 4.134526367171379e-06, 3.92780004881281e-06, 3.7314100463721717e-06, 3.544839544053563e-06, 3.3675975668508836e-06, 3.199217688508339e-06, 3.0392568040829193e-06, 2.887293963878777e-06, 2.7429292656848365e-06, 2.605782802400596e-06, 2.475493662280563e-06, 2.3517189791665358e-06, 2.2341330302082084e-06, 2.1224263786977993e-06, 2.0163050597629093e-06, 1.9154898067747635e-06, 1.819715316436026e-06, 1.7287295506142233e-06, 1.6422930730835125e-06, 1.5601784194293367e-06, 1.4821694984578711e-06, 1.4080610235349764e-06, 1.337657972358228e-06, 1.2707750737403162e-06, 1.2072363200533007e-06, 1.1468745040506357e-06, 1.089530778848104e-06, 1.0350542399056982e-06, 9.83301527910413e-07, 9.34136451514893e-07, 8.874296289391482e-07, 8.430581474921906e-07, 8.00905240117581e-07, 7.608599781117021e-07, 7.228169792061166e-07, 6.866761302458114e-07, 6.523423237335204e-07, 6.197252075468441e-07, 5.887389471695021e-07, 5.593019998110269e-07, 5.313368998204754e-07, 5.047700548294517e-07, 4.795315520879789e-07, 4.555549744835801e-07, 4.3277722575940123e-07, 4.11138364471431e-07, 3.905814462478596e-07, 3.710523739354663e-07, 3.524997552386932e-07, 3.348747674767584e-07, 3.181310291029206e-07, 3.022244776477744e-07, 2.8711325376538583e-07, 2.7275759107711617e-07, 2.5911971152326055e-07, 2.4616372594709754e-07, 2.3385553964974264e-07, 2.2216276266725538e-07, 2.110546245338927e-07, 2.0050189330719793e-07, 1.9047679864183803e-07, 1.809529587097461e-07, 1.7190531077425902e-07, 1.6331004523554584e-07, 1.551445429737687e-07, 1.473873158250802e-07, 1.4001795003382614e-07, 1.3301705253213488e-07, 1.263661999055281e-07, 1.2004788991025169e-07, 1.1404549541473912e-07, 1.0834322064400222e-07, 1.0292605961180208e-07, 9.77797566312119e-08, 9.289076879965134e-08, 8.824623035966877e-08, 8.383391884168531e-08, 7.964222289960107e-08, 7.566011175462103e-08, 7.187710616688993e-08, 6.828325085854543e-08, 6.486908831561813e-08, 6.162563389983726e-08, 5.854435220484539e-08, 5.5617134594603124e-08, 5.283627786487293e-08, 5.019446397162932e-08, 4.7684740773047825e-08, 4.530050373439543e-08, 4.3035478547675656e-08, 4.08837046202919e-08, 3.8839519389277295e-08]\n",
      "Epoch 25, Batch 0, Train Loss: 0.09827607125043869, Memory (GB): 7\n",
      "Epoch 25, Batch 1, Train Loss: 0.08501972258090973, Memory (GB): 7\n",
      "Epoch 25, Batch 2, Train Loss: 0.09400302916765213, Memory (GB): 7\n",
      "Epoch 25, Batch 3, Train Loss: 0.08754511177539825, Memory (GB): 7\n",
      "Epoch 25, Batch 4, Train Loss: 0.0825386717915535, Memory (GB): 7\n",
      "Epoch 25, Batch 5, Train Loss: 0.08570549637079239, Memory (GB): 7\n",
      "Epoch 25, Batch 6, Train Loss: 0.09689471870660782, Memory (GB): 7\n",
      "Epoch 25, Batch 7, Train Loss: 0.09495602548122406, Memory (GB): 7\n",
      "Epoch 25, Batch 8, Train Loss: 0.09793049842119217, Memory (GB): 7\n",
      "Epoch 25, Batch 9, Train Loss: 0.09901271760463715, Memory (GB): 7\n",
      "Epoch 25, Batch 10, Train Loss: 0.10594320297241211, Memory (GB): 7\n",
      "Epoch 25, Batch 11, Train Loss: 0.10080575942993164, Memory (GB): 7\n",
      "Epoch 25, Batch 12, Train Loss: 0.0980507954955101, Memory (GB): 7\n",
      "Epoch 25, Batch 13, Train Loss: 0.10385458916425705, Memory (GB): 7\n",
      "Epoch 25, Batch 14, Train Loss: 0.10156182944774628, Memory (GB): 7\n",
      "Epoch 25, Batch 15, Train Loss: 0.10865572839975357, Memory (GB): 7\n",
      "Epoch 25, Batch 16, Train Loss: 0.11546416580677032, Memory (GB): 7\n",
      "Epoch 25, Batch 17, Train Loss: 0.11553177237510681, Memory (GB): 7\n",
      "Epoch 25, Batch 18, Train Loss: 0.12795951962471008, Memory (GB): 7\n",
      "Epoch 25, Batch 19, Train Loss: 0.11758555471897125, Memory (GB): 7\n",
      "Epoch 25, Batch 20, Train Loss: 0.12120696157217026, Memory (GB): 7\n",
      "Epoch 25, Batch 21, Train Loss: 0.11434851586818695, Memory (GB): 7\n",
      "Epoch 25, Batch 22, Train Loss: 0.11305772513151169, Memory (GB): 7\n",
      "Epoch 25, Batch 23, Train Loss: 0.1152871698141098, Memory (GB): 7\n",
      "Epoch 25, Batch 24, Train Loss: 0.11983846127986908, Memory (GB): 7\n",
      "Epoch 25, Batch 25, Train Loss: 0.11374465376138687, Memory (GB): 7\n",
      "Epoch 25, Batch 26, Train Loss: 0.1091022714972496, Memory (GB): 7\n",
      "Epoch 25, Batch 27, Train Loss: 0.10078895837068558, Memory (GB): 7\n",
      "Epoch 25, Batch 28, Train Loss: 0.09677956998348236, Memory (GB): 7\n",
      "Epoch 25, Batch 29, Train Loss: 0.11530028283596039, Memory (GB): 7\n",
      "Epoch 25, Batch 30, Train Loss: 0.13044703006744385, Memory (GB): 7\n",
      "Epoch 25, Batch 31, Train Loss: 0.11133772879838943, Memory (GB): 7\n",
      "Epoch 25, Batch 32, Train Loss: 0.10463462769985199, Memory (GB): 7\n",
      "Epoch 25, Batch 33, Train Loss: 0.10445132851600647, Memory (GB): 7\n",
      "Epoch 25, Batch 34, Train Loss: 0.0966799408197403, Memory (GB): 7\n",
      "Epoch 25, Batch 35, Train Loss: 0.10324081778526306, Memory (GB): 7\n",
      "Epoch 25, Batch 36, Train Loss: 0.09363220632076263, Memory (GB): 7\n",
      "Epoch 25, Batch 37, Train Loss: 0.10708308219909668, Memory (GB): 7\n",
      "Epoch 25, Batch 38, Train Loss: 0.10130824148654938, Memory (GB): 7\n",
      "Epoch 25, Batch 39, Train Loss: 0.09954359382390976, Memory (GB): 7\n",
      "Epoch 25, Batch 40, Train Loss: 0.1045730710029602, Memory (GB): 7\n",
      "Epoch 25, Batch 41, Train Loss: 0.10135771334171295, Memory (GB): 7\n",
      "Epoch 25, Batch 42, Train Loss: 0.11747299134731293, Memory (GB): 7\n",
      "Epoch 25, Batch 43, Train Loss: 0.11000074446201324, Memory (GB): 7\n",
      "Epoch 25, Batch 44, Train Loss: 0.09603327512741089, Memory (GB): 7\n",
      "Epoch 25, Batch 45, Train Loss: 0.09840337187051773, Memory (GB): 7\n",
      "Epoch 25, Batch 46, Train Loss: 0.0934307873249054, Memory (GB): 7\n",
      "Epoch 25, Batch 47, Train Loss: 0.09828684478998184, Memory (GB): 7\n",
      "Epoch 25, Batch 48, Train Loss: 0.09196735918521881, Memory (GB): 7\n",
      "Epoch 25, Batch 49, Train Loss: 0.0978611558675766, Memory (GB): 7\n",
      "Epoch 25, Batch 50, Train Loss: 0.0981474369764328, Memory (GB): 7\n",
      "Epoch 25, Batch 51, Train Loss: 0.10302499681711197, Memory (GB): 7\n",
      "Epoch 25, Batch 52, Train Loss: 0.08837047964334488, Memory (GB): 7\n",
      "Epoch 25, Batch 53, Train Loss: 0.09538901597261429, Memory (GB): 7\n",
      "Epoch 25, Batch 54, Train Loss: 0.10371767729520798, Memory (GB): 7\n",
      "Epoch 25, Batch 55, Train Loss: 0.09599438309669495, Memory (GB): 7\n",
      "Epoch 25, Batch 56, Train Loss: 0.09438131004571915, Memory (GB): 7\n",
      "Epoch 25, Batch 57, Train Loss: 0.08825182169675827, Memory (GB): 7\n",
      "Epoch 25, Batch 58, Train Loss: 0.1002616211771965, Memory (GB): 7\n",
      "Epoch 25, Batch 59, Train Loss: 0.09364377707242966, Memory (GB): 7\n",
      "Epoch 25, Batch 60, Train Loss: 0.09236916899681091, Memory (GB): 7\n",
      "Epoch 25, Batch 61, Train Loss: 0.10069740563631058, Memory (GB): 7\n",
      "Epoch 25, Batch 62, Train Loss: 0.0886436328291893, Memory (GB): 7\n",
      "Epoch 25, Batch 63, Train Loss: 0.09039758890867233, Memory (GB): 7\n",
      "Epoch 25, Batch 64, Train Loss: 0.09590137749910355, Memory (GB): 7\n",
      "Epoch 25, Batch 65, Train Loss: 0.09376810491085052, Memory (GB): 7\n",
      "Epoch 25, Batch 66, Train Loss: 0.0953335091471672, Memory (GB): 7\n",
      "Epoch 25, Batch 67, Train Loss: 0.07883228361606598, Memory (GB): 7\n",
      "Epoch 25, Batch 68, Train Loss: 0.0835692435503006, Memory (GB): 7\n",
      "Epoch 25, Batch 69, Train Loss: 0.09325595945119858, Memory (GB): 7\n",
      "Epoch 25, Batch 70, Train Loss: 0.08837814629077911, Memory (GB): 7\n",
      "Epoch 25, Batch 71, Train Loss: 0.09949686378240585, Memory (GB): 7\n",
      "Epoch 25, Batch 72, Train Loss: 0.09083735197782516, Memory (GB): 7\n",
      "Epoch 25, Batch 73, Train Loss: 0.09569519758224487, Memory (GB): 7\n",
      "Epoch 25, Batch 74, Train Loss: 0.10213411599397659, Memory (GB): 7\n",
      "Epoch 25, Batch 75, Train Loss: 0.08271010965108871, Memory (GB): 7\n",
      "Epoch 25, Batch 76, Train Loss: 0.07852920144796371, Memory (GB): 7\n",
      "Epoch 25, Batch 77, Train Loss: 0.098782017827034, Memory (GB): 7\n",
      "Epoch 25, Batch 78, Train Loss: 0.08781730383634567, Memory (GB): 7\n",
      "Epoch 25, Batch 79, Train Loss: 0.08676372468471527, Memory (GB): 7\n",
      "Epoch 25, Batch 80, Train Loss: 0.08421670645475388, Memory (GB): 7\n",
      "Epoch 25, Batch 81, Train Loss: 0.08644077926874161, Memory (GB): 7\n",
      "Epoch 25, Batch 82, Train Loss: 0.10212639719247818, Memory (GB): 7\n",
      "Epoch 25, Batch 83, Train Loss: 0.10122723877429962, Memory (GB): 7\n",
      "Epoch 25, Batch 84, Train Loss: 0.09623926877975464, Memory (GB): 7\n",
      "Epoch 25, Batch 85, Train Loss: 0.0851154550909996, Memory (GB): 7\n",
      "Epoch 25, Batch 86, Train Loss: 0.08765925467014313, Memory (GB): 7\n",
      "Epoch 25, Batch 87, Train Loss: 0.09425035864114761, Memory (GB): 7\n",
      "Epoch 25, Batch 88, Train Loss: 0.09181246161460876, Memory (GB): 7\n",
      "Epoch 25, Batch 89, Train Loss: 0.09250012785196304, Memory (GB): 7\n",
      "Epoch 25, Batch 90, Train Loss: 0.08217690140008926, Memory (GB): 7\n",
      "Epoch 25, Batch 91, Train Loss: 0.08614977449178696, Memory (GB): 7\n",
      "Epoch 25, Batch 92, Train Loss: 0.08478407561779022, Memory (GB): 7\n",
      "Epoch 25, Batch 93, Train Loss: 0.09849578887224197, Memory (GB): 7\n",
      "Epoch 25, Batch 94, Train Loss: 0.09451297670602798, Memory (GB): 7\n",
      "Epoch 25, Batch 95, Train Loss: 0.08698908239603043, Memory (GB): 7\n",
      "Epoch 25, Batch 96, Train Loss: 0.09880872815847397, Memory (GB): 7\n",
      "Epoch 25, Batch 97, Train Loss: 0.09703631699085236, Memory (GB): 7\n",
      "Epoch 25, Batch 98, Train Loss: 0.09810564666986465, Memory (GB): 7\n",
      "Epoch 25, Batch 99, Train Loss: 0.09482232481241226, Memory (GB): 7\n",
      "Epoch 25, Batch 100, Train Loss: 0.09167846292257309, Memory (GB): 7\n",
      "Epoch 25, Batch 101, Train Loss: 0.09191344678401947, Memory (GB): 7\n",
      "Epoch 25, Batch 102, Train Loss: 0.10010765492916107, Memory (GB): 7\n",
      "Epoch 25, Batch 103, Train Loss: 0.08778025954961777, Memory (GB): 7\n",
      "Epoch 25, Batch 104, Train Loss: 0.10546652227640152, Memory (GB): 7\n",
      "Epoch 25, Batch 105, Train Loss: 0.0934901088476181, Memory (GB): 7\n",
      "Epoch 25, Batch 106, Train Loss: 0.08212622255086899, Memory (GB): 7\n",
      "Epoch 25, Batch 107, Train Loss: 0.09942243248224258, Memory (GB): 7\n",
      "Epoch 25, Batch 108, Train Loss: 0.09972035139799118, Memory (GB): 7\n",
      "Epoch 25, Batch 109, Train Loss: 0.11124825477600098, Memory (GB): 7\n",
      "Epoch 25, Batch 110, Train Loss: 0.0864725112915039, Memory (GB): 7\n",
      "Epoch 25, Batch 111, Train Loss: 0.10239353775978088, Memory (GB): 7\n",
      "Epoch 25, Batch 112, Train Loss: 0.09344308078289032, Memory (GB): 7\n",
      "Epoch 25, Batch 113, Train Loss: 0.10789114981889725, Memory (GB): 7\n",
      "Epoch 25, Batch 114, Train Loss: 0.09223059564828873, Memory (GB): 7\n",
      "Epoch 25, Batch 115, Train Loss: 0.10901852697134018, Memory (GB): 7\n",
      "Epoch 25, Batch 116, Train Loss: 0.08936909586191177, Memory (GB): 7\n",
      "Epoch 25, Batch 117, Train Loss: 0.09613689035177231, Memory (GB): 7\n",
      "Epoch 25, Batch 118, Train Loss: 0.09931105375289917, Memory (GB): 7\n",
      "Epoch 25, Batch 119, Train Loss: 0.09758377075195312, Memory (GB): 7\n",
      "Epoch 25, Batch 120, Train Loss: 0.09855304658412933, Memory (GB): 7\n",
      "Epoch 25, Batch 121, Train Loss: 0.09685250371694565, Memory (GB): 7\n",
      "Epoch 25, Batch 122, Train Loss: 0.10350275039672852, Memory (GB): 7\n",
      "Epoch 25, Batch 123, Train Loss: 0.10621335357427597, Memory (GB): 7\n",
      "Epoch 25, Batch 124, Train Loss: 0.09079718589782715, Memory (GB): 7\n",
      "Epoch 25, Batch 125, Train Loss: 0.09946423768997192, Memory (GB): 7\n",
      "Epoch 25, Batch 126, Train Loss: 0.09958429634571075, Memory (GB): 7\n",
      "Epoch 25, Batch 127, Train Loss: 0.08660843968391418, Memory (GB): 7\n",
      "Epoch 25, Batch 128, Train Loss: 0.09422332048416138, Memory (GB): 7\n",
      "Epoch 25, Batch 129, Train Loss: 0.08591613173484802, Memory (GB): 7\n",
      "Epoch 25, Batch 130, Train Loss: 0.09092991799116135, Memory (GB): 7\n",
      "Epoch 25, Batch 131, Train Loss: 0.09041595458984375, Memory (GB): 7\n",
      "Epoch 25, Batch 132, Train Loss: 0.09230194985866547, Memory (GB): 7\n",
      "Epoch 25, Batch 133, Train Loss: 0.09213697165250778, Memory (GB): 7\n",
      "Epoch 25, Batch 134, Train Loss: 0.09818165749311447, Memory (GB): 7\n",
      "Epoch 25, Batch 135, Train Loss: 0.08301804959774017, Memory (GB): 7\n",
      "Epoch 25, Batch 136, Train Loss: 0.08848901838064194, Memory (GB): 7\n",
      "Epoch 25, Batch 137, Train Loss: 0.08565010875463486, Memory (GB): 7\n",
      "Epoch 25, Batch 138, Train Loss: 0.07972461730241776, Memory (GB): 7\n",
      "Epoch 25, Batch 139, Train Loss: 0.08779622614383698, Memory (GB): 7\n",
      "Epoch 25, Batch 140, Train Loss: 0.07733992487192154, Memory (GB): 7\n",
      "Epoch 25, Batch 141, Train Loss: 0.08437541872262955, Memory (GB): 7\n",
      "Epoch 25, Batch 142, Train Loss: 0.10439734160900116, Memory (GB): 7\n",
      "Epoch 25, Batch 143, Train Loss: 0.0893467515707016, Memory (GB): 7\n",
      "Epoch 25, Batch 144, Train Loss: 0.08507777750492096, Memory (GB): 7\n",
      "Epoch 25, Batch 145, Train Loss: 0.08127345144748688, Memory (GB): 7\n",
      "Epoch 25, Batch 146, Train Loss: 0.08607129007577896, Memory (GB): 7\n",
      "Epoch 25, Batch 147, Train Loss: 0.09052059799432755, Memory (GB): 7\n",
      "Epoch 25, Batch 148, Train Loss: 0.07907423377037048, Memory (GB): 7\n",
      "Epoch 25, Batch 149, Train Loss: 0.08154817670583725, Memory (GB): 7\n",
      "Epoch 25, Batch 150, Train Loss: 0.09533771127462387, Memory (GB): 7\n",
      "Epoch 25, Batch 151, Train Loss: 0.1030934751033783, Memory (GB): 7\n",
      "Epoch 25, Batch 152, Train Loss: 0.085225909948349, Memory (GB): 7\n",
      "Epoch 25, Batch 153, Train Loss: 0.0800296887755394, Memory (GB): 7\n",
      "Epoch 25, Batch 154, Train Loss: 0.07473498582839966, Memory (GB): 7\n",
      "Epoch 25, Batch 155, Train Loss: 0.09790149331092834, Memory (GB): 7\n",
      "Epoch 25, Batch 156, Train Loss: 0.09801028668880463, Memory (GB): 7\n",
      "Epoch 25, Batch 157, Train Loss: 0.07983358204364777, Memory (GB): 7\n",
      "Epoch 25, Batch 158, Train Loss: 0.08655998110771179, Memory (GB): 7\n",
      "Epoch 25, Batch 159, Train Loss: 0.0778069943189621, Memory (GB): 7\n",
      "Epoch 25, Batch 160, Train Loss: 0.08796931803226471, Memory (GB): 7\n",
      "Epoch 25, Batch 161, Train Loss: 0.07328464835882187, Memory (GB): 7\n",
      "Epoch 25, Batch 162, Train Loss: 0.07545163482427597, Memory (GB): 7\n",
      "Epoch 25, Batch 163, Train Loss: 0.09491991996765137, Memory (GB): 7\n",
      "Epoch 25, Batch 164, Train Loss: 0.09059643745422363, Memory (GB): 7\n",
      "Epoch 25, Batch 165, Train Loss: 0.0788220465183258, Memory (GB): 7\n",
      "Epoch 25, Batch 166, Train Loss: 0.08226189017295837, Memory (GB): 7\n",
      "Epoch 25, Batch 167, Train Loss: 0.077142134308815, Memory (GB): 7\n",
      "Epoch 25, Batch 168, Train Loss: 0.09217632561922073, Memory (GB): 7\n",
      "Epoch 25, Batch 169, Train Loss: 0.08653723448514938, Memory (GB): 7\n",
      "Epoch 25, Batch 170, Train Loss: 0.09655950218439102, Memory (GB): 7\n",
      "Epoch 25, Batch 171, Train Loss: 0.07919970154762268, Memory (GB): 7\n",
      "Epoch 25, Batch 172, Train Loss: 0.08293280005455017, Memory (GB): 7\n",
      "Epoch 25, Batch 173, Train Loss: 0.07744888216257095, Memory (GB): 7\n",
      "Epoch 25, Batch 174, Train Loss: 0.09501120448112488, Memory (GB): 7\n",
      "Epoch 25, Batch 175, Train Loss: 0.08167245984077454, Memory (GB): 7\n",
      "Epoch 25, Batch 176, Train Loss: 0.07562768459320068, Memory (GB): 7\n",
      "Epoch 25, Batch 177, Train Loss: 0.07529046386480331, Memory (GB): 7\n",
      "Epoch 25, Batch 178, Train Loss: 0.06903906911611557, Memory (GB): 7\n",
      "Epoch 25, Batch 179, Train Loss: 0.09122545272111893, Memory (GB): 7\n",
      "[0.0006545084971876434, 0.0006217830723282611, 0.0005906939187118481, 0.0005611592227762559, 0.0005331012616374428, 0.0005064461985555709, 0.0004811238886277921, 0.0004570676941964024, 0.0004342143094865823, 0.00041250359401225295, 0.00039187841431164046, 0.0003722844935960583, 0.0003536702689162555, 0.0003359867554704428, 0.00031918741769692064, 0.0003032280468120747, 0.00028806664447147067, 0.00027366331224789735, 0.00025998014663550245, 0.00024698113930372726, 0.0002346320823385409, 0.00022290047822161386, 0.00021175545431053314, 0.00020116768159500644, 0.00019110929751525602, 0.00018155383263949326, 0.00017247614100751853, 0.00016385233395714265, 0.00015565971725928546, 0.00014787673139632132, 0.0001404828948265052, 0.0001334587500851799, 0.00012678581258092087, 0.00012044652195187479, 0.0001144241958542811, 0.00010870298606156701, 0.0001032678367584887, 9.810444492056421e-05, 9.3199222674536e-05, 8.853926154080922e-05, 8.411229846376872e-05, 7.990668354058034e-05, 7.591134936355128e-05, 7.211578189537372e-05, 6.850999280060501e-05, 6.508449316057476e-05, 6.183026850254601e-05, 5.873875507741872e-05, 5.580181732354777e-05, 5.3011726457370406e-05, 5.036114013450186e-05, 4.784308312777677e-05, 4.545092897138795e-05, 4.317838252281854e-05, 4.101946339667762e-05, 3.896849022684371e-05, 3.702006571550155e-05, 3.5169062429726436e-05, 3.341060930824015e-05, 3.174007884282813e-05, 3.0153074900686708e-05, 2.8645421155652383e-05, 2.7213150097869774e-05, 2.5852492592976265e-05, 2.4559867963327442e-05, 2.3331874565161072e-05, 2.2165280836903017e-05, 2.105701679505787e-05, 2.0004165955304966e-05, 1.900395765753972e-05, 1.805375977466273e-05, 1.7151071785929603e-05, 1.6293518196633124e-05, 1.547884228680147e-05, 1.4704900172461387e-05, 1.3969655163838315e-05, 1.3271172405646394e-05, 1.2607613785364075e-05, 1.1977233096095879e-05, 1.137837144129108e-05, 1.0809452869226524e-05, 1.02689802257652e-05, 9.755531214476941e-06, 9.267754653753095e-06, 8.804366921065436e-06, 8.364148575012167e-06, 7.945941146261557e-06, 7.548644088948483e-06, 7.171211884501059e-06, 6.812651290276002e-06, 6.472018725762204e-06, 6.148417789474093e-06, 5.840996900000386e-06, 5.54894705500037e-06, 5.271499702250351e-06, 5.007924717137832e-06, 4.7575284812809406e-06, 4.519652057216895e-06, 4.2936694543560465e-06, 4.078985981638248e-06, 3.875036682556332e-06, 3.6812848484285164e-06, 3.497220606007091e-06, 3.3223595757067365e-06, 3.156241596921399e-06, 2.99842951707533e-06, 2.8485080412215632e-06, 2.7060826391604836e-06, 2.57077850720246e-06, 2.442239581842337e-06, 2.3201276027502183e-06, 2.2041212226127092e-06, 2.0939151614820734e-06, 1.9892194034079693e-06, 1.8897584332375713e-06, 1.7952705115756917e-06, 1.7055069859969076e-06, 1.6202316366970617e-06, 1.5392200548622092e-06, 1.4622590521190982e-06, 1.3891460995131434e-06, 1.3196887945374857e-06, 1.2537043548106119e-06, 1.191019137070081e-06, 1.1314681802165775e-06, 1.0748947712057482e-06, 1.0211500326454601e-06, 9.700925310131876e-07, 9.215879044625283e-07, 8.755085092394016e-07, 8.317330837774318e-07, 7.901464295885601e-07, 7.506391081091321e-07, 7.131071527036754e-07, 6.774517950684916e-07, 6.43579205315067e-07, 6.114002450493136e-07, 5.808302327968476e-07, 5.517887211570054e-07, 5.241992850991549e-07, 4.979893208441974e-07, 4.7308985480198774e-07, 4.494353620618881e-07, 4.269635939587937e-07, 4.05615414260854e-07, 3.8533464354781123e-07, 3.6606791137042057e-07, 3.477645158018997e-07, 3.303762900118046e-07, 3.138574755112142e-07, 2.9816460173565376e-07, 2.8325637164887094e-07, 2.6909355306642745e-07, 2.5563887541310594e-07, 2.428569316424507e-07, 2.3071408506032815e-07, 2.1917838080731164e-07, 2.082194617669461e-07, 1.9780848867859874e-07, 1.8791806424466882e-07, 1.7852216103243532e-07, 1.6959605298081363e-07, 1.6111625033177294e-07, 1.5306043781518418e-07, 1.45407415924425e-07, 1.3813704512820372e-07, 1.3123019287179355e-07, 1.2466868322820392e-07, 1.1843524906679367e-07, 1.1251348661345394e-07, 1.0688781228278124e-07, 1.015434216686422e-07, 9.64662505852101e-08, 9.164293805594958e-08, 8.706079115315213e-08, 8.27077515954945e-08, 7.857236401571977e-08, 7.464374581493379e-08, 7.091155852418706e-08, 6.736598059797776e-08, 6.399768156807885e-08, 6.07977974896749e-08, 5.775790761519115e-08, 5.487001223443159e-08, 5.212651162271e-08, 4.9520186041574524e-08, 4.704417673949577e-08, 4.469196790252096e-08, 4.2457369507394946e-08, 4.0334501032025184e-08, 3.831777598042392e-08, 3.640188718140273e-08, 3.45817928223326e-08, 3.285270318121595e-08, 3.121006802215515e-08, 2.9649564621047383e-08, 2.8167086389995023e-08, 2.6758732070495262e-08, 2.5420795466970505e-08]\n",
      "Epoch 26, Batch 0, Train Loss: 0.06526824086904526, Memory (GB): 7\n",
      "Epoch 26, Batch 1, Train Loss: 0.09430710971355438, Memory (GB): 7\n",
      "Epoch 26, Batch 2, Train Loss: 0.07894433289766312, Memory (GB): 7\n",
      "Epoch 26, Batch 3, Train Loss: 0.08133258670568466, Memory (GB): 7\n",
      "Epoch 26, Batch 4, Train Loss: 0.0768270269036293, Memory (GB): 7\n",
      "Epoch 26, Batch 5, Train Loss: 0.08720429986715317, Memory (GB): 7\n",
      "Epoch 26, Batch 6, Train Loss: 0.07672589272260666, Memory (GB): 7\n",
      "Epoch 26, Batch 7, Train Loss: 0.07503567636013031, Memory (GB): 7\n",
      "Epoch 26, Batch 8, Train Loss: 0.0810651108622551, Memory (GB): 7\n",
      "Epoch 26, Batch 9, Train Loss: 0.08928330987691879, Memory (GB): 7\n",
      "Epoch 26, Batch 10, Train Loss: 0.07378556579351425, Memory (GB): 7\n",
      "Epoch 26, Batch 11, Train Loss: 0.08307681977748871, Memory (GB): 7\n",
      "Epoch 26, Batch 12, Train Loss: 0.07517360150814056, Memory (GB): 7\n",
      "Epoch 26, Batch 13, Train Loss: 0.08447293192148209, Memory (GB): 7\n",
      "Epoch 26, Batch 14, Train Loss: 0.08237254619598389, Memory (GB): 7\n",
      "Epoch 26, Batch 15, Train Loss: 0.07923771440982819, Memory (GB): 7\n",
      "Epoch 26, Batch 16, Train Loss: 0.08323962241411209, Memory (GB): 7\n",
      "Epoch 26, Batch 17, Train Loss: 0.07516776025295258, Memory (GB): 7\n",
      "Epoch 26, Batch 18, Train Loss: 0.07749880850315094, Memory (GB): 7\n",
      "Epoch 26, Batch 19, Train Loss: 0.07913489639759064, Memory (GB): 7\n",
      "Epoch 26, Batch 20, Train Loss: 0.08912774920463562, Memory (GB): 7\n",
      "Epoch 26, Batch 21, Train Loss: 0.07899107038974762, Memory (GB): 7\n",
      "Epoch 26, Batch 22, Train Loss: 0.08353354781866074, Memory (GB): 7\n",
      "Epoch 26, Batch 23, Train Loss: 0.08986297994852066, Memory (GB): 7\n",
      "Epoch 26, Batch 24, Train Loss: 0.08256836235523224, Memory (GB): 7\n",
      "Epoch 26, Batch 25, Train Loss: 0.08463180810213089, Memory (GB): 7\n",
      "Epoch 26, Batch 26, Train Loss: 0.09625523537397385, Memory (GB): 7\n",
      "Epoch 26, Batch 27, Train Loss: 0.07488604635000229, Memory (GB): 7\n",
      "Epoch 26, Batch 28, Train Loss: 0.0796251967549324, Memory (GB): 7\n",
      "Epoch 26, Batch 29, Train Loss: 0.09827166795730591, Memory (GB): 7\n",
      "Epoch 26, Batch 30, Train Loss: 0.08873637020587921, Memory (GB): 7\n",
      "Epoch 26, Batch 31, Train Loss: 0.08393166214227676, Memory (GB): 7\n",
      "Epoch 26, Batch 32, Train Loss: 0.08637762814760208, Memory (GB): 7\n",
      "Epoch 26, Batch 33, Train Loss: 0.08052271604537964, Memory (GB): 7\n",
      "Epoch 26, Batch 34, Train Loss: 0.08545181155204773, Memory (GB): 7\n",
      "Epoch 26, Batch 35, Train Loss: 0.08111454546451569, Memory (GB): 7\n",
      "Epoch 26, Batch 36, Train Loss: 0.07515379786491394, Memory (GB): 7\n",
      "Epoch 26, Batch 37, Train Loss: 0.08994980156421661, Memory (GB): 7\n",
      "Epoch 26, Batch 38, Train Loss: 0.08790723234415054, Memory (GB): 7\n",
      "Epoch 26, Batch 39, Train Loss: 0.08050396293401718, Memory (GB): 7\n",
      "Epoch 26, Batch 40, Train Loss: 0.09628047049045563, Memory (GB): 7\n",
      "Epoch 26, Batch 41, Train Loss: 0.07997936010360718, Memory (GB): 7\n",
      "Epoch 26, Batch 42, Train Loss: 0.0809558555483818, Memory (GB): 7\n",
      "Epoch 26, Batch 43, Train Loss: 0.08753614127635956, Memory (GB): 7\n",
      "Epoch 26, Batch 44, Train Loss: 0.0729668140411377, Memory (GB): 7\n",
      "Epoch 26, Batch 45, Train Loss: 0.0873049721121788, Memory (GB): 7\n",
      "Epoch 26, Batch 46, Train Loss: 0.08509260416030884, Memory (GB): 7\n",
      "Epoch 26, Batch 47, Train Loss: 0.0870681032538414, Memory (GB): 7\n",
      "Epoch 26, Batch 48, Train Loss: 0.08290790021419525, Memory (GB): 7\n",
      "Epoch 26, Batch 49, Train Loss: 0.07435036450624466, Memory (GB): 7\n",
      "Epoch 26, Batch 50, Train Loss: 0.0785541832447052, Memory (GB): 7\n",
      "Epoch 26, Batch 51, Train Loss: 0.09067748486995697, Memory (GB): 7\n",
      "Epoch 26, Batch 52, Train Loss: 0.09859590232372284, Memory (GB): 7\n",
      "Epoch 26, Batch 53, Train Loss: 0.07054129987955093, Memory (GB): 7\n",
      "Epoch 26, Batch 54, Train Loss: 0.07398082315921783, Memory (GB): 7\n",
      "Epoch 26, Batch 55, Train Loss: 0.08626694977283478, Memory (GB): 7\n",
      "Epoch 26, Batch 56, Train Loss: 0.09687629342079163, Memory (GB): 7\n",
      "Epoch 26, Batch 57, Train Loss: 0.0857115164399147, Memory (GB): 7\n",
      "Epoch 26, Batch 58, Train Loss: 0.07856516540050507, Memory (GB): 7\n",
      "Epoch 26, Batch 59, Train Loss: 0.07539673894643784, Memory (GB): 7\n",
      "Epoch 26, Batch 60, Train Loss: 0.07398521155118942, Memory (GB): 7\n",
      "Epoch 26, Batch 61, Train Loss: 0.07995898276567459, Memory (GB): 7\n",
      "Epoch 26, Batch 62, Train Loss: 0.07928840816020966, Memory (GB): 7\n",
      "Epoch 26, Batch 63, Train Loss: 0.07928987592458725, Memory (GB): 7\n",
      "Epoch 26, Batch 64, Train Loss: 0.07257388532161713, Memory (GB): 7\n",
      "Epoch 26, Batch 65, Train Loss: 0.08460292220115662, Memory (GB): 7\n",
      "Epoch 26, Batch 66, Train Loss: 0.07386861741542816, Memory (GB): 7\n",
      "Epoch 26, Batch 67, Train Loss: 0.07625929266214371, Memory (GB): 7\n",
      "Epoch 26, Batch 68, Train Loss: 0.07675281912088394, Memory (GB): 7\n",
      "Epoch 26, Batch 69, Train Loss: 0.08358825743198395, Memory (GB): 7\n",
      "Epoch 26, Batch 70, Train Loss: 0.08687035739421844, Memory (GB): 7\n",
      "Epoch 26, Batch 71, Train Loss: 0.07563475519418716, Memory (GB): 7\n",
      "Epoch 26, Batch 72, Train Loss: 0.07657261937856674, Memory (GB): 7\n",
      "Epoch 26, Batch 73, Train Loss: 0.07929845154285431, Memory (GB): 7\n",
      "Epoch 26, Batch 74, Train Loss: 0.08013094961643219, Memory (GB): 7\n",
      "Epoch 26, Batch 75, Train Loss: 0.07562769949436188, Memory (GB): 7\n",
      "Epoch 26, Batch 76, Train Loss: 0.08122408390045166, Memory (GB): 7\n",
      "Epoch 26, Batch 77, Train Loss: 0.08595269173383713, Memory (GB): 7\n",
      "Epoch 26, Batch 78, Train Loss: 0.0714288130402565, Memory (GB): 7\n",
      "Epoch 26, Batch 79, Train Loss: 0.08208213746547699, Memory (GB): 7\n",
      "Epoch 26, Batch 80, Train Loss: 0.06854435056447983, Memory (GB): 7\n",
      "Epoch 26, Batch 81, Train Loss: 0.07164214551448822, Memory (GB): 7\n",
      "Epoch 26, Batch 82, Train Loss: 0.06762560456991196, Memory (GB): 7\n",
      "Epoch 26, Batch 83, Train Loss: 0.07749733328819275, Memory (GB): 7\n",
      "Epoch 26, Batch 84, Train Loss: 0.07473081350326538, Memory (GB): 7\n",
      "Epoch 26, Batch 85, Train Loss: 0.072764553129673, Memory (GB): 7\n",
      "Epoch 26, Batch 86, Train Loss: 0.0799819827079773, Memory (GB): 7\n",
      "Epoch 26, Batch 87, Train Loss: 0.07225237041711807, Memory (GB): 7\n",
      "Epoch 26, Batch 88, Train Loss: 0.07552318274974823, Memory (GB): 7\n",
      "Epoch 26, Batch 89, Train Loss: 0.06776592880487442, Memory (GB): 7\n",
      "Epoch 26, Batch 90, Train Loss: 0.06744927167892456, Memory (GB): 7\n",
      "Epoch 26, Batch 91, Train Loss: 0.081806480884552, Memory (GB): 7\n",
      "Epoch 26, Batch 92, Train Loss: 0.06911423802375793, Memory (GB): 7\n",
      "Epoch 26, Batch 93, Train Loss: 0.07582484930753708, Memory (GB): 7\n",
      "Epoch 26, Batch 94, Train Loss: 0.08395183086395264, Memory (GB): 7\n",
      "Epoch 26, Batch 95, Train Loss: 0.08320830762386322, Memory (GB): 7\n",
      "Epoch 26, Batch 96, Train Loss: 0.07844545692205429, Memory (GB): 7\n",
      "Epoch 26, Batch 97, Train Loss: 0.08080337196588516, Memory (GB): 7\n",
      "Epoch 26, Batch 98, Train Loss: 0.07197509706020355, Memory (GB): 7\n",
      "Epoch 26, Batch 99, Train Loss: 0.07659249007701874, Memory (GB): 7\n",
      "Epoch 26, Batch 100, Train Loss: 0.07376406341791153, Memory (GB): 7\n",
      "Epoch 26, Batch 101, Train Loss: 0.07999582588672638, Memory (GB): 7\n",
      "Epoch 26, Batch 102, Train Loss: 0.0781451091170311, Memory (GB): 7\n",
      "Epoch 26, Batch 103, Train Loss: 0.07954779267311096, Memory (GB): 7\n",
      "Epoch 26, Batch 104, Train Loss: 0.0884377509355545, Memory (GB): 7\n",
      "Epoch 26, Batch 105, Train Loss: 0.07504736632108688, Memory (GB): 7\n",
      "Epoch 26, Batch 106, Train Loss: 0.07520449906587601, Memory (GB): 7\n",
      "Epoch 26, Batch 107, Train Loss: 0.08739981800317764, Memory (GB): 7\n",
      "Epoch 26, Batch 108, Train Loss: 0.07677359879016876, Memory (GB): 7\n",
      "Epoch 26, Batch 109, Train Loss: 0.0880991518497467, Memory (GB): 7\n",
      "Epoch 26, Batch 110, Train Loss: 0.08201389014720917, Memory (GB): 7\n",
      "Epoch 26, Batch 111, Train Loss: 0.0760902389883995, Memory (GB): 7\n",
      "Epoch 26, Batch 112, Train Loss: 0.07396689802408218, Memory (GB): 7\n",
      "Epoch 26, Batch 113, Train Loss: 0.08462940901517868, Memory (GB): 7\n",
      "Epoch 26, Batch 114, Train Loss: 0.08429573476314545, Memory (GB): 7\n",
      "Epoch 26, Batch 115, Train Loss: 0.0837060883641243, Memory (GB): 7\n",
      "Epoch 26, Batch 116, Train Loss: 0.0767064318060875, Memory (GB): 7\n",
      "Epoch 26, Batch 117, Train Loss: 0.07283720374107361, Memory (GB): 7\n",
      "Epoch 26, Batch 118, Train Loss: 0.08380100131034851, Memory (GB): 7\n",
      "Epoch 26, Batch 119, Train Loss: 0.08659356832504272, Memory (GB): 7\n",
      "Epoch 26, Batch 120, Train Loss: 0.09130226075649261, Memory (GB): 7\n",
      "Epoch 26, Batch 121, Train Loss: 0.08035643398761749, Memory (GB): 7\n",
      "Epoch 26, Batch 122, Train Loss: 0.08578511327505112, Memory (GB): 7\n",
      "Epoch 26, Batch 123, Train Loss: 0.11321789771318436, Memory (GB): 7\n",
      "Epoch 26, Batch 124, Train Loss: 0.08744083344936371, Memory (GB): 7\n",
      "Epoch 26, Batch 125, Train Loss: 0.08485206216573715, Memory (GB): 7\n",
      "Epoch 26, Batch 126, Train Loss: 0.08318062126636505, Memory (GB): 7\n",
      "Epoch 26, Batch 127, Train Loss: 0.09100867807865143, Memory (GB): 7\n",
      "Epoch 26, Batch 128, Train Loss: 0.08138206601142883, Memory (GB): 7\n",
      "Epoch 26, Batch 129, Train Loss: 0.08887035399675369, Memory (GB): 7\n",
      "Epoch 26, Batch 130, Train Loss: 0.08922060579061508, Memory (GB): 7\n",
      "Epoch 26, Batch 131, Train Loss: 0.08750028908252716, Memory (GB): 7\n",
      "Epoch 26, Batch 132, Train Loss: 0.0817146748304367, Memory (GB): 7\n",
      "Epoch 26, Batch 133, Train Loss: 0.08269789069890976, Memory (GB): 7\n",
      "Epoch 26, Batch 134, Train Loss: 0.0865456610918045, Memory (GB): 7\n",
      "Epoch 26, Batch 135, Train Loss: 0.08542565256357193, Memory (GB): 7\n",
      "Epoch 26, Batch 136, Train Loss: 0.08324164152145386, Memory (GB): 7\n",
      "Epoch 26, Batch 137, Train Loss: 0.07838156074285507, Memory (GB): 7\n",
      "Epoch 26, Batch 138, Train Loss: 0.08964155614376068, Memory (GB): 7\n",
      "Epoch 26, Batch 139, Train Loss: 0.09361318498849869, Memory (GB): 7\n",
      "Epoch 26, Batch 140, Train Loss: 0.08111413568258286, Memory (GB): 7\n",
      "Epoch 26, Batch 141, Train Loss: 0.09137509763240814, Memory (GB): 7\n",
      "Epoch 26, Batch 142, Train Loss: 0.09055415540933609, Memory (GB): 7\n",
      "Epoch 26, Batch 143, Train Loss: 0.09159570932388306, Memory (GB): 7\n",
      "Epoch 26, Batch 144, Train Loss: 0.09231454879045486, Memory (GB): 7\n",
      "Epoch 26, Batch 145, Train Loss: 0.09192684292793274, Memory (GB): 7\n",
      "Epoch 26, Batch 146, Train Loss: 0.08774949610233307, Memory (GB): 7\n",
      "Epoch 26, Batch 147, Train Loss: 0.07842131704092026, Memory (GB): 7\n",
      "Epoch 26, Batch 148, Train Loss: 0.0929512158036232, Memory (GB): 7\n",
      "Epoch 26, Batch 149, Train Loss: 0.08094663172960281, Memory (GB): 7\n",
      "Epoch 26, Batch 150, Train Loss: 0.08723486214876175, Memory (GB): 7\n",
      "Epoch 26, Batch 151, Train Loss: 0.08125747740268707, Memory (GB): 7\n",
      "Epoch 26, Batch 152, Train Loss: 0.09085230529308319, Memory (GB): 7\n",
      "Epoch 26, Batch 153, Train Loss: 0.08224516361951828, Memory (GB): 7\n",
      "Epoch 26, Batch 154, Train Loss: 0.08096206933259964, Memory (GB): 7\n",
      "Epoch 26, Batch 155, Train Loss: 0.07889622449874878, Memory (GB): 7\n",
      "Epoch 26, Batch 156, Train Loss: 0.0834818109869957, Memory (GB): 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 71\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(model, train, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#scaler.scale(loss).backward()\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#scaler.step(optimizer)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#scaler.update()\u001b[39;00m\n\u001b[0;32m     70\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 71\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warmup_scheduler\u001b[38;5;241m.\u001b[39mdampening():\n\u001b[0;32m     74\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adamw.py:531\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    528\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_params, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[0;32m    534\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_addcmul_(\n\u001b[0;32m    535\u001b[0m     device_exp_avg_sqs, device_grads, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2\n\u001b[0;32m    536\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "fine_tune(model, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e41fef-0798-436e-80ea-b2176bd47995",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"temp_model_parameters29.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "919a9752-1a61-47d2-8af5-05abdd9f2cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebei\\AppData\\Local\\Temp\\ipykernel_20296\\437713335.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 0, Train Loss: 0.061187975108623505, Memory (GB): 7\n",
      "Epoch 30, Batch 1, Train Loss: 0.06359926611185074, Memory (GB): 7\n",
      "Epoch 30, Batch 2, Train Loss: 0.05394243076443672, Memory (GB): 7\n",
      "Epoch 30, Batch 3, Train Loss: 0.06585253775119781, Memory (GB): 7\n",
      "Epoch 30, Batch 4, Train Loss: 0.057372547686100006, Memory (GB): 7\n",
      "Epoch 30, Batch 5, Train Loss: 0.06427426636219025, Memory (GB): 7\n",
      "Epoch 30, Batch 6, Train Loss: 0.055464811623096466, Memory (GB): 7\n",
      "Epoch 30, Batch 7, Train Loss: 0.05922051891684532, Memory (GB): 7\n",
      "Epoch 30, Batch 8, Train Loss: 0.06786184012889862, Memory (GB): 7\n",
      "Epoch 30, Batch 9, Train Loss: 0.06415615975856781, Memory (GB): 7\n",
      "Epoch 30, Batch 10, Train Loss: 0.07264306396245956, Memory (GB): 7\n",
      "Epoch 30, Batch 11, Train Loss: 0.058965977281332016, Memory (GB): 7\n",
      "Epoch 30, Batch 12, Train Loss: 0.06786637008190155, Memory (GB): 7\n",
      "Epoch 30, Batch 13, Train Loss: 0.05905142426490784, Memory (GB): 7\n",
      "Epoch 30, Batch 14, Train Loss: 0.05926058441400528, Memory (GB): 7\n",
      "Epoch 30, Batch 15, Train Loss: 0.05632343888282776, Memory (GB): 7\n",
      "Epoch 30, Batch 16, Train Loss: 0.06584202498197556, Memory (GB): 7\n",
      "Epoch 30, Batch 17, Train Loss: 0.05978696048259735, Memory (GB): 7\n",
      "Epoch 30, Batch 18, Train Loss: 0.04831116646528244, Memory (GB): 7\n",
      "Epoch 30, Batch 19, Train Loss: 0.0676761269569397, Memory (GB): 7\n",
      "Epoch 30, Batch 20, Train Loss: 0.059553034603595734, Memory (GB): 7\n",
      "Epoch 30, Batch 21, Train Loss: 0.066217802464962, Memory (GB): 7\n",
      "Epoch 30, Batch 22, Train Loss: 0.06889481842517853, Memory (GB): 7\n",
      "Epoch 30, Batch 23, Train Loss: 0.056881316006183624, Memory (GB): 7\n",
      "Epoch 30, Batch 24, Train Loss: 0.06176824867725372, Memory (GB): 7\n",
      "Epoch 30, Batch 25, Train Loss: 0.058977287262678146, Memory (GB): 7\n",
      "Epoch 30, Batch 26, Train Loss: 0.06536442041397095, Memory (GB): 7\n",
      "Epoch 30, Batch 27, Train Loss: 0.05664531886577606, Memory (GB): 7\n",
      "Epoch 30, Batch 28, Train Loss: 0.07057712227106094, Memory (GB): 7\n",
      "Epoch 30, Batch 29, Train Loss: 0.056255340576171875, Memory (GB): 7\n",
      "Epoch 30, Batch 30, Train Loss: 0.05743025988340378, Memory (GB): 7\n",
      "Epoch 30, Batch 31, Train Loss: 0.07238622009754181, Memory (GB): 7\n",
      "Epoch 30, Batch 32, Train Loss: 0.05913149565458298, Memory (GB): 7\n",
      "Epoch 30, Batch 33, Train Loss: 0.06919564306735992, Memory (GB): 7\n",
      "Epoch 30, Batch 34, Train Loss: 0.06419102102518082, Memory (GB): 7\n",
      "Epoch 30, Batch 35, Train Loss: 0.06045014411211014, Memory (GB): 7\n",
      "Epoch 30, Batch 36, Train Loss: 0.061608608812093735, Memory (GB): 7\n",
      "Epoch 30, Batch 37, Train Loss: 0.06204964965581894, Memory (GB): 7\n",
      "Epoch 30, Batch 38, Train Loss: 0.06542914360761642, Memory (GB): 7\n",
      "Epoch 30, Batch 39, Train Loss: 0.06334871053695679, Memory (GB): 7\n",
      "Epoch 30, Batch 40, Train Loss: 0.05867643654346466, Memory (GB): 7\n",
      "Epoch 30, Batch 41, Train Loss: 0.05597735568881035, Memory (GB): 7\n",
      "Epoch 30, Batch 42, Train Loss: 0.06275583058595657, Memory (GB): 7\n",
      "Epoch 30, Batch 43, Train Loss: 0.05403321608901024, Memory (GB): 7\n",
      "Epoch 30, Batch 44, Train Loss: 0.06331808865070343, Memory (GB): 7\n",
      "Epoch 30, Batch 45, Train Loss: 0.06492003053426743, Memory (GB): 7\n",
      "Epoch 30, Batch 46, Train Loss: 0.056077390909194946, Memory (GB): 7\n",
      "Epoch 30, Batch 47, Train Loss: 0.06910938024520874, Memory (GB): 7\n",
      "Epoch 30, Batch 48, Train Loss: 0.054470550268888474, Memory (GB): 7\n",
      "Epoch 30, Batch 49, Train Loss: 0.05738444626331329, Memory (GB): 7\n",
      "Epoch 30, Batch 50, Train Loss: 0.07085617631673813, Memory (GB): 7\n",
      "Epoch 30, Batch 51, Train Loss: 0.05878095328807831, Memory (GB): 7\n",
      "Epoch 30, Batch 52, Train Loss: 0.059281595051288605, Memory (GB): 7\n",
      "Epoch 30, Batch 53, Train Loss: 0.06077346205711365, Memory (GB): 7\n",
      "Epoch 30, Batch 54, Train Loss: 0.06785200536251068, Memory (GB): 7\n",
      "Epoch 30, Batch 55, Train Loss: 0.06463830918073654, Memory (GB): 7\n",
      "Epoch 30, Batch 56, Train Loss: 0.06136039271950722, Memory (GB): 7\n",
      "Epoch 30, Batch 57, Train Loss: 0.07217270135879517, Memory (GB): 7\n",
      "Epoch 30, Batch 58, Train Loss: 0.06576208025217056, Memory (GB): 7\n",
      "Epoch 30, Batch 59, Train Loss: 0.06364834308624268, Memory (GB): 7\n",
      "Epoch 30, Batch 60, Train Loss: 0.056975286453962326, Memory (GB): 7\n",
      "Epoch 30, Batch 61, Train Loss: 0.06330880522727966, Memory (GB): 7\n",
      "Epoch 30, Batch 62, Train Loss: 0.05944833159446716, Memory (GB): 7\n",
      "Epoch 30, Batch 63, Train Loss: 0.054282937198877335, Memory (GB): 7\n",
      "Epoch 30, Batch 64, Train Loss: 0.06110160052776337, Memory (GB): 7\n",
      "Epoch 30, Batch 65, Train Loss: 0.07107137888669968, Memory (GB): 7\n",
      "Epoch 30, Batch 66, Train Loss: 0.06729655712842941, Memory (GB): 7\n",
      "Epoch 30, Batch 67, Train Loss: 0.05746360123157501, Memory (GB): 7\n",
      "Epoch 30, Batch 68, Train Loss: 0.06392983347177505, Memory (GB): 7\n",
      "Epoch 30, Batch 69, Train Loss: 0.06416726112365723, Memory (GB): 7\n",
      "Epoch 30, Batch 70, Train Loss: 0.060476154088974, Memory (GB): 7\n",
      "Epoch 30, Batch 71, Train Loss: 0.05799923464655876, Memory (GB): 7\n",
      "Epoch 30, Batch 72, Train Loss: 0.05757129192352295, Memory (GB): 7\n",
      "Epoch 30, Batch 73, Train Loss: 0.056831326335668564, Memory (GB): 7\n",
      "Epoch 30, Batch 74, Train Loss: 0.055713068693876266, Memory (GB): 7\n",
      "Epoch 30, Batch 75, Train Loss: 0.06085939705371857, Memory (GB): 7\n",
      "Epoch 30, Batch 76, Train Loss: 0.052834466099739075, Memory (GB): 7\n",
      "Epoch 30, Batch 77, Train Loss: 0.05957166478037834, Memory (GB): 7\n",
      "Epoch 30, Batch 78, Train Loss: 0.06200224533677101, Memory (GB): 7\n",
      "Epoch 30, Batch 79, Train Loss: 0.057518571615219116, Memory (GB): 7\n",
      "Epoch 30, Batch 80, Train Loss: 0.05856490135192871, Memory (GB): 7\n",
      "Epoch 30, Batch 81, Train Loss: 0.06330281496047974, Memory (GB): 7\n",
      "Epoch 30, Batch 82, Train Loss: 0.07347820699214935, Memory (GB): 7\n",
      "Epoch 30, Batch 83, Train Loss: 0.054846398532390594, Memory (GB): 7\n",
      "Epoch 30, Batch 84, Train Loss: 0.05895579606294632, Memory (GB): 7\n",
      "Epoch 30, Batch 85, Train Loss: 0.05806732177734375, Memory (GB): 7\n",
      "Epoch 30, Batch 86, Train Loss: 0.06650558114051819, Memory (GB): 7\n",
      "Epoch 30, Batch 87, Train Loss: 0.0664297491312027, Memory (GB): 7\n",
      "Epoch 30, Batch 88, Train Loss: 0.06640003621578217, Memory (GB): 7\n",
      "Epoch 30, Batch 89, Train Loss: 0.0592842735350132, Memory (GB): 7\n",
      "Epoch 30, Batch 90, Train Loss: 0.05778448283672333, Memory (GB): 7\n",
      "Epoch 30, Batch 91, Train Loss: 0.0561247318983078, Memory (GB): 7\n",
      "Epoch 30, Batch 92, Train Loss: 0.06340844184160233, Memory (GB): 7\n",
      "Epoch 30, Batch 93, Train Loss: 0.06331229209899902, Memory (GB): 7\n",
      "Epoch 30, Batch 94, Train Loss: 0.06358958780765533, Memory (GB): 7\n",
      "Epoch 30, Batch 95, Train Loss: 0.06031767651438713, Memory (GB): 7\n",
      "Epoch 30, Batch 96, Train Loss: 0.060560986399650574, Memory (GB): 7\n",
      "Epoch 30, Batch 97, Train Loss: 0.07006760686635971, Memory (GB): 7\n",
      "Epoch 30, Batch 98, Train Loss: 0.059593115001916885, Memory (GB): 7\n",
      "Epoch 30, Batch 99, Train Loss: 0.06153040751814842, Memory (GB): 7\n",
      "Epoch 30, Batch 100, Train Loss: 0.05895046889781952, Memory (GB): 7\n",
      "Epoch 30, Batch 101, Train Loss: 0.060462553054094315, Memory (GB): 7\n",
      "Epoch 30, Batch 102, Train Loss: 0.057936690747737885, Memory (GB): 7\n",
      "Epoch 30, Batch 103, Train Loss: 0.06648524850606918, Memory (GB): 7\n",
      "Epoch 30, Batch 104, Train Loss: 0.060422420501708984, Memory (GB): 7\n",
      "Epoch 30, Batch 105, Train Loss: 0.06171605736017227, Memory (GB): 7\n",
      "Epoch 30, Batch 106, Train Loss: 0.060367852449417114, Memory (GB): 7\n",
      "Epoch 30, Batch 107, Train Loss: 0.059157632291316986, Memory (GB): 7\n",
      "Epoch 30, Batch 108, Train Loss: 0.0653826966881752, Memory (GB): 7\n",
      "Epoch 30, Batch 109, Train Loss: 0.07033798843622208, Memory (GB): 7\n",
      "Epoch 30, Batch 110, Train Loss: 0.05933661758899689, Memory (GB): 7\n",
      "Epoch 30, Batch 111, Train Loss: 0.06615249812602997, Memory (GB): 7\n",
      "Epoch 30, Batch 112, Train Loss: 0.059040673077106476, Memory (GB): 7\n",
      "Epoch 30, Batch 113, Train Loss: 0.061889249831438065, Memory (GB): 7\n",
      "Epoch 30, Batch 114, Train Loss: 0.06553075462579727, Memory (GB): 7\n",
      "Epoch 30, Batch 115, Train Loss: 0.05733579024672508, Memory (GB): 7\n",
      "Epoch 30, Batch 116, Train Loss: 0.057436466217041016, Memory (GB): 7\n",
      "Epoch 30, Batch 117, Train Loss: 0.057729993015527725, Memory (GB): 7\n",
      "Epoch 30, Batch 118, Train Loss: 0.07009182870388031, Memory (GB): 7\n",
      "Epoch 30, Batch 119, Train Loss: 0.06229357421398163, Memory (GB): 7\n",
      "Epoch 30, Batch 120, Train Loss: 0.07463302463293076, Memory (GB): 7\n",
      "Epoch 30, Batch 121, Train Loss: 0.0636705756187439, Memory (GB): 7\n",
      "Epoch 30, Batch 122, Train Loss: 0.04708462208509445, Memory (GB): 7\n",
      "Epoch 30, Batch 123, Train Loss: 0.07413806021213531, Memory (GB): 7\n",
      "Epoch 30, Batch 124, Train Loss: 0.05619402229785919, Memory (GB): 7\n",
      "Epoch 30, Batch 125, Train Loss: 0.058297038078308105, Memory (GB): 7\n",
      "Epoch 30, Batch 126, Train Loss: 0.062009766697883606, Memory (GB): 7\n",
      "Epoch 30, Batch 127, Train Loss: 0.059838782995939255, Memory (GB): 7\n",
      "Epoch 30, Batch 128, Train Loss: 0.06363381445407867, Memory (GB): 7\n",
      "Epoch 30, Batch 129, Train Loss: 0.06682116538286209, Memory (GB): 7\n",
      "Epoch 30, Batch 130, Train Loss: 0.05835194140672684, Memory (GB): 7\n",
      "Epoch 30, Batch 131, Train Loss: 0.0604606568813324, Memory (GB): 7\n",
      "Epoch 30, Batch 132, Train Loss: 0.05749578773975372, Memory (GB): 7\n",
      "Epoch 30, Batch 133, Train Loss: 0.06059439852833748, Memory (GB): 7\n",
      "Epoch 30, Batch 134, Train Loss: 0.06299668550491333, Memory (GB): 7\n",
      "Epoch 30, Batch 135, Train Loss: 0.06211904063820839, Memory (GB): 7\n",
      "Epoch 30, Batch 136, Train Loss: 0.0564102903008461, Memory (GB): 7\n",
      "Epoch 30, Batch 137, Train Loss: 0.05606396496295929, Memory (GB): 7\n",
      "Epoch 30, Batch 138, Train Loss: 0.06849607080221176, Memory (GB): 7\n",
      "Epoch 30, Batch 139, Train Loss: 0.07478853315114975, Memory (GB): 7\n",
      "Epoch 30, Batch 140, Train Loss: 0.05597066879272461, Memory (GB): 7\n",
      "Epoch 30, Batch 141, Train Loss: 0.09051492810249329, Memory (GB): 7\n",
      "Epoch 30, Batch 142, Train Loss: 0.07540655136108398, Memory (GB): 7\n",
      "Epoch 30, Batch 143, Train Loss: 0.05683720111846924, Memory (GB): 7\n",
      "Epoch 30, Batch 144, Train Loss: 0.06353439390659332, Memory (GB): 7\n",
      "Epoch 30, Batch 145, Train Loss: 0.05960506573319435, Memory (GB): 7\n",
      "Epoch 30, Batch 146, Train Loss: 0.06432494521141052, Memory (GB): 7\n",
      "Epoch 30, Batch 147, Train Loss: 0.05882297828793526, Memory (GB): 7\n",
      "Epoch 30, Batch 148, Train Loss: 0.06205296516418457, Memory (GB): 7\n",
      "Epoch 30, Batch 149, Train Loss: 0.0629308894276619, Memory (GB): 7\n",
      "Epoch 30, Batch 150, Train Loss: 0.06265631318092346, Memory (GB): 7\n",
      "Epoch 30, Batch 151, Train Loss: 0.06046128273010254, Memory (GB): 7\n",
      "Epoch 30, Batch 152, Train Loss: 0.05922726169228554, Memory (GB): 7\n",
      "Epoch 30, Batch 153, Train Loss: 0.06142627075314522, Memory (GB): 7\n",
      "Epoch 30, Batch 154, Train Loss: 0.0644456148147583, Memory (GB): 7\n",
      "Epoch 30, Batch 155, Train Loss: 0.05577073618769646, Memory (GB): 7\n",
      "Epoch 30, Batch 156, Train Loss: 0.0552627258002758, Memory (GB): 7\n",
      "Epoch 30, Batch 157, Train Loss: 0.059991661459207535, Memory (GB): 7\n",
      "Epoch 30, Batch 158, Train Loss: 0.06037302315235138, Memory (GB): 7\n",
      "Epoch 30, Batch 159, Train Loss: 0.0646405816078186, Memory (GB): 7\n",
      "Epoch 30, Batch 160, Train Loss: 0.057658951729536057, Memory (GB): 7\n",
      "Epoch 30, Batch 161, Train Loss: 0.057349901646375656, Memory (GB): 7\n",
      "Epoch 30, Batch 162, Train Loss: 0.07468472421169281, Memory (GB): 7\n",
      "Epoch 30, Batch 163, Train Loss: 0.06622966378927231, Memory (GB): 7\n",
      "Epoch 30, Batch 164, Train Loss: 0.06914694607257843, Memory (GB): 7\n",
      "Epoch 30, Batch 165, Train Loss: 0.056165166199207306, Memory (GB): 7\n",
      "Epoch 30, Batch 166, Train Loss: 0.061586130410432816, Memory (GB): 7\n",
      "Epoch 30, Batch 167, Train Loss: 0.066778264939785, Memory (GB): 7\n",
      "Epoch 30, Batch 168, Train Loss: 0.08204621076583862, Memory (GB): 7\n",
      "Epoch 30, Batch 169, Train Loss: 0.06339584290981293, Memory (GB): 7\n",
      "Epoch 30, Batch 170, Train Loss: 0.06402406096458435, Memory (GB): 7\n",
      "Epoch 30, Batch 171, Train Loss: 0.0625886395573616, Memory (GB): 7\n",
      "Epoch 30, Batch 172, Train Loss: 0.06033658981323242, Memory (GB): 7\n",
      "Epoch 30, Batch 173, Train Loss: 0.06822650879621506, Memory (GB): 7\n",
      "Epoch 30, Batch 174, Train Loss: 0.05930473655462265, Memory (GB): 7\n",
      "Epoch 30, Batch 175, Train Loss: 0.06314080953598022, Memory (GB): 7\n",
      "Epoch 30, Batch 176, Train Loss: 0.056107163429260254, Memory (GB): 7\n",
      "Epoch 30, Batch 177, Train Loss: 0.07344378530979156, Memory (GB): 7\n",
      "Epoch 30, Batch 178, Train Loss: 0.0740688368678093, Memory (GB): 7\n",
      "Epoch 30, Batch 179, Train Loss: 0.06456152349710464, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 31, Batch 0, Train Loss: 0.054610684514045715, Memory (GB): 7\n",
      "Epoch 31, Batch 1, Train Loss: 0.06427057832479477, Memory (GB): 7\n",
      "Epoch 31, Batch 2, Train Loss: 0.04745001718401909, Memory (GB): 7\n",
      "Epoch 31, Batch 3, Train Loss: 0.054659463465213776, Memory (GB): 7\n",
      "Epoch 31, Batch 4, Train Loss: 0.05647074431180954, Memory (GB): 7\n",
      "Epoch 31, Batch 5, Train Loss: 0.05459863692522049, Memory (GB): 7\n",
      "Epoch 31, Batch 6, Train Loss: 0.05748403072357178, Memory (GB): 7\n",
      "Epoch 31, Batch 7, Train Loss: 0.052771955728530884, Memory (GB): 7\n",
      "Epoch 31, Batch 8, Train Loss: 0.04973811283707619, Memory (GB): 7\n",
      "Epoch 31, Batch 9, Train Loss: 0.059862978756427765, Memory (GB): 7\n",
      "Epoch 31, Batch 10, Train Loss: 0.05169793963432312, Memory (GB): 7\n",
      "Epoch 31, Batch 11, Train Loss: 0.05488903075456619, Memory (GB): 7\n",
      "Epoch 31, Batch 12, Train Loss: 0.049763765186071396, Memory (GB): 7\n",
      "Epoch 31, Batch 13, Train Loss: 0.05580006167292595, Memory (GB): 7\n",
      "Epoch 31, Batch 14, Train Loss: 0.051459942013025284, Memory (GB): 7\n",
      "Epoch 31, Batch 15, Train Loss: 0.0495310015976429, Memory (GB): 7\n",
      "Epoch 31, Batch 16, Train Loss: 0.07179782539606094, Memory (GB): 7\n",
      "Epoch 31, Batch 17, Train Loss: 0.04814411327242851, Memory (GB): 7\n",
      "Epoch 31, Batch 18, Train Loss: 0.0651804581284523, Memory (GB): 7\n",
      "Epoch 31, Batch 19, Train Loss: 0.05159979686141014, Memory (GB): 7\n",
      "Epoch 31, Batch 20, Train Loss: 0.05766473338007927, Memory (GB): 7\n",
      "Epoch 31, Batch 21, Train Loss: 0.053500209003686905, Memory (GB): 7\n",
      "Epoch 31, Batch 22, Train Loss: 0.049216218292713165, Memory (GB): 7\n",
      "Epoch 31, Batch 23, Train Loss: 0.05270686745643616, Memory (GB): 7\n",
      "Epoch 31, Batch 24, Train Loss: 0.05686522275209427, Memory (GB): 7\n",
      "Epoch 31, Batch 25, Train Loss: 0.06342148035764694, Memory (GB): 7\n",
      "Epoch 31, Batch 26, Train Loss: 0.07945654541254044, Memory (GB): 7\n",
      "Epoch 31, Batch 27, Train Loss: 0.05695047974586487, Memory (GB): 7\n",
      "Epoch 31, Batch 28, Train Loss: 0.05183890461921692, Memory (GB): 7\n",
      "Epoch 31, Batch 29, Train Loss: 0.049652133136987686, Memory (GB): 7\n",
      "Epoch 31, Batch 30, Train Loss: 0.05708718299865723, Memory (GB): 7\n",
      "Epoch 31, Batch 31, Train Loss: 0.055539973080158234, Memory (GB): 7\n",
      "Epoch 31, Batch 32, Train Loss: 0.05822015553712845, Memory (GB): 7\n",
      "Epoch 31, Batch 33, Train Loss: 0.05693788081407547, Memory (GB): 7\n",
      "Epoch 31, Batch 34, Train Loss: 0.06000564247369766, Memory (GB): 7\n",
      "Epoch 31, Batch 35, Train Loss: 0.06691958010196686, Memory (GB): 7\n",
      "Epoch 31, Batch 36, Train Loss: 0.05395670235157013, Memory (GB): 7\n",
      "Epoch 31, Batch 37, Train Loss: 0.05645587295293808, Memory (GB): 7\n",
      "Epoch 31, Batch 38, Train Loss: 0.0564560666680336, Memory (GB): 7\n",
      "Epoch 31, Batch 39, Train Loss: 0.052656129002571106, Memory (GB): 7\n",
      "Epoch 31, Batch 40, Train Loss: 0.06045558303594589, Memory (GB): 7\n",
      "Epoch 31, Batch 41, Train Loss: 0.05639563873410225, Memory (GB): 7\n",
      "Epoch 31, Batch 42, Train Loss: 0.06012403219938278, Memory (GB): 7\n",
      "Epoch 31, Batch 43, Train Loss: 0.05710916966199875, Memory (GB): 7\n",
      "Epoch 31, Batch 44, Train Loss: 0.057726722210645676, Memory (GB): 7\n",
      "Epoch 31, Batch 45, Train Loss: 0.05350695177912712, Memory (GB): 7\n",
      "Epoch 31, Batch 46, Train Loss: 0.057468779385089874, Memory (GB): 7\n",
      "Epoch 31, Batch 47, Train Loss: 0.0501810722053051, Memory (GB): 7\n",
      "Epoch 31, Batch 48, Train Loss: 0.051710888743400574, Memory (GB): 7\n",
      "Epoch 31, Batch 49, Train Loss: 0.056801602244377136, Memory (GB): 7\n",
      "Epoch 31, Batch 50, Train Loss: 0.0601770356297493, Memory (GB): 7\n",
      "Epoch 31, Batch 51, Train Loss: 0.05841783061623573, Memory (GB): 7\n",
      "Epoch 31, Batch 52, Train Loss: 0.052879754453897476, Memory (GB): 7\n",
      "Epoch 31, Batch 53, Train Loss: 0.05231605842709541, Memory (GB): 7\n",
      "Epoch 31, Batch 54, Train Loss: 0.05481293052434921, Memory (GB): 7\n",
      "Epoch 31, Batch 55, Train Loss: 0.05018923804163933, Memory (GB): 7\n",
      "Epoch 31, Batch 56, Train Loss: 0.05360626429319382, Memory (GB): 7\n",
      "Epoch 31, Batch 57, Train Loss: 0.059580620378255844, Memory (GB): 7\n",
      "Epoch 31, Batch 58, Train Loss: 0.05057959258556366, Memory (GB): 7\n",
      "Epoch 31, Batch 59, Train Loss: 0.05321618914604187, Memory (GB): 7\n",
      "Epoch 31, Batch 60, Train Loss: 0.047784674912691116, Memory (GB): 7\n",
      "Epoch 31, Batch 61, Train Loss: 0.06796900182962418, Memory (GB): 7\n",
      "Epoch 31, Batch 62, Train Loss: 0.052064068615436554, Memory (GB): 7\n",
      "Epoch 31, Batch 63, Train Loss: 0.061936456710100174, Memory (GB): 7\n",
      "Epoch 31, Batch 64, Train Loss: 0.050779350101947784, Memory (GB): 7\n",
      "Epoch 31, Batch 65, Train Loss: 0.05036954581737518, Memory (GB): 7\n",
      "Epoch 31, Batch 66, Train Loss: 0.05112515017390251, Memory (GB): 7\n",
      "Epoch 31, Batch 67, Train Loss: 0.05086945369839668, Memory (GB): 7\n",
      "Epoch 31, Batch 68, Train Loss: 0.05272866412997246, Memory (GB): 7\n",
      "Epoch 31, Batch 69, Train Loss: 0.06243216246366501, Memory (GB): 7\n",
      "Epoch 31, Batch 70, Train Loss: 0.05150960013270378, Memory (GB): 7\n",
      "Epoch 31, Batch 71, Train Loss: 0.05229194462299347, Memory (GB): 7\n",
      "Epoch 31, Batch 72, Train Loss: 0.05322849005460739, Memory (GB): 7\n",
      "Epoch 31, Batch 73, Train Loss: 0.047334495931863785, Memory (GB): 7\n",
      "Epoch 31, Batch 74, Train Loss: 0.06213150545954704, Memory (GB): 7\n",
      "Epoch 31, Batch 75, Train Loss: 0.06469228118658066, Memory (GB): 7\n",
      "Epoch 31, Batch 76, Train Loss: 0.06321749091148376, Memory (GB): 7\n",
      "Epoch 31, Batch 77, Train Loss: 0.056018102914094925, Memory (GB): 7\n",
      "Epoch 31, Batch 78, Train Loss: 0.04928118363022804, Memory (GB): 7\n",
      "Epoch 31, Batch 79, Train Loss: 0.05969209969043732, Memory (GB): 7\n",
      "Epoch 31, Batch 80, Train Loss: 0.05568147823214531, Memory (GB): 7\n",
      "Epoch 31, Batch 81, Train Loss: 0.057074613869190216, Memory (GB): 7\n",
      "Epoch 31, Batch 82, Train Loss: 0.06047658622264862, Memory (GB): 7\n",
      "Epoch 31, Batch 83, Train Loss: 0.06277140974998474, Memory (GB): 7\n",
      "Epoch 31, Batch 84, Train Loss: 0.060727279633283615, Memory (GB): 7\n",
      "Epoch 31, Batch 85, Train Loss: 0.05654147267341614, Memory (GB): 7\n",
      "Epoch 31, Batch 86, Train Loss: 0.05445932224392891, Memory (GB): 7\n",
      "Epoch 31, Batch 87, Train Loss: 0.05041426792740822, Memory (GB): 7\n",
      "Epoch 31, Batch 88, Train Loss: 0.0498276948928833, Memory (GB): 7\n",
      "Epoch 31, Batch 89, Train Loss: 0.055633947253227234, Memory (GB): 7\n",
      "Epoch 31, Batch 90, Train Loss: 0.05831977725028992, Memory (GB): 7\n",
      "Epoch 31, Batch 91, Train Loss: 0.05459598824381828, Memory (GB): 7\n",
      "Epoch 31, Batch 92, Train Loss: 0.05318592116236687, Memory (GB): 7\n",
      "Epoch 31, Batch 93, Train Loss: 0.05384642630815506, Memory (GB): 7\n",
      "Epoch 31, Batch 94, Train Loss: 0.054965026676654816, Memory (GB): 7\n",
      "Epoch 31, Batch 95, Train Loss: 0.056561991572380066, Memory (GB): 7\n",
      "Epoch 31, Batch 96, Train Loss: 0.061422210186719894, Memory (GB): 7\n",
      "Epoch 31, Batch 97, Train Loss: 0.054256290197372437, Memory (GB): 7\n",
      "Epoch 31, Batch 98, Train Loss: 0.07055628299713135, Memory (GB): 7\n",
      "Epoch 31, Batch 99, Train Loss: 0.05903235077857971, Memory (GB): 7\n",
      "Epoch 31, Batch 100, Train Loss: 0.0508192777633667, Memory (GB): 7\n",
      "Epoch 31, Batch 101, Train Loss: 0.04952195659279823, Memory (GB): 7\n",
      "Epoch 31, Batch 102, Train Loss: 0.049823351204395294, Memory (GB): 7\n",
      "Epoch 31, Batch 103, Train Loss: 0.07011908292770386, Memory (GB): 7\n",
      "Epoch 31, Batch 104, Train Loss: 0.05256903916597366, Memory (GB): 7\n",
      "Epoch 31, Batch 105, Train Loss: 0.04861722141504288, Memory (GB): 7\n",
      "Epoch 31, Batch 106, Train Loss: 0.04833506792783737, Memory (GB): 7\n",
      "Epoch 31, Batch 107, Train Loss: 0.062420617789030075, Memory (GB): 7\n",
      "Epoch 31, Batch 108, Train Loss: 0.05057213455438614, Memory (GB): 7\n",
      "Epoch 31, Batch 109, Train Loss: 0.05321521311998367, Memory (GB): 7\n",
      "Epoch 31, Batch 110, Train Loss: 0.04621199890971184, Memory (GB): 7\n",
      "Epoch 31, Batch 111, Train Loss: 0.04642054811120033, Memory (GB): 7\n",
      "Epoch 31, Batch 112, Train Loss: 0.056753017008304596, Memory (GB): 7\n",
      "Epoch 31, Batch 113, Train Loss: 0.06909149885177612, Memory (GB): 7\n",
      "Epoch 31, Batch 114, Train Loss: 0.058555569499731064, Memory (GB): 7\n",
      "Epoch 31, Batch 115, Train Loss: 0.0602768249809742, Memory (GB): 7\n",
      "Epoch 31, Batch 116, Train Loss: 0.0503387525677681, Memory (GB): 7\n",
      "Epoch 31, Batch 117, Train Loss: 0.05160436034202576, Memory (GB): 7\n",
      "Epoch 31, Batch 118, Train Loss: 0.05627397075295448, Memory (GB): 7\n",
      "Epoch 31, Batch 119, Train Loss: 0.0532301589846611, Memory (GB): 7\n",
      "Epoch 31, Batch 120, Train Loss: 0.05361773818731308, Memory (GB): 7\n",
      "Epoch 31, Batch 121, Train Loss: 0.0560908205807209, Memory (GB): 7\n",
      "Epoch 31, Batch 122, Train Loss: 0.05526619404554367, Memory (GB): 7\n",
      "Epoch 31, Batch 123, Train Loss: 0.054595254361629486, Memory (GB): 7\n",
      "Epoch 31, Batch 124, Train Loss: 0.058420680463314056, Memory (GB): 7\n",
      "Epoch 31, Batch 125, Train Loss: 0.05153735354542732, Memory (GB): 7\n",
      "Epoch 31, Batch 126, Train Loss: 0.054995134472846985, Memory (GB): 7\n",
      "Epoch 31, Batch 127, Train Loss: 0.051422469317913055, Memory (GB): 7\n",
      "Epoch 31, Batch 128, Train Loss: 0.07365310937166214, Memory (GB): 7\n",
      "Epoch 31, Batch 129, Train Loss: 0.06337077170610428, Memory (GB): 7\n",
      "Epoch 31, Batch 130, Train Loss: 0.05256987735629082, Memory (GB): 7\n",
      "Epoch 31, Batch 131, Train Loss: 0.05646609514951706, Memory (GB): 7\n",
      "Epoch 31, Batch 132, Train Loss: 0.05169147625565529, Memory (GB): 7\n",
      "Epoch 31, Batch 133, Train Loss: 0.061774320900440216, Memory (GB): 7\n",
      "Epoch 31, Batch 134, Train Loss: 0.05522467568516731, Memory (GB): 7\n",
      "Epoch 31, Batch 135, Train Loss: 0.056424934417009354, Memory (GB): 7\n",
      "Epoch 31, Batch 136, Train Loss: 0.0499429926276207, Memory (GB): 7\n",
      "Epoch 31, Batch 137, Train Loss: 0.0505540706217289, Memory (GB): 7\n",
      "Epoch 31, Batch 138, Train Loss: 0.04863481968641281, Memory (GB): 7\n",
      "Epoch 31, Batch 139, Train Loss: 0.04818084090948105, Memory (GB): 7\n",
      "Epoch 31, Batch 140, Train Loss: 0.05306587368249893, Memory (GB): 7\n",
      "Epoch 31, Batch 141, Train Loss: 0.05451243370771408, Memory (GB): 7\n",
      "Epoch 31, Batch 142, Train Loss: 0.05268006771802902, Memory (GB): 7\n",
      "Epoch 31, Batch 143, Train Loss: 0.05611329898238182, Memory (GB): 7\n",
      "Epoch 31, Batch 144, Train Loss: 0.055162213742733, Memory (GB): 7\n",
      "Epoch 31, Batch 145, Train Loss: 0.05526889115571976, Memory (GB): 7\n",
      "Epoch 31, Batch 146, Train Loss: 0.06131701171398163, Memory (GB): 7\n",
      "Epoch 31, Batch 147, Train Loss: 0.05072864890098572, Memory (GB): 7\n",
      "Epoch 31, Batch 148, Train Loss: 0.05032262206077576, Memory (GB): 7\n",
      "Epoch 31, Batch 149, Train Loss: 0.047981586307287216, Memory (GB): 7\n",
      "Epoch 31, Batch 150, Train Loss: 0.05629293620586395, Memory (GB): 7\n",
      "Epoch 31, Batch 151, Train Loss: 0.04934059828519821, Memory (GB): 7\n",
      "Epoch 31, Batch 152, Train Loss: 0.05153709277510643, Memory (GB): 7\n",
      "Epoch 31, Batch 153, Train Loss: 0.06859954446554184, Memory (GB): 7\n",
      "Epoch 31, Batch 154, Train Loss: 0.061264004558324814, Memory (GB): 7\n",
      "Epoch 31, Batch 155, Train Loss: 0.055897362530231476, Memory (GB): 7\n",
      "Epoch 31, Batch 156, Train Loss: 0.052383922040462494, Memory (GB): 7\n",
      "Epoch 31, Batch 157, Train Loss: 0.05468979850411415, Memory (GB): 7\n",
      "Epoch 31, Batch 158, Train Loss: 0.04614966735243797, Memory (GB): 7\n",
      "Epoch 31, Batch 159, Train Loss: 0.049184855073690414, Memory (GB): 7\n",
      "Epoch 31, Batch 160, Train Loss: 0.048055268824100494, Memory (GB): 7\n",
      "Epoch 31, Batch 161, Train Loss: 0.0516342893242836, Memory (GB): 7\n",
      "Epoch 31, Batch 162, Train Loss: 0.053812503814697266, Memory (GB): 7\n",
      "Epoch 31, Batch 163, Train Loss: 0.05999169498682022, Memory (GB): 7\n",
      "Epoch 31, Batch 164, Train Loss: 0.05355561524629593, Memory (GB): 7\n",
      "Epoch 31, Batch 165, Train Loss: 0.05176766589283943, Memory (GB): 7\n",
      "Epoch 31, Batch 166, Train Loss: 0.06476932018995285, Memory (GB): 7\n",
      "Epoch 31, Batch 167, Train Loss: 0.05516725406050682, Memory (GB): 7\n",
      "Epoch 31, Batch 168, Train Loss: 0.05702182278037071, Memory (GB): 7\n",
      "Epoch 31, Batch 169, Train Loss: 0.05215075984597206, Memory (GB): 7\n",
      "Epoch 31, Batch 170, Train Loss: 0.062212616205215454, Memory (GB): 7\n",
      "Epoch 31, Batch 171, Train Loss: 0.05402851104736328, Memory (GB): 7\n",
      "Epoch 31, Batch 172, Train Loss: 0.05549607798457146, Memory (GB): 7\n",
      "Epoch 31, Batch 173, Train Loss: 0.05638334900140762, Memory (GB): 7\n",
      "Epoch 31, Batch 174, Train Loss: 0.05336236208677292, Memory (GB): 7\n",
      "Epoch 31, Batch 175, Train Loss: 0.061448756605386734, Memory (GB): 7\n",
      "Epoch 31, Batch 176, Train Loss: 0.0573529414832592, Memory (GB): 7\n",
      "Epoch 31, Batch 177, Train Loss: 0.05502692982554436, Memory (GB): 7\n",
      "Epoch 31, Batch 178, Train Loss: 0.04822792485356331, Memory (GB): 7\n",
      "Epoch 31, Batch 179, Train Loss: 0.059972409158945084, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 32, Batch 0, Train Loss: 0.05600779503583908, Memory (GB): 7\n",
      "Epoch 32, Batch 1, Train Loss: 0.054979823529720306, Memory (GB): 7\n",
      "Epoch 32, Batch 2, Train Loss: 0.049371011555194855, Memory (GB): 7\n",
      "Epoch 32, Batch 3, Train Loss: 0.05179901793599129, Memory (GB): 7\n",
      "Epoch 32, Batch 4, Train Loss: 0.06252798438072205, Memory (GB): 7\n",
      "Epoch 32, Batch 5, Train Loss: 0.04548276588320732, Memory (GB): 7\n",
      "Epoch 32, Batch 6, Train Loss: 0.04474236071109772, Memory (GB): 7\n",
      "Epoch 32, Batch 7, Train Loss: 0.04594837874174118, Memory (GB): 7\n",
      "Epoch 32, Batch 8, Train Loss: 0.05168573558330536, Memory (GB): 7\n",
      "Epoch 32, Batch 9, Train Loss: 0.0466204471886158, Memory (GB): 7\n",
      "Epoch 32, Batch 10, Train Loss: 0.046404365450143814, Memory (GB): 7\n",
      "Epoch 32, Batch 11, Train Loss: 0.05286071449518204, Memory (GB): 7\n",
      "Epoch 32, Batch 12, Train Loss: 0.052839867770671844, Memory (GB): 7\n",
      "Epoch 32, Batch 13, Train Loss: 0.04604587331414223, Memory (GB): 7\n",
      "Epoch 32, Batch 14, Train Loss: 0.05201191082596779, Memory (GB): 7\n",
      "Epoch 32, Batch 15, Train Loss: 0.05019540339708328, Memory (GB): 7\n",
      "Epoch 32, Batch 16, Train Loss: 0.05961859971284866, Memory (GB): 7\n",
      "Epoch 32, Batch 17, Train Loss: 0.048586733639240265, Memory (GB): 7\n",
      "Epoch 32, Batch 18, Train Loss: 0.062078673392534256, Memory (GB): 7\n",
      "Epoch 32, Batch 19, Train Loss: 0.04424874484539032, Memory (GB): 7\n",
      "Epoch 32, Batch 20, Train Loss: 0.04947752133011818, Memory (GB): 7\n",
      "Epoch 32, Batch 21, Train Loss: 0.05202242359519005, Memory (GB): 7\n",
      "Epoch 32, Batch 22, Train Loss: 0.052880968898534775, Memory (GB): 7\n",
      "Epoch 32, Batch 23, Train Loss: 0.045709382742643356, Memory (GB): 7\n",
      "Epoch 32, Batch 24, Train Loss: 0.05178332328796387, Memory (GB): 7\n",
      "Epoch 32, Batch 25, Train Loss: 0.05068081244826317, Memory (GB): 7\n",
      "Epoch 32, Batch 26, Train Loss: 0.05094805359840393, Memory (GB): 7\n",
      "Epoch 32, Batch 27, Train Loss: 0.04343090578913689, Memory (GB): 7\n",
      "Epoch 32, Batch 28, Train Loss: 0.04899463802576065, Memory (GB): 7\n",
      "Epoch 32, Batch 29, Train Loss: 0.04761995002627373, Memory (GB): 7\n",
      "Epoch 32, Batch 30, Train Loss: 0.06398016959428787, Memory (GB): 7\n",
      "Epoch 32, Batch 31, Train Loss: 0.04725567623972893, Memory (GB): 7\n",
      "Epoch 32, Batch 32, Train Loss: 0.04899589344859123, Memory (GB): 7\n",
      "Epoch 32, Batch 33, Train Loss: 0.05025530606508255, Memory (GB): 7\n",
      "Epoch 32, Batch 34, Train Loss: 0.05179498344659805, Memory (GB): 7\n",
      "Epoch 32, Batch 35, Train Loss: 0.05736616626381874, Memory (GB): 7\n",
      "Epoch 32, Batch 36, Train Loss: 0.05590534210205078, Memory (GB): 7\n",
      "Epoch 32, Batch 37, Train Loss: 0.04728630185127258, Memory (GB): 7\n",
      "Epoch 32, Batch 38, Train Loss: 0.04402521625161171, Memory (GB): 7\n",
      "Epoch 32, Batch 39, Train Loss: 0.04348021000623703, Memory (GB): 7\n",
      "Epoch 32, Batch 40, Train Loss: 0.04618937149643898, Memory (GB): 7\n",
      "Epoch 32, Batch 41, Train Loss: 0.056953031569719315, Memory (GB): 7\n",
      "Epoch 32, Batch 42, Train Loss: 0.043997496366500854, Memory (GB): 7\n",
      "Epoch 32, Batch 43, Train Loss: 0.0489959791302681, Memory (GB): 7\n",
      "Epoch 32, Batch 44, Train Loss: 0.059809889644384384, Memory (GB): 7\n",
      "Epoch 32, Batch 45, Train Loss: 0.05384675785899162, Memory (GB): 7\n",
      "Epoch 32, Batch 46, Train Loss: 0.04967876896262169, Memory (GB): 7\n",
      "Epoch 32, Batch 47, Train Loss: 0.057538874447345734, Memory (GB): 7\n",
      "Epoch 32, Batch 48, Train Loss: 0.05401059612631798, Memory (GB): 7\n",
      "Epoch 32, Batch 49, Train Loss: 0.04736386612057686, Memory (GB): 7\n",
      "Epoch 32, Batch 50, Train Loss: 0.054267518222332, Memory (GB): 7\n",
      "Epoch 32, Batch 51, Train Loss: 0.05202639847993851, Memory (GB): 7\n",
      "Epoch 32, Batch 52, Train Loss: 0.05197323113679886, Memory (GB): 7\n",
      "Epoch 32, Batch 53, Train Loss: 0.0462200865149498, Memory (GB): 7\n",
      "Epoch 32, Batch 54, Train Loss: 0.05411525070667267, Memory (GB): 7\n",
      "Epoch 32, Batch 55, Train Loss: 0.04489511996507645, Memory (GB): 7\n",
      "Epoch 32, Batch 56, Train Loss: 0.04535152018070221, Memory (GB): 7\n",
      "Epoch 32, Batch 57, Train Loss: 0.045932333916425705, Memory (GB): 7\n",
      "Epoch 32, Batch 58, Train Loss: 0.05036035552620888, Memory (GB): 7\n",
      "Epoch 32, Batch 59, Train Loss: 0.04982088878750801, Memory (GB): 7\n",
      "Epoch 32, Batch 60, Train Loss: 0.05094725638628006, Memory (GB): 7\n",
      "Epoch 32, Batch 61, Train Loss: 0.05509120970964432, Memory (GB): 7\n",
      "Epoch 32, Batch 62, Train Loss: 0.05892643332481384, Memory (GB): 7\n",
      "Epoch 32, Batch 63, Train Loss: 0.05344974994659424, Memory (GB): 7\n",
      "Epoch 32, Batch 64, Train Loss: 0.0531473271548748, Memory (GB): 7\n",
      "Epoch 32, Batch 65, Train Loss: 0.04636875167489052, Memory (GB): 7\n",
      "Epoch 32, Batch 66, Train Loss: 0.053717631846666336, Memory (GB): 7\n",
      "Epoch 32, Batch 67, Train Loss: 0.051483154296875, Memory (GB): 7\n",
      "Epoch 32, Batch 68, Train Loss: 0.04994028061628342, Memory (GB): 7\n",
      "Epoch 32, Batch 69, Train Loss: 0.05160819739103317, Memory (GB): 7\n",
      "Epoch 32, Batch 70, Train Loss: 0.05568189173936844, Memory (GB): 7\n",
      "Epoch 32, Batch 71, Train Loss: 0.05338079482316971, Memory (GB): 7\n",
      "Epoch 32, Batch 72, Train Loss: 0.04896993935108185, Memory (GB): 7\n",
      "Epoch 32, Batch 73, Train Loss: 0.048267465084791183, Memory (GB): 7\n",
      "Epoch 32, Batch 74, Train Loss: 0.04556998610496521, Memory (GB): 7\n",
      "Epoch 32, Batch 75, Train Loss: 0.05604894459247589, Memory (GB): 7\n",
      "Epoch 32, Batch 76, Train Loss: 0.04185293987393379, Memory (GB): 7\n",
      "Epoch 32, Batch 77, Train Loss: 0.05547503009438515, Memory (GB): 7\n",
      "Epoch 32, Batch 78, Train Loss: 0.0528748482465744, Memory (GB): 7\n",
      "Epoch 32, Batch 79, Train Loss: 0.053586993366479874, Memory (GB): 7\n",
      "Epoch 32, Batch 80, Train Loss: 0.05690433830022812, Memory (GB): 7\n",
      "Epoch 32, Batch 81, Train Loss: 0.0437186062335968, Memory (GB): 7\n",
      "Epoch 32, Batch 82, Train Loss: 0.05414609611034393, Memory (GB): 7\n",
      "Epoch 32, Batch 83, Train Loss: 0.05041126906871796, Memory (GB): 7\n",
      "Epoch 32, Batch 84, Train Loss: 0.05476759746670723, Memory (GB): 7\n",
      "Epoch 32, Batch 85, Train Loss: 0.04835912585258484, Memory (GB): 7\n",
      "Epoch 32, Batch 86, Train Loss: 0.04951885715126991, Memory (GB): 7\n",
      "Epoch 32, Batch 87, Train Loss: 0.045716624706983566, Memory (GB): 7\n",
      "Epoch 32, Batch 88, Train Loss: 0.04599786922335625, Memory (GB): 7\n",
      "Epoch 32, Batch 89, Train Loss: 0.048149410635232925, Memory (GB): 7\n",
      "Epoch 32, Batch 90, Train Loss: 0.048527080565690994, Memory (GB): 7\n",
      "Epoch 32, Batch 91, Train Loss: 0.05185313895344734, Memory (GB): 7\n",
      "Epoch 32, Batch 92, Train Loss: 0.05487162619829178, Memory (GB): 7\n",
      "Epoch 32, Batch 93, Train Loss: 0.05237535014748573, Memory (GB): 7\n",
      "Epoch 32, Batch 94, Train Loss: 0.04824288189411163, Memory (GB): 7\n",
      "Epoch 32, Batch 95, Train Loss: 0.058502815663814545, Memory (GB): 7\n",
      "Epoch 32, Batch 96, Train Loss: 0.060239747166633606, Memory (GB): 7\n",
      "Epoch 32, Batch 97, Train Loss: 0.04664580151438713, Memory (GB): 7\n",
      "Epoch 32, Batch 98, Train Loss: 0.052955854684114456, Memory (GB): 7\n",
      "Epoch 32, Batch 99, Train Loss: 0.046913936734199524, Memory (GB): 7\n",
      "Epoch 32, Batch 100, Train Loss: 0.054992612451314926, Memory (GB): 7\n",
      "Epoch 32, Batch 101, Train Loss: 0.04793158546090126, Memory (GB): 7\n",
      "Epoch 32, Batch 102, Train Loss: 0.05411912128329277, Memory (GB): 7\n",
      "Epoch 32, Batch 103, Train Loss: 0.05146485194563866, Memory (GB): 7\n",
      "Epoch 32, Batch 104, Train Loss: 0.05294935032725334, Memory (GB): 7\n",
      "Epoch 32, Batch 105, Train Loss: 0.05272630974650383, Memory (GB): 7\n",
      "Epoch 32, Batch 106, Train Loss: 0.04968670383095741, Memory (GB): 7\n",
      "Epoch 32, Batch 107, Train Loss: 0.05595803260803223, Memory (GB): 7\n",
      "Epoch 32, Batch 108, Train Loss: 0.05121757835149765, Memory (GB): 7\n",
      "Epoch 32, Batch 109, Train Loss: 0.0515868254005909, Memory (GB): 7\n",
      "Epoch 32, Batch 110, Train Loss: 0.05072655528783798, Memory (GB): 7\n",
      "Epoch 32, Batch 111, Train Loss: 0.047890909016132355, Memory (GB): 7\n",
      "Epoch 32, Batch 112, Train Loss: 0.04739058017730713, Memory (GB): 7\n",
      "Epoch 32, Batch 113, Train Loss: 0.056607168167829514, Memory (GB): 7\n",
      "Epoch 32, Batch 114, Train Loss: 0.04683423042297363, Memory (GB): 7\n",
      "Epoch 32, Batch 115, Train Loss: 0.04782930016517639, Memory (GB): 7\n",
      "Epoch 32, Batch 116, Train Loss: 0.04711992293596268, Memory (GB): 7\n",
      "Epoch 32, Batch 117, Train Loss: 0.05213335528969765, Memory (GB): 7\n",
      "Epoch 32, Batch 118, Train Loss: 0.05386717990040779, Memory (GB): 7\n",
      "Epoch 32, Batch 119, Train Loss: 0.04993503540754318, Memory (GB): 7\n",
      "Epoch 32, Batch 120, Train Loss: 0.056975238025188446, Memory (GB): 7\n",
      "Epoch 32, Batch 121, Train Loss: 0.051045507192611694, Memory (GB): 7\n",
      "Epoch 32, Batch 122, Train Loss: 0.048243001103401184, Memory (GB): 7\n",
      "Epoch 32, Batch 123, Train Loss: 0.04924799129366875, Memory (GB): 7\n",
      "Epoch 32, Batch 124, Train Loss: 0.05570964515209198, Memory (GB): 7\n",
      "Epoch 32, Batch 125, Train Loss: 0.04710736498236656, Memory (GB): 7\n",
      "Epoch 32, Batch 126, Train Loss: 0.05299081280827522, Memory (GB): 7\n",
      "Epoch 32, Batch 127, Train Loss: 0.04816119000315666, Memory (GB): 7\n",
      "Epoch 32, Batch 128, Train Loss: 0.05043213814496994, Memory (GB): 7\n",
      "Epoch 32, Batch 129, Train Loss: 0.05460985004901886, Memory (GB): 7\n",
      "Epoch 32, Batch 130, Train Loss: 0.05128001049160957, Memory (GB): 7\n",
      "Epoch 32, Batch 131, Train Loss: 0.04505627229809761, Memory (GB): 7\n",
      "Epoch 32, Batch 132, Train Loss: 0.0517612062394619, Memory (GB): 7\n",
      "Epoch 32, Batch 133, Train Loss: 0.04559575393795967, Memory (GB): 7\n",
      "Epoch 32, Batch 134, Train Loss: 0.05177922546863556, Memory (GB): 7\n",
      "Epoch 32, Batch 135, Train Loss: 0.049647457897663116, Memory (GB): 7\n",
      "Epoch 32, Batch 136, Train Loss: 0.055572740733623505, Memory (GB): 7\n",
      "Epoch 32, Batch 137, Train Loss: 0.04930431395769119, Memory (GB): 7\n",
      "Epoch 32, Batch 138, Train Loss: 0.0475681833922863, Memory (GB): 7\n",
      "Epoch 32, Batch 139, Train Loss: 0.04827307537198067, Memory (GB): 7\n",
      "Epoch 32, Batch 140, Train Loss: 0.05242914333939552, Memory (GB): 7\n",
      "Epoch 32, Batch 141, Train Loss: 0.050282031297683716, Memory (GB): 7\n",
      "Epoch 32, Batch 142, Train Loss: 0.048891521990299225, Memory (GB): 7\n",
      "Epoch 32, Batch 143, Train Loss: 0.049174483865499496, Memory (GB): 7\n",
      "Epoch 32, Batch 144, Train Loss: 0.04617147520184517, Memory (GB): 7\n",
      "Epoch 32, Batch 145, Train Loss: 0.050995927304029465, Memory (GB): 7\n",
      "Epoch 32, Batch 146, Train Loss: 0.05544286593794823, Memory (GB): 7\n",
      "Epoch 32, Batch 147, Train Loss: 0.04959343001246452, Memory (GB): 7\n",
      "Epoch 32, Batch 148, Train Loss: 0.06069215014576912, Memory (GB): 7\n",
      "Epoch 32, Batch 149, Train Loss: 0.0639372244477272, Memory (GB): 7\n",
      "Epoch 32, Batch 150, Train Loss: 0.04952147975564003, Memory (GB): 7\n",
      "Epoch 32, Batch 151, Train Loss: 0.0516936331987381, Memory (GB): 7\n",
      "Epoch 32, Batch 152, Train Loss: 0.05414179340004921, Memory (GB): 7\n",
      "Epoch 32, Batch 153, Train Loss: 0.05205368995666504, Memory (GB): 7\n",
      "Epoch 32, Batch 154, Train Loss: 0.0491923950612545, Memory (GB): 7\n",
      "Epoch 32, Batch 155, Train Loss: 0.046868130564689636, Memory (GB): 7\n",
      "Epoch 32, Batch 156, Train Loss: 0.053457848727703094, Memory (GB): 7\n",
      "Epoch 32, Batch 157, Train Loss: 0.05700358748435974, Memory (GB): 7\n",
      "Epoch 32, Batch 158, Train Loss: 0.056923024356365204, Memory (GB): 7\n",
      "Epoch 32, Batch 159, Train Loss: 0.048190485686063766, Memory (GB): 7\n",
      "Epoch 32, Batch 160, Train Loss: 0.055404894053936005, Memory (GB): 7\n",
      "Epoch 32, Batch 161, Train Loss: 0.05604340881109238, Memory (GB): 7\n",
      "Epoch 32, Batch 162, Train Loss: 0.04849613830447197, Memory (GB): 7\n",
      "Epoch 32, Batch 163, Train Loss: 0.04970305413007736, Memory (GB): 7\n",
      "Epoch 32, Batch 164, Train Loss: 0.0539504699409008, Memory (GB): 7\n",
      "Epoch 32, Batch 165, Train Loss: 0.05180614814162254, Memory (GB): 7\n",
      "Epoch 32, Batch 166, Train Loss: 0.047717299312353134, Memory (GB): 7\n",
      "Epoch 32, Batch 167, Train Loss: 0.04869603365659714, Memory (GB): 7\n",
      "Epoch 32, Batch 168, Train Loss: 0.05470334738492966, Memory (GB): 7\n",
      "Epoch 32, Batch 169, Train Loss: 0.059231236577034, Memory (GB): 7\n",
      "Epoch 32, Batch 170, Train Loss: 0.0690288171172142, Memory (GB): 7\n",
      "Epoch 32, Batch 171, Train Loss: 0.0547601617872715, Memory (GB): 7\n",
      "Epoch 32, Batch 172, Train Loss: 0.05208246037364006, Memory (GB): 7\n",
      "Epoch 32, Batch 173, Train Loss: 0.04436257854104042, Memory (GB): 7\n",
      "Epoch 32, Batch 174, Train Loss: 0.04426894709467888, Memory (GB): 7\n",
      "Epoch 32, Batch 175, Train Loss: 0.05631958693265915, Memory (GB): 7\n",
      "Epoch 32, Batch 176, Train Loss: 0.05401166155934334, Memory (GB): 7\n",
      "Epoch 32, Batch 177, Train Loss: 0.053638797253370285, Memory (GB): 7\n",
      "Epoch 32, Batch 178, Train Loss: 0.05730034038424492, Memory (GB): 7\n",
      "Epoch 32, Batch 179, Train Loss: 0.053063876926898956, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 33, Batch 0, Train Loss: 0.04641569033265114, Memory (GB): 7\n",
      "Epoch 33, Batch 1, Train Loss: 0.043118853121995926, Memory (GB): 7\n",
      "Epoch 33, Batch 2, Train Loss: 0.05487329512834549, Memory (GB): 7\n",
      "Epoch 33, Batch 3, Train Loss: 0.04756038635969162, Memory (GB): 7\n",
      "Epoch 33, Batch 4, Train Loss: 0.044308118522167206, Memory (GB): 7\n",
      "Epoch 33, Batch 5, Train Loss: 0.046560969203710556, Memory (GB): 7\n",
      "Epoch 33, Batch 6, Train Loss: 0.05354825779795647, Memory (GB): 7\n",
      "Epoch 33, Batch 7, Train Loss: 0.047751136124134064, Memory (GB): 7\n",
      "Epoch 33, Batch 8, Train Loss: 0.04146553948521614, Memory (GB): 7\n",
      "Epoch 33, Batch 9, Train Loss: 0.04825878143310547, Memory (GB): 7\n",
      "Epoch 33, Batch 10, Train Loss: 0.04452614113688469, Memory (GB): 7\n",
      "Epoch 33, Batch 11, Train Loss: 0.045474227517843246, Memory (GB): 7\n",
      "Epoch 33, Batch 12, Train Loss: 0.059162504971027374, Memory (GB): 7\n",
      "Epoch 33, Batch 13, Train Loss: 0.052267998456954956, Memory (GB): 7\n",
      "Epoch 33, Batch 14, Train Loss: 0.04814588278532028, Memory (GB): 7\n",
      "Epoch 33, Batch 15, Train Loss: 0.06122651696205139, Memory (GB): 7\n",
      "Epoch 33, Batch 16, Train Loss: 0.048128776252269745, Memory (GB): 7\n",
      "Epoch 33, Batch 17, Train Loss: 0.04658196493983269, Memory (GB): 7\n",
      "Epoch 33, Batch 18, Train Loss: 0.06732043623924255, Memory (GB): 7\n",
      "Epoch 33, Batch 19, Train Loss: 0.045247633010149, Memory (GB): 7\n",
      "Epoch 33, Batch 20, Train Loss: 0.05017140880227089, Memory (GB): 7\n",
      "Epoch 33, Batch 21, Train Loss: 0.0416564978659153, Memory (GB): 7\n",
      "Epoch 33, Batch 22, Train Loss: 0.043517764657735825, Memory (GB): 7\n",
      "Epoch 33, Batch 23, Train Loss: 0.054221540689468384, Memory (GB): 7\n",
      "Epoch 33, Batch 24, Train Loss: 0.04545390605926514, Memory (GB): 7\n",
      "Epoch 33, Batch 25, Train Loss: 0.047047387808561325, Memory (GB): 7\n",
      "Epoch 33, Batch 26, Train Loss: 0.0462455078959465, Memory (GB): 7\n",
      "Epoch 33, Batch 27, Train Loss: 0.04274537414312363, Memory (GB): 7\n",
      "Epoch 33, Batch 28, Train Loss: 0.05487335845828056, Memory (GB): 7\n",
      "Epoch 33, Batch 29, Train Loss: 0.04604876786470413, Memory (GB): 7\n",
      "Epoch 33, Batch 30, Train Loss: 0.048825059086084366, Memory (GB): 7\n",
      "Epoch 33, Batch 31, Train Loss: 0.045676879584789276, Memory (GB): 7\n",
      "Epoch 33, Batch 32, Train Loss: 0.050980277359485626, Memory (GB): 7\n",
      "Epoch 33, Batch 33, Train Loss: 0.05341852828860283, Memory (GB): 7\n",
      "Epoch 33, Batch 34, Train Loss: 0.045888036489486694, Memory (GB): 7\n",
      "Epoch 33, Batch 35, Train Loss: 0.048555340617895126, Memory (GB): 7\n",
      "Epoch 33, Batch 36, Train Loss: 0.04553167521953583, Memory (GB): 7\n",
      "Epoch 33, Batch 37, Train Loss: 0.04787352681159973, Memory (GB): 7\n",
      "Epoch 33, Batch 38, Train Loss: 0.04690713807940483, Memory (GB): 7\n",
      "Epoch 33, Batch 39, Train Loss: 0.04933886229991913, Memory (GB): 7\n",
      "Epoch 33, Batch 40, Train Loss: 0.05038726329803467, Memory (GB): 7\n",
      "Epoch 33, Batch 41, Train Loss: 0.04852105304598808, Memory (GB): 7\n",
      "Epoch 33, Batch 42, Train Loss: 0.04021110385656357, Memory (GB): 7\n",
      "Epoch 33, Batch 43, Train Loss: 0.054422345012426376, Memory (GB): 7\n",
      "Epoch 33, Batch 44, Train Loss: 0.0468660369515419, Memory (GB): 7\n",
      "Epoch 33, Batch 45, Train Loss: 0.05414244160056114, Memory (GB): 7\n",
      "Epoch 33, Batch 46, Train Loss: 0.04200856760144234, Memory (GB): 7\n",
      "Epoch 33, Batch 47, Train Loss: 0.04094464331865311, Memory (GB): 7\n",
      "Epoch 33, Batch 48, Train Loss: 0.04011007770895958, Memory (GB): 7\n",
      "Epoch 33, Batch 49, Train Loss: 0.05229383334517479, Memory (GB): 7\n",
      "Epoch 33, Batch 50, Train Loss: 0.048448596149683, Memory (GB): 7\n",
      "Epoch 33, Batch 51, Train Loss: 0.054233700037002563, Memory (GB): 7\n",
      "Epoch 33, Batch 52, Train Loss: 0.047876521944999695, Memory (GB): 7\n",
      "Epoch 33, Batch 53, Train Loss: 0.05632684379816055, Memory (GB): 7\n",
      "Epoch 33, Batch 54, Train Loss: 0.06409657746553421, Memory (GB): 7\n",
      "Epoch 33, Batch 55, Train Loss: 0.05639921873807907, Memory (GB): 7\n",
      "Epoch 33, Batch 56, Train Loss: 0.05340525135397911, Memory (GB): 7\n",
      "Epoch 33, Batch 57, Train Loss: 0.05462552234530449, Memory (GB): 7\n",
      "Epoch 33, Batch 58, Train Loss: 0.04789561778306961, Memory (GB): 7\n",
      "Epoch 33, Batch 59, Train Loss: 0.04859084263443947, Memory (GB): 7\n",
      "Epoch 33, Batch 60, Train Loss: 0.04721362516283989, Memory (GB): 7\n",
      "Epoch 33, Batch 61, Train Loss: 0.05527888983488083, Memory (GB): 7\n",
      "Epoch 33, Batch 62, Train Loss: 0.044043391942977905, Memory (GB): 7\n",
      "Epoch 33, Batch 63, Train Loss: 0.04981514438986778, Memory (GB): 7\n",
      "Epoch 33, Batch 64, Train Loss: 0.05006701499223709, Memory (GB): 7\n",
      "Epoch 33, Batch 65, Train Loss: 0.05866269767284393, Memory (GB): 7\n",
      "Epoch 33, Batch 66, Train Loss: 0.04241007938981056, Memory (GB): 7\n",
      "Epoch 33, Batch 67, Train Loss: 0.049789633601903915, Memory (GB): 7\n",
      "Epoch 33, Batch 68, Train Loss: 0.046364765614271164, Memory (GB): 7\n",
      "Epoch 33, Batch 69, Train Loss: 0.05043252930045128, Memory (GB): 7\n",
      "Epoch 33, Batch 70, Train Loss: 0.04131265729665756, Memory (GB): 7\n",
      "Epoch 33, Batch 71, Train Loss: 0.052383534610271454, Memory (GB): 7\n",
      "Epoch 33, Batch 72, Train Loss: 0.04432036355137825, Memory (GB): 7\n",
      "Epoch 33, Batch 73, Train Loss: 0.048437152057886124, Memory (GB): 7\n",
      "Epoch 33, Batch 74, Train Loss: 0.058269478380680084, Memory (GB): 7\n",
      "Epoch 33, Batch 75, Train Loss: 0.054207175970077515, Memory (GB): 7\n",
      "Epoch 33, Batch 76, Train Loss: 0.05116979777812958, Memory (GB): 7\n",
      "Epoch 33, Batch 77, Train Loss: 0.0414043627679348, Memory (GB): 7\n",
      "Epoch 33, Batch 78, Train Loss: 0.0500316396355629, Memory (GB): 7\n",
      "Epoch 33, Batch 79, Train Loss: 0.062300920486450195, Memory (GB): 7\n",
      "Epoch 33, Batch 80, Train Loss: 0.04468896612524986, Memory (GB): 7\n",
      "Epoch 33, Batch 81, Train Loss: 0.05201440304517746, Memory (GB): 7\n",
      "Epoch 33, Batch 82, Train Loss: 0.04509690776467323, Memory (GB): 7\n",
      "Epoch 33, Batch 83, Train Loss: 0.051507335156202316, Memory (GB): 7\n",
      "Epoch 33, Batch 84, Train Loss: 0.04472779855132103, Memory (GB): 7\n",
      "Epoch 33, Batch 85, Train Loss: 0.04767564311623573, Memory (GB): 7\n",
      "Epoch 33, Batch 86, Train Loss: 0.03977664187550545, Memory (GB): 7\n",
      "Epoch 33, Batch 87, Train Loss: 0.04286840930581093, Memory (GB): 7\n",
      "Epoch 33, Batch 88, Train Loss: 0.042645446956157684, Memory (GB): 7\n",
      "Epoch 33, Batch 89, Train Loss: 0.0411488302052021, Memory (GB): 7\n",
      "Epoch 33, Batch 90, Train Loss: 0.04277670010924339, Memory (GB): 7\n",
      "Epoch 33, Batch 91, Train Loss: 0.05481622368097305, Memory (GB): 7\n",
      "Epoch 33, Batch 92, Train Loss: 0.05620710179209709, Memory (GB): 7\n",
      "Epoch 33, Batch 93, Train Loss: 0.04707653820514679, Memory (GB): 7\n",
      "Epoch 33, Batch 94, Train Loss: 0.04386097937822342, Memory (GB): 7\n",
      "Epoch 33, Batch 95, Train Loss: 0.045981042087078094, Memory (GB): 7\n",
      "Epoch 33, Batch 96, Train Loss: 0.04435573145747185, Memory (GB): 7\n",
      "Epoch 33, Batch 97, Train Loss: 0.04254055395722389, Memory (GB): 7\n",
      "Epoch 33, Batch 98, Train Loss: 0.05196164920926094, Memory (GB): 7\n",
      "Epoch 33, Batch 99, Train Loss: 0.04985575005412102, Memory (GB): 7\n",
      "Epoch 33, Batch 100, Train Loss: 0.04733772575855255, Memory (GB): 7\n",
      "Epoch 33, Batch 101, Train Loss: 0.04415091872215271, Memory (GB): 7\n",
      "Epoch 33, Batch 102, Train Loss: 0.0534525141119957, Memory (GB): 7\n",
      "Epoch 33, Batch 103, Train Loss: 0.04298673942685127, Memory (GB): 7\n",
      "Epoch 33, Batch 104, Train Loss: 0.051559533923864365, Memory (GB): 7\n",
      "Epoch 33, Batch 105, Train Loss: 0.04594670236110687, Memory (GB): 7\n",
      "Epoch 33, Batch 106, Train Loss: 0.04653939604759216, Memory (GB): 7\n",
      "Epoch 33, Batch 107, Train Loss: 0.0456935279071331, Memory (GB): 7\n",
      "Epoch 33, Batch 108, Train Loss: 0.045499369502067566, Memory (GB): 7\n",
      "Epoch 33, Batch 109, Train Loss: 0.04176497831940651, Memory (GB): 7\n",
      "Epoch 33, Batch 110, Train Loss: 0.04754485562443733, Memory (GB): 7\n",
      "Epoch 33, Batch 111, Train Loss: 0.052920807152986526, Memory (GB): 7\n",
      "Epoch 33, Batch 112, Train Loss: 0.04331902787089348, Memory (GB): 7\n",
      "Epoch 33, Batch 113, Train Loss: 0.050890546292066574, Memory (GB): 7\n",
      "Epoch 33, Batch 114, Train Loss: 0.054609037935733795, Memory (GB): 7\n",
      "Epoch 33, Batch 115, Train Loss: 0.05034948140382767, Memory (GB): 7\n",
      "Epoch 33, Batch 116, Train Loss: 0.04324137419462204, Memory (GB): 7\n",
      "Epoch 33, Batch 117, Train Loss: 0.044666990637779236, Memory (GB): 7\n",
      "Epoch 33, Batch 118, Train Loss: 0.04776063561439514, Memory (GB): 7\n",
      "Epoch 33, Batch 119, Train Loss: 0.042592599987983704, Memory (GB): 7\n",
      "Epoch 33, Batch 120, Train Loss: 0.04671734943985939, Memory (GB): 7\n",
      "Epoch 33, Batch 121, Train Loss: 0.04881753399968147, Memory (GB): 7\n",
      "Epoch 33, Batch 122, Train Loss: 0.049182742834091187, Memory (GB): 7\n",
      "Epoch 33, Batch 123, Train Loss: 0.04470035433769226, Memory (GB): 7\n",
      "Epoch 33, Batch 124, Train Loss: 0.0497085303068161, Memory (GB): 7\n",
      "Epoch 33, Batch 125, Train Loss: 0.04747916758060455, Memory (GB): 7\n",
      "Epoch 33, Batch 126, Train Loss: 0.0456373430788517, Memory (GB): 7\n",
      "Epoch 33, Batch 127, Train Loss: 0.04442758113145828, Memory (GB): 7\n",
      "Epoch 33, Batch 128, Train Loss: 0.04627962410449982, Memory (GB): 7\n",
      "Epoch 33, Batch 129, Train Loss: 0.037593282759189606, Memory (GB): 7\n",
      "Epoch 33, Batch 130, Train Loss: 0.04728815704584122, Memory (GB): 7\n",
      "Epoch 33, Batch 131, Train Loss: 0.048079460859298706, Memory (GB): 7\n",
      "Epoch 33, Batch 132, Train Loss: 0.04696061462163925, Memory (GB): 7\n",
      "Epoch 33, Batch 133, Train Loss: 0.052848704159259796, Memory (GB): 7\n",
      "Epoch 33, Batch 134, Train Loss: 0.053511641919612885, Memory (GB): 7\n",
      "Epoch 33, Batch 135, Train Loss: 0.05170723423361778, Memory (GB): 7\n",
      "Epoch 33, Batch 136, Train Loss: 0.055693767964839935, Memory (GB): 7\n",
      "Epoch 33, Batch 137, Train Loss: 0.048752326518297195, Memory (GB): 7\n",
      "Epoch 33, Batch 138, Train Loss: 0.048847753554582596, Memory (GB): 7\n",
      "Epoch 33, Batch 139, Train Loss: 0.04797254130244255, Memory (GB): 7\n",
      "Epoch 33, Batch 140, Train Loss: 0.043368615210056305, Memory (GB): 7\n",
      "Epoch 33, Batch 141, Train Loss: 0.04167965054512024, Memory (GB): 7\n",
      "Epoch 33, Batch 142, Train Loss: 0.04540540277957916, Memory (GB): 7\n",
      "Epoch 33, Batch 143, Train Loss: 0.05825020745396614, Memory (GB): 7\n",
      "Epoch 33, Batch 144, Train Loss: 0.05261017009615898, Memory (GB): 7\n",
      "Epoch 33, Batch 145, Train Loss: 0.054772235453128815, Memory (GB): 7\n",
      "Epoch 33, Batch 146, Train Loss: 0.043859075754880905, Memory (GB): 7\n",
      "Epoch 33, Batch 147, Train Loss: 0.0506526343524456, Memory (GB): 7\n",
      "Epoch 33, Batch 148, Train Loss: 0.051551591604948044, Memory (GB): 7\n",
      "Epoch 33, Batch 149, Train Loss: 0.04457862675189972, Memory (GB): 7\n",
      "Epoch 33, Batch 150, Train Loss: 0.0453437976539135, Memory (GB): 7\n",
      "Epoch 33, Batch 151, Train Loss: 0.04904458671808243, Memory (GB): 7\n",
      "Epoch 33, Batch 152, Train Loss: 0.04592530429363251, Memory (GB): 7\n",
      "Epoch 33, Batch 153, Train Loss: 0.057440757751464844, Memory (GB): 7\n",
      "Epoch 33, Batch 154, Train Loss: 0.056782983243465424, Memory (GB): 7\n",
      "Epoch 33, Batch 155, Train Loss: 0.052377767860889435, Memory (GB): 7\n",
      "Epoch 33, Batch 156, Train Loss: 0.04782100394368172, Memory (GB): 7\n",
      "Epoch 33, Batch 157, Train Loss: 0.055244080722332, Memory (GB): 7\n",
      "Epoch 33, Batch 158, Train Loss: 0.050610192120075226, Memory (GB): 7\n",
      "Epoch 33, Batch 159, Train Loss: 0.05037898197770119, Memory (GB): 7\n",
      "Epoch 33, Batch 160, Train Loss: 0.053576357662677765, Memory (GB): 7\n",
      "Epoch 33, Batch 161, Train Loss: 0.046638838946819305, Memory (GB): 7\n",
      "Epoch 33, Batch 162, Train Loss: 0.04254545643925667, Memory (GB): 7\n",
      "Epoch 33, Batch 163, Train Loss: 0.04271506518125534, Memory (GB): 7\n",
      "Epoch 33, Batch 164, Train Loss: 0.04442266747355461, Memory (GB): 7\n",
      "Epoch 33, Batch 165, Train Loss: 0.042072515934705734, Memory (GB): 7\n",
      "Epoch 33, Batch 166, Train Loss: 0.04960811883211136, Memory (GB): 7\n",
      "Epoch 33, Batch 167, Train Loss: 0.05438567325472832, Memory (GB): 7\n",
      "Epoch 33, Batch 168, Train Loss: 0.05698303505778313, Memory (GB): 7\n",
      "Epoch 33, Batch 169, Train Loss: 0.04538735747337341, Memory (GB): 7\n",
      "Epoch 33, Batch 170, Train Loss: 0.04640335217118263, Memory (GB): 7\n",
      "Epoch 33, Batch 171, Train Loss: 0.05083642527461052, Memory (GB): 7\n",
      "Epoch 33, Batch 172, Train Loss: 0.04954513907432556, Memory (GB): 7\n",
      "Epoch 33, Batch 173, Train Loss: 0.04215479642152786, Memory (GB): 7\n",
      "Epoch 33, Batch 174, Train Loss: 0.04765060916543007, Memory (GB): 7\n",
      "Epoch 33, Batch 175, Train Loss: 0.046824607998132706, Memory (GB): 7\n",
      "Epoch 33, Batch 176, Train Loss: 0.04018582031130791, Memory (GB): 7\n",
      "Epoch 33, Batch 177, Train Loss: 0.052831366658210754, Memory (GB): 7\n",
      "Epoch 33, Batch 178, Train Loss: 0.05001905560493469, Memory (GB): 7\n",
      "Epoch 33, Batch 179, Train Loss: 0.05089128017425537, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 34, Batch 0, Train Loss: 0.049962520599365234, Memory (GB): 7\n",
      "Epoch 34, Batch 1, Train Loss: 0.038342736661434174, Memory (GB): 7\n",
      "Epoch 34, Batch 2, Train Loss: 0.044734422117471695, Memory (GB): 7\n",
      "Epoch 34, Batch 3, Train Loss: 0.047977980226278305, Memory (GB): 7\n",
      "Epoch 34, Batch 4, Train Loss: 0.04588761180639267, Memory (GB): 7\n",
      "Epoch 34, Batch 5, Train Loss: 0.03977501019835472, Memory (GB): 7\n",
      "Epoch 34, Batch 6, Train Loss: 0.047526534646749496, Memory (GB): 7\n",
      "Epoch 34, Batch 7, Train Loss: 0.04996123164892197, Memory (GB): 7\n",
      "Epoch 34, Batch 8, Train Loss: 0.04781563952565193, Memory (GB): 7\n",
      "Epoch 34, Batch 9, Train Loss: 0.045008428394794464, Memory (GB): 7\n",
      "Epoch 34, Batch 10, Train Loss: 0.04174623638391495, Memory (GB): 7\n",
      "Epoch 34, Batch 11, Train Loss: 0.050640951842069626, Memory (GB): 7\n",
      "Epoch 34, Batch 12, Train Loss: 0.04453304409980774, Memory (GB): 7\n",
      "Epoch 34, Batch 13, Train Loss: 0.04145190864801407, Memory (GB): 7\n",
      "Epoch 34, Batch 14, Train Loss: 0.04155547544360161, Memory (GB): 7\n",
      "Epoch 34, Batch 15, Train Loss: 0.04727506265044212, Memory (GB): 7\n",
      "Epoch 34, Batch 16, Train Loss: 0.0422513522207737, Memory (GB): 7\n",
      "Epoch 34, Batch 17, Train Loss: 0.06429842859506607, Memory (GB): 7\n",
      "Epoch 34, Batch 18, Train Loss: 0.04125003516674042, Memory (GB): 7\n",
      "Epoch 34, Batch 19, Train Loss: 0.04592490941286087, Memory (GB): 7\n",
      "Epoch 34, Batch 20, Train Loss: 0.047157566994428635, Memory (GB): 7\n",
      "Epoch 34, Batch 21, Train Loss: 0.050059087574481964, Memory (GB): 7\n",
      "Epoch 34, Batch 22, Train Loss: 0.040496308356523514, Memory (GB): 7\n",
      "Epoch 34, Batch 23, Train Loss: 0.046832066029310226, Memory (GB): 7\n",
      "Epoch 34, Batch 24, Train Loss: 0.04454883560538292, Memory (GB): 7\n",
      "Epoch 34, Batch 25, Train Loss: 0.04450702294707298, Memory (GB): 7\n",
      "Epoch 34, Batch 26, Train Loss: 0.041473958641290665, Memory (GB): 7\n",
      "Epoch 34, Batch 27, Train Loss: 0.039673447608947754, Memory (GB): 7\n",
      "Epoch 34, Batch 28, Train Loss: 0.0495927669107914, Memory (GB): 7\n",
      "Epoch 34, Batch 29, Train Loss: 0.048849768936634064, Memory (GB): 7\n",
      "Epoch 34, Batch 30, Train Loss: 0.05022566765546799, Memory (GB): 7\n",
      "Epoch 34, Batch 31, Train Loss: 0.04493693634867668, Memory (GB): 7\n",
      "Epoch 34, Batch 32, Train Loss: 0.047001276165246964, Memory (GB): 7\n",
      "Epoch 34, Batch 33, Train Loss: 0.036914654076099396, Memory (GB): 7\n",
      "Epoch 34, Batch 34, Train Loss: 0.044245731085538864, Memory (GB): 7\n",
      "Epoch 34, Batch 35, Train Loss: 0.045483481138944626, Memory (GB): 7\n",
      "Epoch 34, Batch 36, Train Loss: 0.04091373085975647, Memory (GB): 7\n",
      "Epoch 34, Batch 37, Train Loss: 0.03827682137489319, Memory (GB): 7\n",
      "Epoch 34, Batch 38, Train Loss: 0.044318731874227524, Memory (GB): 7\n",
      "Epoch 34, Batch 39, Train Loss: 0.0373423770070076, Memory (GB): 7\n",
      "Epoch 34, Batch 40, Train Loss: 0.04385038837790489, Memory (GB): 7\n",
      "Epoch 34, Batch 41, Train Loss: 0.045821089297533035, Memory (GB): 7\n",
      "Epoch 34, Batch 42, Train Loss: 0.04659780487418175, Memory (GB): 7\n",
      "Epoch 34, Batch 43, Train Loss: 0.04848343878984451, Memory (GB): 7\n",
      "Epoch 34, Batch 44, Train Loss: 0.04209772124886513, Memory (GB): 7\n",
      "Epoch 34, Batch 45, Train Loss: 0.04253992810845375, Memory (GB): 7\n",
      "Epoch 34, Batch 46, Train Loss: 0.04452422633767128, Memory (GB): 7\n",
      "Epoch 34, Batch 47, Train Loss: 0.047760266810655594, Memory (GB): 7\n",
      "Epoch 34, Batch 48, Train Loss: 0.047739189118146896, Memory (GB): 7\n",
      "Epoch 34, Batch 49, Train Loss: 0.04229917749762535, Memory (GB): 7\n",
      "Epoch 34, Batch 50, Train Loss: 0.05416414886713028, Memory (GB): 7\n",
      "Epoch 34, Batch 51, Train Loss: 0.04390450194478035, Memory (GB): 7\n",
      "Epoch 34, Batch 52, Train Loss: 0.04539049416780472, Memory (GB): 7\n",
      "Epoch 34, Batch 53, Train Loss: 0.04703816771507263, Memory (GB): 7\n",
      "Epoch 34, Batch 54, Train Loss: 0.04274577647447586, Memory (GB): 7\n",
      "Epoch 34, Batch 55, Train Loss: 0.04140253737568855, Memory (GB): 7\n",
      "Epoch 34, Batch 56, Train Loss: 0.04788198322057724, Memory (GB): 7\n",
      "Epoch 34, Batch 57, Train Loss: 0.054420068860054016, Memory (GB): 7\n",
      "Epoch 34, Batch 58, Train Loss: 0.049333833158016205, Memory (GB): 7\n",
      "Epoch 34, Batch 59, Train Loss: 0.04248260334134102, Memory (GB): 7\n",
      "Epoch 34, Batch 60, Train Loss: 0.048012733459472656, Memory (GB): 7\n",
      "Epoch 34, Batch 61, Train Loss: 0.050538189709186554, Memory (GB): 7\n",
      "Epoch 34, Batch 62, Train Loss: 0.04477574676275253, Memory (GB): 7\n",
      "Epoch 34, Batch 63, Train Loss: 0.043247073888778687, Memory (GB): 7\n",
      "Epoch 34, Batch 64, Train Loss: 0.047013260424137115, Memory (GB): 7\n",
      "Epoch 34, Batch 65, Train Loss: 0.0393206961452961, Memory (GB): 7\n",
      "Epoch 34, Batch 66, Train Loss: 0.05803538113832474, Memory (GB): 7\n",
      "Epoch 34, Batch 67, Train Loss: 0.047739095985889435, Memory (GB): 7\n",
      "Epoch 34, Batch 68, Train Loss: 0.04677120968699455, Memory (GB): 7\n",
      "Epoch 34, Batch 69, Train Loss: 0.05512480437755585, Memory (GB): 7\n",
      "Epoch 34, Batch 70, Train Loss: 0.048506952822208405, Memory (GB): 7\n",
      "Epoch 34, Batch 71, Train Loss: 0.04329809546470642, Memory (GB): 7\n",
      "Epoch 34, Batch 72, Train Loss: 0.04405412822961807, Memory (GB): 7\n",
      "Epoch 34, Batch 73, Train Loss: 0.049873095005750656, Memory (GB): 7\n",
      "Epoch 34, Batch 74, Train Loss: 0.05210677161812782, Memory (GB): 7\n",
      "Epoch 34, Batch 75, Train Loss: 0.047791317105293274, Memory (GB): 7\n",
      "Epoch 34, Batch 76, Train Loss: 0.0519552156329155, Memory (GB): 7\n",
      "Epoch 34, Batch 77, Train Loss: 0.04763038828969002, Memory (GB): 7\n",
      "Epoch 34, Batch 78, Train Loss: 0.05534070357680321, Memory (GB): 7\n",
      "Epoch 34, Batch 79, Train Loss: 0.050176750868558884, Memory (GB): 7\n",
      "Epoch 34, Batch 80, Train Loss: 0.05052030086517334, Memory (GB): 7\n",
      "Epoch 34, Batch 81, Train Loss: 0.0431792251765728, Memory (GB): 7\n",
      "Epoch 34, Batch 82, Train Loss: 0.0581841915845871, Memory (GB): 7\n",
      "Epoch 34, Batch 83, Train Loss: 0.052777647972106934, Memory (GB): 7\n",
      "Epoch 34, Batch 84, Train Loss: 0.045086730271577835, Memory (GB): 7\n",
      "Epoch 34, Batch 85, Train Loss: 0.06008842960000038, Memory (GB): 7\n",
      "Epoch 34, Batch 86, Train Loss: 0.05786905810236931, Memory (GB): 7\n",
      "Epoch 34, Batch 87, Train Loss: 0.052085064351558685, Memory (GB): 7\n",
      "Epoch 34, Batch 88, Train Loss: 0.04487742856144905, Memory (GB): 7\n",
      "Epoch 34, Batch 89, Train Loss: 0.06016508862376213, Memory (GB): 7\n",
      "Epoch 34, Batch 90, Train Loss: 0.04714878648519516, Memory (GB): 7\n",
      "Epoch 34, Batch 91, Train Loss: 0.0422552190721035, Memory (GB): 7\n",
      "Epoch 34, Batch 92, Train Loss: 0.04214194044470787, Memory (GB): 7\n",
      "Epoch 34, Batch 93, Train Loss: 0.048065442591905594, Memory (GB): 7\n",
      "Epoch 34, Batch 94, Train Loss: 0.04450184479355812, Memory (GB): 7\n",
      "Epoch 34, Batch 95, Train Loss: 0.04439259693026543, Memory (GB): 7\n",
      "Epoch 34, Batch 96, Train Loss: 0.04864205792546272, Memory (GB): 7\n",
      "Epoch 34, Batch 97, Train Loss: 0.04602019116282463, Memory (GB): 7\n",
      "Epoch 34, Batch 98, Train Loss: 0.04690216854214668, Memory (GB): 7\n",
      "Epoch 34, Batch 99, Train Loss: 0.04390646889805794, Memory (GB): 7\n",
      "Epoch 34, Batch 100, Train Loss: 0.049927495419979095, Memory (GB): 7\n",
      "Epoch 34, Batch 101, Train Loss: 0.04835480824112892, Memory (GB): 7\n",
      "Epoch 34, Batch 102, Train Loss: 0.04583194851875305, Memory (GB): 7\n",
      "Epoch 34, Batch 103, Train Loss: 0.04885856434702873, Memory (GB): 7\n",
      "Epoch 34, Batch 104, Train Loss: 0.04262840002775192, Memory (GB): 7\n",
      "Epoch 34, Batch 105, Train Loss: 0.05325911194086075, Memory (GB): 7\n",
      "Epoch 34, Batch 106, Train Loss: 0.05546422675251961, Memory (GB): 7\n",
      "Epoch 34, Batch 107, Train Loss: 0.04615414887666702, Memory (GB): 7\n",
      "Epoch 34, Batch 108, Train Loss: 0.04960557445883751, Memory (GB): 7\n",
      "Epoch 34, Batch 109, Train Loss: 0.05002216622233391, Memory (GB): 7\n",
      "Epoch 34, Batch 110, Train Loss: 0.04935656115412712, Memory (GB): 7\n",
      "Epoch 34, Batch 111, Train Loss: 0.04506395757198334, Memory (GB): 7\n",
      "Epoch 34, Batch 112, Train Loss: 0.049129121005535126, Memory (GB): 7\n",
      "Epoch 34, Batch 113, Train Loss: 0.05022142454981804, Memory (GB): 7\n",
      "Epoch 34, Batch 114, Train Loss: 0.041561998426914215, Memory (GB): 7\n",
      "Epoch 34, Batch 115, Train Loss: 0.04246619716286659, Memory (GB): 7\n",
      "Epoch 34, Batch 116, Train Loss: 0.052701015025377274, Memory (GB): 7\n",
      "Epoch 34, Batch 117, Train Loss: 0.044462356716394424, Memory (GB): 7\n",
      "Epoch 34, Batch 118, Train Loss: 0.05252537131309509, Memory (GB): 7\n",
      "Epoch 34, Batch 119, Train Loss: 0.04420346021652222, Memory (GB): 7\n",
      "Epoch 34, Batch 120, Train Loss: 0.04156786948442459, Memory (GB): 7\n",
      "Epoch 34, Batch 121, Train Loss: 0.0473463349044323, Memory (GB): 7\n",
      "Epoch 34, Batch 122, Train Loss: 0.049626611173152924, Memory (GB): 7\n",
      "Epoch 34, Batch 123, Train Loss: 0.04745565727353096, Memory (GB): 7\n",
      "Epoch 34, Batch 124, Train Loss: 0.04833360016345978, Memory (GB): 7\n",
      "Epoch 34, Batch 125, Train Loss: 0.05088215321302414, Memory (GB): 7\n",
      "Epoch 34, Batch 126, Train Loss: 0.04345760866999626, Memory (GB): 7\n",
      "Epoch 34, Batch 127, Train Loss: 0.05234125629067421, Memory (GB): 7\n",
      "Epoch 34, Batch 128, Train Loss: 0.04282398521900177, Memory (GB): 7\n",
      "Epoch 34, Batch 129, Train Loss: 0.0467546284198761, Memory (GB): 7\n",
      "Epoch 34, Batch 130, Train Loss: 0.044894564896821976, Memory (GB): 7\n",
      "Epoch 34, Batch 131, Train Loss: 0.04880529269576073, Memory (GB): 7\n",
      "Epoch 34, Batch 132, Train Loss: 0.044404249638319016, Memory (GB): 7\n",
      "Epoch 34, Batch 133, Train Loss: 0.04460941255092621, Memory (GB): 7\n",
      "Epoch 34, Batch 134, Train Loss: 0.053038857877254486, Memory (GB): 7\n",
      "Epoch 34, Batch 135, Train Loss: 0.04619905352592468, Memory (GB): 7\n",
      "Epoch 34, Batch 136, Train Loss: 0.053030069917440414, Memory (GB): 7\n",
      "Epoch 34, Batch 137, Train Loss: 0.04148048907518387, Memory (GB): 7\n",
      "Epoch 34, Batch 138, Train Loss: 0.05610097572207451, Memory (GB): 7\n",
      "Epoch 34, Batch 139, Train Loss: 0.055991411209106445, Memory (GB): 7\n",
      "Epoch 34, Batch 140, Train Loss: 0.05335729569196701, Memory (GB): 7\n",
      "Epoch 34, Batch 141, Train Loss: 0.0483957938849926, Memory (GB): 7\n",
      "Epoch 34, Batch 142, Train Loss: 0.0504976324737072, Memory (GB): 7\n",
      "Epoch 34, Batch 143, Train Loss: 0.046320654451847076, Memory (GB): 7\n",
      "Epoch 34, Batch 144, Train Loss: 0.04893222451210022, Memory (GB): 7\n",
      "Epoch 34, Batch 145, Train Loss: 0.042127206921577454, Memory (GB): 7\n",
      "Epoch 34, Batch 146, Train Loss: 0.042433302849531174, Memory (GB): 7\n",
      "Epoch 34, Batch 147, Train Loss: 0.04288915917277336, Memory (GB): 7\n",
      "Epoch 34, Batch 148, Train Loss: 0.043082401156425476, Memory (GB): 7\n",
      "Epoch 34, Batch 149, Train Loss: 0.06128918379545212, Memory (GB): 7\n",
      "Epoch 34, Batch 150, Train Loss: 0.04690982401371002, Memory (GB): 7\n",
      "Epoch 34, Batch 151, Train Loss: 0.05282510071992874, Memory (GB): 7\n",
      "Epoch 34, Batch 152, Train Loss: 0.05066008120775223, Memory (GB): 7\n",
      "Epoch 34, Batch 153, Train Loss: 0.04567798227071762, Memory (GB): 7\n",
      "Epoch 34, Batch 154, Train Loss: 0.043018639087677, Memory (GB): 7\n",
      "Epoch 34, Batch 155, Train Loss: 0.04598044604063034, Memory (GB): 7\n",
      "Epoch 34, Batch 156, Train Loss: 0.04817838966846466, Memory (GB): 7\n",
      "Epoch 34, Batch 157, Train Loss: 0.04441652446985245, Memory (GB): 7\n",
      "Epoch 34, Batch 158, Train Loss: 0.04606913402676582, Memory (GB): 7\n",
      "Epoch 34, Batch 159, Train Loss: 0.05063663050532341, Memory (GB): 7\n",
      "Epoch 34, Batch 160, Train Loss: 0.041962120682001114, Memory (GB): 7\n",
      "Epoch 34, Batch 161, Train Loss: 0.040944911539554596, Memory (GB): 7\n",
      "Epoch 34, Batch 162, Train Loss: 0.04985857009887695, Memory (GB): 7\n",
      "Epoch 34, Batch 163, Train Loss: 0.04941072314977646, Memory (GB): 7\n",
      "Epoch 34, Batch 164, Train Loss: 0.056008968502283096, Memory (GB): 7\n",
      "Epoch 34, Batch 165, Train Loss: 0.05169271305203438, Memory (GB): 7\n",
      "Epoch 34, Batch 166, Train Loss: 0.04459822177886963, Memory (GB): 7\n",
      "Epoch 34, Batch 167, Train Loss: 0.053181398659944534, Memory (GB): 7\n",
      "Epoch 34, Batch 168, Train Loss: 0.04655739665031433, Memory (GB): 7\n",
      "Epoch 34, Batch 169, Train Loss: 0.0480673648416996, Memory (GB): 7\n",
      "Epoch 34, Batch 170, Train Loss: 0.061694588512182236, Memory (GB): 7\n",
      "Epoch 34, Batch 171, Train Loss: 0.04686770960688591, Memory (GB): 7\n",
      "Epoch 34, Batch 172, Train Loss: 0.0565887875854969, Memory (GB): 7\n",
      "Epoch 34, Batch 173, Train Loss: 0.0460289902985096, Memory (GB): 7\n",
      "Epoch 34, Batch 174, Train Loss: 0.052864257246255875, Memory (GB): 7\n",
      "Epoch 34, Batch 175, Train Loss: 0.0467071458697319, Memory (GB): 7\n",
      "Epoch 34, Batch 176, Train Loss: 0.053953420370817184, Memory (GB): 7\n",
      "Epoch 34, Batch 177, Train Loss: 0.04326261952519417, Memory (GB): 7\n",
      "Epoch 34, Batch 178, Train Loss: 0.04397227615118027, Memory (GB): 7\n",
      "Epoch 34, Batch 179, Train Loss: 0.05323950946331024, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 35, Batch 0, Train Loss: 0.04024740308523178, Memory (GB): 7\n",
      "Epoch 35, Batch 1, Train Loss: 0.04353296011686325, Memory (GB): 7\n",
      "Epoch 35, Batch 2, Train Loss: 0.05118945613503456, Memory (GB): 7\n",
      "Epoch 35, Batch 3, Train Loss: 0.043164558708667755, Memory (GB): 7\n",
      "Epoch 35, Batch 4, Train Loss: 0.04751120135188103, Memory (GB): 7\n",
      "Epoch 35, Batch 5, Train Loss: 0.04587395489215851, Memory (GB): 7\n",
      "Epoch 35, Batch 6, Train Loss: 0.05163811892271042, Memory (GB): 7\n",
      "Epoch 35, Batch 7, Train Loss: 0.04674587771296501, Memory (GB): 7\n",
      "Epoch 35, Batch 8, Train Loss: 0.03989263251423836, Memory (GB): 7\n",
      "Epoch 35, Batch 9, Train Loss: 0.05310766026377678, Memory (GB): 7\n",
      "Epoch 35, Batch 10, Train Loss: 0.048125311732292175, Memory (GB): 7\n",
      "Epoch 35, Batch 11, Train Loss: 0.0484449528157711, Memory (GB): 7\n",
      "Epoch 35, Batch 12, Train Loss: 0.05152314528822899, Memory (GB): 7\n",
      "Epoch 35, Batch 13, Train Loss: 0.043293435126543045, Memory (GB): 7\n",
      "Epoch 35, Batch 14, Train Loss: 0.049035754054784775, Memory (GB): 7\n",
      "Epoch 35, Batch 15, Train Loss: 0.04116778075695038, Memory (GB): 7\n",
      "Epoch 35, Batch 16, Train Loss: 0.04987982288002968, Memory (GB): 7\n",
      "Epoch 35, Batch 17, Train Loss: 0.0413062646985054, Memory (GB): 7\n",
      "Epoch 35, Batch 18, Train Loss: 0.04179287701845169, Memory (GB): 7\n",
      "Epoch 35, Batch 19, Train Loss: 0.057294685393571854, Memory (GB): 7\n",
      "Epoch 35, Batch 20, Train Loss: 0.03961583971977234, Memory (GB): 7\n",
      "Epoch 35, Batch 21, Train Loss: 0.04650982469320297, Memory (GB): 7\n",
      "Epoch 35, Batch 22, Train Loss: 0.05800081044435501, Memory (GB): 7\n",
      "Epoch 35, Batch 23, Train Loss: 0.04238777607679367, Memory (GB): 7\n",
      "Epoch 35, Batch 24, Train Loss: 0.047799546271562576, Memory (GB): 7\n",
      "Epoch 35, Batch 25, Train Loss: 0.0517038032412529, Memory (GB): 7\n",
      "Epoch 35, Batch 26, Train Loss: 0.04793364554643631, Memory (GB): 7\n",
      "Epoch 35, Batch 27, Train Loss: 0.05039234459400177, Memory (GB): 7\n",
      "Epoch 35, Batch 28, Train Loss: 0.04874786362051964, Memory (GB): 7\n",
      "Epoch 35, Batch 29, Train Loss: 0.04791313782334328, Memory (GB): 7\n",
      "Epoch 35, Batch 30, Train Loss: 0.05438973754644394, Memory (GB): 7\n",
      "Epoch 35, Batch 31, Train Loss: 0.037348516285419464, Memory (GB): 7\n",
      "Epoch 35, Batch 32, Train Loss: 0.04360204190015793, Memory (GB): 7\n",
      "Epoch 35, Batch 33, Train Loss: 0.04390363395214081, Memory (GB): 7\n",
      "Epoch 35, Batch 34, Train Loss: 0.04222226142883301, Memory (GB): 7\n",
      "Epoch 35, Batch 35, Train Loss: 0.04648742079734802, Memory (GB): 7\n",
      "Epoch 35, Batch 36, Train Loss: 0.045502644032239914, Memory (GB): 7\n",
      "Epoch 35, Batch 37, Train Loss: 0.04182231426239014, Memory (GB): 7\n",
      "Epoch 35, Batch 38, Train Loss: 0.04344774782657623, Memory (GB): 7\n",
      "Epoch 35, Batch 39, Train Loss: 0.046446289867162704, Memory (GB): 7\n",
      "Epoch 35, Batch 40, Train Loss: 0.041282616555690765, Memory (GB): 7\n",
      "Epoch 35, Batch 41, Train Loss: 0.04518434405326843, Memory (GB): 7\n",
      "Epoch 35, Batch 42, Train Loss: 0.05826359614729881, Memory (GB): 7\n",
      "Epoch 35, Batch 43, Train Loss: 0.04227854311466217, Memory (GB): 7\n",
      "Epoch 35, Batch 44, Train Loss: 0.044328298419713974, Memory (GB): 7\n",
      "Epoch 35, Batch 45, Train Loss: 0.04901913180947304, Memory (GB): 7\n",
      "Epoch 35, Batch 46, Train Loss: 0.05445784702897072, Memory (GB): 7\n",
      "Epoch 35, Batch 47, Train Loss: 0.044503312557935715, Memory (GB): 7\n",
      "Epoch 35, Batch 48, Train Loss: 0.04421180486679077, Memory (GB): 7\n",
      "Epoch 35, Batch 49, Train Loss: 0.03852470591664314, Memory (GB): 7\n",
      "Epoch 35, Batch 50, Train Loss: 0.047544561326503754, Memory (GB): 7\n",
      "Epoch 35, Batch 51, Train Loss: 0.04744192957878113, Memory (GB): 7\n",
      "Epoch 35, Batch 52, Train Loss: 0.04874676093459129, Memory (GB): 7\n",
      "Epoch 35, Batch 53, Train Loss: 0.0431160032749176, Memory (GB): 7\n",
      "Epoch 35, Batch 54, Train Loss: 0.0419994592666626, Memory (GB): 7\n",
      "Epoch 35, Batch 55, Train Loss: 0.045124683529138565, Memory (GB): 7\n",
      "Epoch 35, Batch 56, Train Loss: 0.04064738750457764, Memory (GB): 7\n",
      "Epoch 35, Batch 57, Train Loss: 0.04065057635307312, Memory (GB): 7\n",
      "Epoch 35, Batch 58, Train Loss: 0.03898952528834343, Memory (GB): 7\n",
      "Epoch 35, Batch 59, Train Loss: 0.03888367861509323, Memory (GB): 7\n",
      "Epoch 35, Batch 60, Train Loss: 0.045705728232860565, Memory (GB): 7\n",
      "Epoch 35, Batch 61, Train Loss: 0.04038349911570549, Memory (GB): 7\n",
      "Epoch 35, Batch 62, Train Loss: 0.046456824988126755, Memory (GB): 7\n",
      "Epoch 35, Batch 63, Train Loss: 0.03932937979698181, Memory (GB): 7\n",
      "Epoch 35, Batch 64, Train Loss: 0.04004018008708954, Memory (GB): 7\n",
      "Epoch 35, Batch 65, Train Loss: 0.039752207696437836, Memory (GB): 7\n",
      "Epoch 35, Batch 66, Train Loss: 0.04616633430123329, Memory (GB): 7\n",
      "Epoch 35, Batch 67, Train Loss: 0.04962999373674393, Memory (GB): 7\n",
      "Epoch 35, Batch 68, Train Loss: 0.04287460073828697, Memory (GB): 7\n",
      "Epoch 35, Batch 69, Train Loss: 0.04453550651669502, Memory (GB): 7\n",
      "Epoch 35, Batch 70, Train Loss: 0.04675312340259552, Memory (GB): 7\n",
      "Epoch 35, Batch 71, Train Loss: 0.044532809406518936, Memory (GB): 7\n",
      "Epoch 35, Batch 72, Train Loss: 0.04924510791897774, Memory (GB): 7\n",
      "Epoch 35, Batch 73, Train Loss: 0.0474141426384449, Memory (GB): 7\n",
      "Epoch 35, Batch 74, Train Loss: 0.03827229142189026, Memory (GB): 7\n",
      "Epoch 35, Batch 75, Train Loss: 0.04738077148795128, Memory (GB): 7\n",
      "Epoch 35, Batch 76, Train Loss: 0.04324640333652496, Memory (GB): 7\n",
      "Epoch 35, Batch 77, Train Loss: 0.04739901050925255, Memory (GB): 7\n",
      "Epoch 35, Batch 78, Train Loss: 0.04037356376647949, Memory (GB): 7\n",
      "Epoch 35, Batch 79, Train Loss: 0.0474962554872036, Memory (GB): 7\n",
      "Epoch 35, Batch 80, Train Loss: 0.037881039083004, Memory (GB): 7\n",
      "Epoch 35, Batch 81, Train Loss: 0.05014675483107567, Memory (GB): 7\n",
      "Epoch 35, Batch 82, Train Loss: 0.04258247837424278, Memory (GB): 7\n",
      "Epoch 35, Batch 83, Train Loss: 0.057352215051651, Memory (GB): 7\n",
      "Epoch 35, Batch 84, Train Loss: 0.041342414915561676, Memory (GB): 7\n",
      "Epoch 35, Batch 85, Train Loss: 0.044818539172410965, Memory (GB): 7\n",
      "Epoch 35, Batch 86, Train Loss: 0.05307302996516228, Memory (GB): 7\n",
      "Epoch 35, Batch 87, Train Loss: 0.04225166141986847, Memory (GB): 7\n",
      "Epoch 35, Batch 88, Train Loss: 0.03737559914588928, Memory (GB): 7\n",
      "Epoch 35, Batch 89, Train Loss: 0.046666745096445084, Memory (GB): 7\n",
      "Epoch 35, Batch 90, Train Loss: 0.050046756863594055, Memory (GB): 7\n",
      "Epoch 35, Batch 91, Train Loss: 0.03865096718072891, Memory (GB): 7\n",
      "Epoch 35, Batch 92, Train Loss: 0.045718953013420105, Memory (GB): 7\n",
      "Epoch 35, Batch 93, Train Loss: 0.04917459934949875, Memory (GB): 7\n",
      "Epoch 35, Batch 94, Train Loss: 0.040654104202985764, Memory (GB): 7\n",
      "Epoch 35, Batch 95, Train Loss: 0.041234325617551804, Memory (GB): 7\n",
      "Epoch 35, Batch 96, Train Loss: 0.04606202617287636, Memory (GB): 7\n",
      "Epoch 35, Batch 97, Train Loss: 0.04800765961408615, Memory (GB): 7\n",
      "Epoch 35, Batch 98, Train Loss: 0.04634084925055504, Memory (GB): 7\n",
      "Epoch 35, Batch 99, Train Loss: 0.03882497921586037, Memory (GB): 7\n",
      "Epoch 35, Batch 100, Train Loss: 0.04886127635836601, Memory (GB): 7\n",
      "Epoch 35, Batch 101, Train Loss: 0.04636649787425995, Memory (GB): 7\n",
      "Epoch 35, Batch 102, Train Loss: 0.0444285050034523, Memory (GB): 7\n",
      "Epoch 35, Batch 103, Train Loss: 0.043109167367219925, Memory (GB): 7\n",
      "Epoch 35, Batch 104, Train Loss: 0.038959402590990067, Memory (GB): 7\n",
      "Epoch 35, Batch 105, Train Loss: 0.03917372226715088, Memory (GB): 7\n",
      "Epoch 35, Batch 106, Train Loss: 0.05020226910710335, Memory (GB): 7\n",
      "Epoch 35, Batch 107, Train Loss: 0.04190133139491081, Memory (GB): 7\n",
      "Epoch 35, Batch 108, Train Loss: 0.04023139551281929, Memory (GB): 7\n",
      "Epoch 35, Batch 109, Train Loss: 0.040671639144420624, Memory (GB): 7\n",
      "Epoch 35, Batch 110, Train Loss: 0.04710143059492111, Memory (GB): 7\n",
      "Epoch 35, Batch 111, Train Loss: 0.044187165796756744, Memory (GB): 7\n",
      "Epoch 35, Batch 112, Train Loss: 0.05026073753833771, Memory (GB): 7\n",
      "Epoch 35, Batch 113, Train Loss: 0.040946051478385925, Memory (GB): 7\n",
      "Epoch 35, Batch 114, Train Loss: 0.04367278143763542, Memory (GB): 7\n",
      "Epoch 35, Batch 115, Train Loss: 0.03807692229747772, Memory (GB): 7\n",
      "Epoch 35, Batch 116, Train Loss: 0.04999803379178047, Memory (GB): 7\n",
      "Epoch 35, Batch 117, Train Loss: 0.0529363788664341, Memory (GB): 7\n",
      "Epoch 35, Batch 118, Train Loss: 0.04527495801448822, Memory (GB): 7\n",
      "Epoch 35, Batch 119, Train Loss: 0.04192514345049858, Memory (GB): 7\n",
      "Epoch 35, Batch 120, Train Loss: 0.04740247502923012, Memory (GB): 7\n",
      "Epoch 35, Batch 121, Train Loss: 0.05072503909468651, Memory (GB): 7\n",
      "Epoch 35, Batch 122, Train Loss: 0.045113857835531235, Memory (GB): 7\n",
      "Epoch 35, Batch 123, Train Loss: 0.0432320274412632, Memory (GB): 7\n",
      "Epoch 35, Batch 124, Train Loss: 0.047505609691143036, Memory (GB): 7\n",
      "Epoch 35, Batch 125, Train Loss: 0.040535226464271545, Memory (GB): 7\n",
      "Epoch 35, Batch 126, Train Loss: 0.03942034766077995, Memory (GB): 7\n",
      "Epoch 35, Batch 127, Train Loss: 0.0429389588534832, Memory (GB): 7\n",
      "Epoch 35, Batch 128, Train Loss: 0.04399450123310089, Memory (GB): 7\n",
      "Epoch 35, Batch 129, Train Loss: 0.047108475118875504, Memory (GB): 7\n",
      "Epoch 35, Batch 130, Train Loss: 0.04513490945100784, Memory (GB): 7\n",
      "Epoch 35, Batch 131, Train Loss: 0.04302913323044777, Memory (GB): 7\n",
      "Epoch 35, Batch 132, Train Loss: 0.044316548854112625, Memory (GB): 7\n",
      "Epoch 35, Batch 133, Train Loss: 0.042671117931604385, Memory (GB): 7\n",
      "Epoch 35, Batch 134, Train Loss: 0.04450910910964012, Memory (GB): 7\n",
      "Epoch 35, Batch 135, Train Loss: 0.04032271355390549, Memory (GB): 7\n",
      "Epoch 35, Batch 136, Train Loss: 0.04173373430967331, Memory (GB): 7\n",
      "Epoch 35, Batch 137, Train Loss: 0.03855413943529129, Memory (GB): 7\n",
      "Epoch 35, Batch 138, Train Loss: 0.04928038269281387, Memory (GB): 7\n",
      "Epoch 35, Batch 139, Train Loss: 0.03946195915341377, Memory (GB): 7\n",
      "Epoch 35, Batch 140, Train Loss: 0.04690789058804512, Memory (GB): 7\n",
      "Epoch 35, Batch 141, Train Loss: 0.04328816384077072, Memory (GB): 7\n",
      "Epoch 35, Batch 142, Train Loss: 0.040322430431842804, Memory (GB): 7\n",
      "Epoch 35, Batch 143, Train Loss: 0.0502072349190712, Memory (GB): 7\n",
      "Epoch 35, Batch 144, Train Loss: 0.046755217015743256, Memory (GB): 7\n",
      "Epoch 35, Batch 145, Train Loss: 0.04467790201306343, Memory (GB): 7\n",
      "Epoch 35, Batch 146, Train Loss: 0.04070650041103363, Memory (GB): 7\n",
      "Epoch 35, Batch 147, Train Loss: 0.041378505527973175, Memory (GB): 7\n",
      "Epoch 35, Batch 148, Train Loss: 0.040001507848501205, Memory (GB): 7\n",
      "Epoch 35, Batch 149, Train Loss: 0.043817948549985886, Memory (GB): 7\n",
      "Epoch 35, Batch 150, Train Loss: 0.04242151603102684, Memory (GB): 7\n",
      "Epoch 35, Batch 151, Train Loss: 0.04060979187488556, Memory (GB): 7\n",
      "Epoch 35, Batch 152, Train Loss: 0.03905392810702324, Memory (GB): 7\n",
      "Epoch 35, Batch 153, Train Loss: 0.04172811657190323, Memory (GB): 7\n",
      "Epoch 35, Batch 154, Train Loss: 0.03838507831096649, Memory (GB): 7\n",
      "Epoch 35, Batch 155, Train Loss: 0.0460144579410553, Memory (GB): 7\n",
      "Epoch 35, Batch 156, Train Loss: 0.04548971727490425, Memory (GB): 7\n",
      "Epoch 35, Batch 157, Train Loss: 0.0420425720512867, Memory (GB): 7\n",
      "Epoch 35, Batch 158, Train Loss: 0.05219747871160507, Memory (GB): 7\n",
      "Epoch 35, Batch 159, Train Loss: 0.0422888845205307, Memory (GB): 7\n",
      "Epoch 35, Batch 160, Train Loss: 0.04545897990465164, Memory (GB): 7\n",
      "Epoch 35, Batch 161, Train Loss: 0.05595479533076286, Memory (GB): 7\n",
      "Epoch 35, Batch 162, Train Loss: 0.04675361514091492, Memory (GB): 7\n",
      "Epoch 35, Batch 163, Train Loss: 0.048444803804159164, Memory (GB): 7\n",
      "Epoch 35, Batch 164, Train Loss: 0.048447925597429276, Memory (GB): 7\n",
      "Epoch 35, Batch 165, Train Loss: 0.04460981860756874, Memory (GB): 7\n",
      "Epoch 35, Batch 166, Train Loss: 0.052669819444417953, Memory (GB): 7\n",
      "Epoch 35, Batch 167, Train Loss: 0.04411327466368675, Memory (GB): 7\n",
      "Epoch 35, Batch 168, Train Loss: 0.05612789839506149, Memory (GB): 7\n",
      "Epoch 35, Batch 169, Train Loss: 0.042659394443035126, Memory (GB): 7\n",
      "Epoch 35, Batch 170, Train Loss: 0.04186864197254181, Memory (GB): 7\n",
      "Epoch 35, Batch 171, Train Loss: 0.040418513119220734, Memory (GB): 7\n",
      "Epoch 35, Batch 172, Train Loss: 0.048922453075647354, Memory (GB): 7\n",
      "Epoch 35, Batch 173, Train Loss: 0.04202884063124657, Memory (GB): 7\n",
      "Epoch 35, Batch 174, Train Loss: 0.04430116340517998, Memory (GB): 7\n",
      "Epoch 35, Batch 175, Train Loss: 0.04532244801521301, Memory (GB): 7\n",
      "Epoch 35, Batch 176, Train Loss: 0.04661191254854202, Memory (GB): 7\n",
      "Epoch 35, Batch 177, Train Loss: 0.040332645177841187, Memory (GB): 7\n",
      "Epoch 35, Batch 178, Train Loss: 0.04627662152051926, Memory (GB): 7\n",
      "Epoch 35, Batch 179, Train Loss: 0.0523342490196228, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 36, Batch 0, Train Loss: 0.045006170868873596, Memory (GB): 7\n",
      "Epoch 36, Batch 1, Train Loss: 0.038087110966444016, Memory (GB): 7\n",
      "Epoch 36, Batch 2, Train Loss: 0.042430680245161057, Memory (GB): 7\n",
      "Epoch 36, Batch 3, Train Loss: 0.04161196947097778, Memory (GB): 7\n",
      "Epoch 36, Batch 4, Train Loss: 0.04517881199717522, Memory (GB): 7\n",
      "Epoch 36, Batch 5, Train Loss: 0.045060936361551285, Memory (GB): 7\n",
      "Epoch 36, Batch 6, Train Loss: 0.04149562120437622, Memory (GB): 7\n",
      "Epoch 36, Batch 7, Train Loss: 0.04425304755568504, Memory (GB): 7\n",
      "Epoch 36, Batch 8, Train Loss: 0.046449530869722366, Memory (GB): 7\n",
      "Epoch 36, Batch 9, Train Loss: 0.04472137242555618, Memory (GB): 7\n",
      "Epoch 36, Batch 10, Train Loss: 0.03969290852546692, Memory (GB): 7\n",
      "Epoch 36, Batch 11, Train Loss: 0.036687832325696945, Memory (GB): 7\n",
      "Epoch 36, Batch 12, Train Loss: 0.04835255816578865, Memory (GB): 7\n",
      "Epoch 36, Batch 13, Train Loss: 0.040060218423604965, Memory (GB): 7\n",
      "Epoch 36, Batch 14, Train Loss: 0.04590708762407303, Memory (GB): 7\n",
      "Epoch 36, Batch 15, Train Loss: 0.049547627568244934, Memory (GB): 7\n",
      "Epoch 36, Batch 16, Train Loss: 0.03852444514632225, Memory (GB): 7\n",
      "Epoch 36, Batch 17, Train Loss: 0.04613737389445305, Memory (GB): 7\n",
      "Epoch 36, Batch 18, Train Loss: 0.03702773526310921, Memory (GB): 7\n",
      "Epoch 36, Batch 19, Train Loss: 0.04086073860526085, Memory (GB): 7\n",
      "Epoch 36, Batch 20, Train Loss: 0.038879431784152985, Memory (GB): 7\n",
      "Epoch 36, Batch 21, Train Loss: 0.04421977326273918, Memory (GB): 7\n",
      "Epoch 36, Batch 22, Train Loss: 0.03925337642431259, Memory (GB): 7\n",
      "Epoch 36, Batch 23, Train Loss: 0.04010022431612015, Memory (GB): 7\n",
      "Epoch 36, Batch 24, Train Loss: 0.0417439304292202, Memory (GB): 7\n",
      "Epoch 36, Batch 25, Train Loss: 0.03954053670167923, Memory (GB): 7\n",
      "Epoch 36, Batch 26, Train Loss: 0.04215506091713905, Memory (GB): 7\n",
      "Epoch 36, Batch 27, Train Loss: 0.05044585093855858, Memory (GB): 7\n",
      "Epoch 36, Batch 28, Train Loss: 0.0419134683907032, Memory (GB): 7\n",
      "Epoch 36, Batch 29, Train Loss: 0.04394553229212761, Memory (GB): 7\n",
      "Epoch 36, Batch 30, Train Loss: 0.041985996067523956, Memory (GB): 7\n",
      "Epoch 36, Batch 31, Train Loss: 0.047751255333423615, Memory (GB): 7\n",
      "Epoch 36, Batch 32, Train Loss: 0.03977878391742706, Memory (GB): 7\n",
      "Epoch 36, Batch 33, Train Loss: 0.05285156890749931, Memory (GB): 7\n",
      "Epoch 36, Batch 34, Train Loss: 0.04200471192598343, Memory (GB): 7\n",
      "Epoch 36, Batch 35, Train Loss: 0.04374619200825691, Memory (GB): 7\n",
      "Epoch 36, Batch 36, Train Loss: 0.038438551127910614, Memory (GB): 7\n",
      "Epoch 36, Batch 37, Train Loss: 0.04419346898794174, Memory (GB): 7\n",
      "Epoch 36, Batch 38, Train Loss: 0.04208476468920708, Memory (GB): 7\n",
      "Epoch 36, Batch 39, Train Loss: 0.04692041873931885, Memory (GB): 7\n",
      "Epoch 36, Batch 40, Train Loss: 0.04432138800621033, Memory (GB): 7\n",
      "Epoch 36, Batch 41, Train Loss: 0.04203486442565918, Memory (GB): 7\n",
      "Epoch 36, Batch 42, Train Loss: 0.04693346470594406, Memory (GB): 7\n",
      "Epoch 36, Batch 43, Train Loss: 0.04295683652162552, Memory (GB): 7\n",
      "Epoch 36, Batch 44, Train Loss: 0.03817642852663994, Memory (GB): 7\n",
      "Epoch 36, Batch 45, Train Loss: 0.04187856242060661, Memory (GB): 7\n",
      "Epoch 36, Batch 46, Train Loss: 0.04147974029183388, Memory (GB): 7\n",
      "Epoch 36, Batch 47, Train Loss: 0.054247837513685226, Memory (GB): 7\n",
      "Epoch 36, Batch 48, Train Loss: 0.03860335052013397, Memory (GB): 7\n",
      "Epoch 36, Batch 49, Train Loss: 0.04451804608106613, Memory (GB): 7\n",
      "Epoch 36, Batch 50, Train Loss: 0.03823002055287361, Memory (GB): 7\n",
      "Epoch 36, Batch 51, Train Loss: 0.04144570976495743, Memory (GB): 7\n",
      "Epoch 36, Batch 52, Train Loss: 0.05005374923348427, Memory (GB): 7\n",
      "Epoch 36, Batch 53, Train Loss: 0.03723478689789772, Memory (GB): 7\n",
      "Epoch 36, Batch 54, Train Loss: 0.04153849929571152, Memory (GB): 7\n",
      "Epoch 36, Batch 55, Train Loss: 0.04552542418241501, Memory (GB): 7\n",
      "Epoch 36, Batch 56, Train Loss: 0.03553503751754761, Memory (GB): 7\n",
      "Epoch 36, Batch 57, Train Loss: 0.045042771846055984, Memory (GB): 7\n",
      "Epoch 36, Batch 58, Train Loss: 0.03944223374128342, Memory (GB): 7\n",
      "Epoch 36, Batch 59, Train Loss: 0.043912820518016815, Memory (GB): 7\n",
      "Epoch 36, Batch 60, Train Loss: 0.03673012927174568, Memory (GB): 7\n",
      "Epoch 36, Batch 61, Train Loss: 0.04224780574440956, Memory (GB): 7\n",
      "Epoch 36, Batch 62, Train Loss: 0.04888292774558067, Memory (GB): 7\n",
      "Epoch 36, Batch 63, Train Loss: 0.04359641298651695, Memory (GB): 7\n",
      "Epoch 36, Batch 64, Train Loss: 0.03878343850374222, Memory (GB): 7\n",
      "Epoch 36, Batch 65, Train Loss: 0.0430884063243866, Memory (GB): 7\n",
      "Epoch 36, Batch 66, Train Loss: 0.0486670546233654, Memory (GB): 7\n",
      "Epoch 36, Batch 67, Train Loss: 0.040453068912029266, Memory (GB): 7\n",
      "Epoch 36, Batch 68, Train Loss: 0.03808214142918587, Memory (GB): 7\n",
      "Epoch 36, Batch 69, Train Loss: 0.04694928601384163, Memory (GB): 7\n",
      "Epoch 36, Batch 70, Train Loss: 0.046887997537851334, Memory (GB): 7\n",
      "Epoch 36, Batch 71, Train Loss: 0.03625185787677765, Memory (GB): 7\n",
      "Epoch 36, Batch 72, Train Loss: 0.05898073688149452, Memory (GB): 7\n",
      "Epoch 36, Batch 73, Train Loss: 0.03672047331929207, Memory (GB): 7\n",
      "Epoch 36, Batch 74, Train Loss: 0.04412318021059036, Memory (GB): 7\n",
      "Epoch 36, Batch 75, Train Loss: 0.039571039378643036, Memory (GB): 7\n",
      "Epoch 36, Batch 76, Train Loss: 0.05549878254532814, Memory (GB): 7\n",
      "Epoch 36, Batch 77, Train Loss: 0.05118423327803612, Memory (GB): 7\n",
      "Epoch 36, Batch 78, Train Loss: 0.037751663476228714, Memory (GB): 7\n",
      "Epoch 36, Batch 79, Train Loss: 0.03934512659907341, Memory (GB): 7\n",
      "Epoch 36, Batch 80, Train Loss: 0.05519633740186691, Memory (GB): 7\n",
      "Epoch 36, Batch 81, Train Loss: 0.042152732610702515, Memory (GB): 7\n",
      "Epoch 36, Batch 82, Train Loss: 0.04124760255217552, Memory (GB): 7\n",
      "Epoch 36, Batch 83, Train Loss: 0.05277024954557419, Memory (GB): 7\n",
      "Epoch 36, Batch 84, Train Loss: 0.04063667729496956, Memory (GB): 7\n",
      "Epoch 36, Batch 85, Train Loss: 0.03921685367822647, Memory (GB): 7\n",
      "Epoch 36, Batch 86, Train Loss: 0.0467744879424572, Memory (GB): 7\n",
      "Epoch 36, Batch 87, Train Loss: 0.04290810599923134, Memory (GB): 7\n",
      "Epoch 36, Batch 88, Train Loss: 0.053403548896312714, Memory (GB): 7\n",
      "Epoch 36, Batch 89, Train Loss: 0.04579491540789604, Memory (GB): 7\n",
      "Epoch 36, Batch 90, Train Loss: 0.038208018988370895, Memory (GB): 7\n",
      "Epoch 36, Batch 91, Train Loss: 0.050384920090436935, Memory (GB): 7\n",
      "Epoch 36, Batch 92, Train Loss: 0.04958077892661095, Memory (GB): 7\n",
      "Epoch 36, Batch 93, Train Loss: 0.03895638510584831, Memory (GB): 7\n",
      "Epoch 36, Batch 94, Train Loss: 0.054446786642074585, Memory (GB): 7\n",
      "Epoch 36, Batch 95, Train Loss: 0.04376676306128502, Memory (GB): 7\n",
      "Epoch 36, Batch 96, Train Loss: 0.042564090341329575, Memory (GB): 7\n",
      "Epoch 36, Batch 97, Train Loss: 0.03947389870882034, Memory (GB): 7\n",
      "Epoch 36, Batch 98, Train Loss: 0.04605577886104584, Memory (GB): 7\n",
      "Epoch 36, Batch 99, Train Loss: 0.04262150824069977, Memory (GB): 7\n",
      "Epoch 36, Batch 100, Train Loss: 0.042677659541368484, Memory (GB): 7\n",
      "Epoch 36, Batch 101, Train Loss: 0.04317686706781387, Memory (GB): 7\n",
      "Epoch 36, Batch 102, Train Loss: 0.04261205345392227, Memory (GB): 7\n",
      "Epoch 36, Batch 103, Train Loss: 0.04530242085456848, Memory (GB): 7\n",
      "Epoch 36, Batch 104, Train Loss: 0.04212456941604614, Memory (GB): 7\n",
      "Epoch 36, Batch 105, Train Loss: 0.04307517781853676, Memory (GB): 7\n",
      "Epoch 36, Batch 106, Train Loss: 0.03597114607691765, Memory (GB): 7\n",
      "Epoch 36, Batch 107, Train Loss: 0.03942430764436722, Memory (GB): 7\n",
      "Epoch 36, Batch 108, Train Loss: 0.04343368858098984, Memory (GB): 7\n",
      "Epoch 36, Batch 109, Train Loss: 0.038833994418382645, Memory (GB): 7\n",
      "Epoch 36, Batch 110, Train Loss: 0.04609231650829315, Memory (GB): 7\n",
      "Epoch 36, Batch 111, Train Loss: 0.04334881529211998, Memory (GB): 7\n",
      "Epoch 36, Batch 112, Train Loss: 0.04522939398884773, Memory (GB): 7\n",
      "Epoch 36, Batch 113, Train Loss: 0.03911063075065613, Memory (GB): 7\n",
      "Epoch 36, Batch 114, Train Loss: 0.04473438486456871, Memory (GB): 7\n",
      "Epoch 36, Batch 115, Train Loss: 0.05029500275850296, Memory (GB): 7\n",
      "Epoch 36, Batch 116, Train Loss: 0.037684690207242966, Memory (GB): 7\n",
      "Epoch 36, Batch 117, Train Loss: 0.03581402823328972, Memory (GB): 7\n",
      "Epoch 36, Batch 118, Train Loss: 0.03848320245742798, Memory (GB): 7\n",
      "Epoch 36, Batch 119, Train Loss: 0.0404280349612236, Memory (GB): 7\n",
      "Epoch 36, Batch 120, Train Loss: 0.04447462409734726, Memory (GB): 7\n",
      "Epoch 36, Batch 121, Train Loss: 0.04368259385228157, Memory (GB): 7\n",
      "Epoch 36, Batch 122, Train Loss: 0.044290825724601746, Memory (GB): 7\n",
      "Epoch 36, Batch 123, Train Loss: 0.04828799515962601, Memory (GB): 7\n",
      "Epoch 36, Batch 124, Train Loss: 0.039284128695726395, Memory (GB): 7\n",
      "Epoch 36, Batch 125, Train Loss: 0.04721291735768318, Memory (GB): 7\n",
      "Epoch 36, Batch 126, Train Loss: 0.03682582825422287, Memory (GB): 7\n",
      "Epoch 36, Batch 127, Train Loss: 0.04022720828652382, Memory (GB): 7\n",
      "Epoch 36, Batch 128, Train Loss: 0.04002915695309639, Memory (GB): 7\n",
      "Epoch 36, Batch 129, Train Loss: 0.03585575148463249, Memory (GB): 7\n",
      "Epoch 36, Batch 130, Train Loss: 0.05165631324052811, Memory (GB): 7\n",
      "Epoch 36, Batch 131, Train Loss: 0.03761199489235878, Memory (GB): 7\n",
      "Epoch 36, Batch 132, Train Loss: 0.051751989871263504, Memory (GB): 7\n",
      "Epoch 36, Batch 133, Train Loss: 0.03924890235066414, Memory (GB): 7\n",
      "Epoch 36, Batch 134, Train Loss: 0.04866498336195946, Memory (GB): 7\n",
      "Epoch 36, Batch 135, Train Loss: 0.04809378832578659, Memory (GB): 7\n",
      "Epoch 36, Batch 136, Train Loss: 0.039107609540224075, Memory (GB): 7\n",
      "Epoch 36, Batch 137, Train Loss: 0.039671678096055984, Memory (GB): 7\n",
      "Epoch 36, Batch 138, Train Loss: 0.03762815147638321, Memory (GB): 7\n",
      "Epoch 36, Batch 139, Train Loss: 0.03979228064417839, Memory (GB): 7\n",
      "Epoch 36, Batch 140, Train Loss: 0.04911157116293907, Memory (GB): 7\n",
      "Epoch 36, Batch 141, Train Loss: 0.04084474593400955, Memory (GB): 7\n",
      "Epoch 36, Batch 142, Train Loss: 0.041019752621650696, Memory (GB): 7\n",
      "Epoch 36, Batch 143, Train Loss: 0.0444803312420845, Memory (GB): 7\n",
      "Epoch 36, Batch 144, Train Loss: 0.04250843822956085, Memory (GB): 7\n",
      "Epoch 36, Batch 145, Train Loss: 0.04389525204896927, Memory (GB): 7\n",
      "Epoch 36, Batch 146, Train Loss: 0.04157840833067894, Memory (GB): 7\n",
      "Epoch 36, Batch 147, Train Loss: 0.036845818161964417, Memory (GB): 7\n",
      "Epoch 36, Batch 148, Train Loss: 0.039110422134399414, Memory (GB): 7\n",
      "Epoch 36, Batch 149, Train Loss: 0.04229728877544403, Memory (GB): 7\n",
      "Epoch 36, Batch 150, Train Loss: 0.03699502348899841, Memory (GB): 7\n",
      "Epoch 36, Batch 151, Train Loss: 0.03643328323960304, Memory (GB): 7\n",
      "Epoch 36, Batch 152, Train Loss: 0.03900957107543945, Memory (GB): 7\n",
      "Epoch 36, Batch 153, Train Loss: 0.04550810530781746, Memory (GB): 7\n",
      "Epoch 36, Batch 154, Train Loss: 0.03817497193813324, Memory (GB): 7\n",
      "Epoch 36, Batch 155, Train Loss: 0.0375104621052742, Memory (GB): 7\n",
      "Epoch 36, Batch 156, Train Loss: 0.04210488125681877, Memory (GB): 7\n",
      "Epoch 36, Batch 157, Train Loss: 0.03774042800068855, Memory (GB): 7\n",
      "Epoch 36, Batch 158, Train Loss: 0.04223252087831497, Memory (GB): 7\n",
      "Epoch 36, Batch 159, Train Loss: 0.03554065525531769, Memory (GB): 7\n",
      "Epoch 36, Batch 160, Train Loss: 0.04273192584514618, Memory (GB): 7\n",
      "Epoch 36, Batch 161, Train Loss: 0.039220694452524185, Memory (GB): 7\n",
      "Epoch 36, Batch 162, Train Loss: 0.03829367458820343, Memory (GB): 7\n",
      "Epoch 36, Batch 163, Train Loss: 0.04341210797429085, Memory (GB): 7\n",
      "Epoch 36, Batch 164, Train Loss: 0.04262612387537956, Memory (GB): 7\n",
      "Epoch 36, Batch 165, Train Loss: 0.038796622306108475, Memory (GB): 7\n",
      "Epoch 36, Batch 166, Train Loss: 0.04331531003117561, Memory (GB): 7\n",
      "Epoch 36, Batch 167, Train Loss: 0.03940628468990326, Memory (GB): 7\n",
      "Epoch 36, Batch 168, Train Loss: 0.04942184314131737, Memory (GB): 7\n",
      "Epoch 36, Batch 169, Train Loss: 0.037165023386478424, Memory (GB): 7\n",
      "Epoch 36, Batch 170, Train Loss: 0.0374763086438179, Memory (GB): 7\n",
      "Epoch 36, Batch 171, Train Loss: 0.036852747201919556, Memory (GB): 7\n",
      "Epoch 36, Batch 172, Train Loss: 0.04286196082830429, Memory (GB): 7\n",
      "Epoch 36, Batch 173, Train Loss: 0.040951430797576904, Memory (GB): 7\n",
      "Epoch 36, Batch 174, Train Loss: 0.03938662260770798, Memory (GB): 7\n",
      "Epoch 36, Batch 175, Train Loss: 0.03760852292180061, Memory (GB): 7\n",
      "Epoch 36, Batch 176, Train Loss: 0.03890664130449295, Memory (GB): 7\n",
      "Epoch 36, Batch 177, Train Loss: 0.054795775562524796, Memory (GB): 7\n",
      "Epoch 36, Batch 178, Train Loss: 0.04205537587404251, Memory (GB): 7\n",
      "Epoch 36, Batch 179, Train Loss: 0.03989625722169876, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 37, Batch 0, Train Loss: 0.040770113468170166, Memory (GB): 7\n",
      "Epoch 37, Batch 1, Train Loss: 0.04866764694452286, Memory (GB): 7\n",
      "Epoch 37, Batch 2, Train Loss: 0.04458114504814148, Memory (GB): 7\n",
      "Epoch 37, Batch 3, Train Loss: 0.04056861996650696, Memory (GB): 7\n",
      "Epoch 37, Batch 4, Train Loss: 0.037823017686605453, Memory (GB): 7\n",
      "Epoch 37, Batch 5, Train Loss: 0.04167468100786209, Memory (GB): 7\n",
      "Epoch 37, Batch 6, Train Loss: 0.04060167446732521, Memory (GB): 7\n",
      "Epoch 37, Batch 7, Train Loss: 0.04221966117620468, Memory (GB): 7\n",
      "Epoch 37, Batch 8, Train Loss: 0.03883684426546097, Memory (GB): 7\n",
      "Epoch 37, Batch 9, Train Loss: 0.03991825133562088, Memory (GB): 7\n",
      "Epoch 37, Batch 10, Train Loss: 0.042878709733486176, Memory (GB): 7\n",
      "Epoch 37, Batch 11, Train Loss: 0.03859958052635193, Memory (GB): 7\n",
      "Epoch 37, Batch 12, Train Loss: 0.05441227927803993, Memory (GB): 7\n",
      "Epoch 37, Batch 13, Train Loss: 0.04584582895040512, Memory (GB): 7\n",
      "Epoch 37, Batch 14, Train Loss: 0.03851653262972832, Memory (GB): 7\n",
      "Epoch 37, Batch 15, Train Loss: 0.04197311028838158, Memory (GB): 7\n",
      "Epoch 37, Batch 16, Train Loss: 0.03861703723669052, Memory (GB): 7\n",
      "Epoch 37, Batch 17, Train Loss: 0.03863050788640976, Memory (GB): 7\n",
      "Epoch 37, Batch 18, Train Loss: 0.039336688816547394, Memory (GB): 7\n",
      "Epoch 37, Batch 19, Train Loss: 0.03671596199274063, Memory (GB): 7\n",
      "Epoch 37, Batch 20, Train Loss: 0.038927145302295685, Memory (GB): 7\n",
      "Epoch 37, Batch 21, Train Loss: 0.03645704314112663, Memory (GB): 7\n",
      "Epoch 37, Batch 22, Train Loss: 0.043086934834718704, Memory (GB): 7\n",
      "Epoch 37, Batch 23, Train Loss: 0.03761928528547287, Memory (GB): 7\n",
      "Epoch 37, Batch 24, Train Loss: 0.05242817848920822, Memory (GB): 7\n",
      "Epoch 37, Batch 25, Train Loss: 0.0391455739736557, Memory (GB): 7\n",
      "Epoch 37, Batch 26, Train Loss: 0.042815227061510086, Memory (GB): 7\n",
      "Epoch 37, Batch 27, Train Loss: 0.03636486828327179, Memory (GB): 7\n",
      "Epoch 37, Batch 28, Train Loss: 0.03938976302742958, Memory (GB): 7\n",
      "Epoch 37, Batch 29, Train Loss: 0.036142490804195404, Memory (GB): 7\n",
      "Epoch 37, Batch 30, Train Loss: 0.03614756092429161, Memory (GB): 7\n",
      "Epoch 37, Batch 31, Train Loss: 0.046553004533052444, Memory (GB): 7\n",
      "Epoch 37, Batch 32, Train Loss: 0.03812117874622345, Memory (GB): 7\n",
      "Epoch 37, Batch 33, Train Loss: 0.03592432662844658, Memory (GB): 7\n",
      "Epoch 37, Batch 34, Train Loss: 0.039975740015506744, Memory (GB): 7\n",
      "Epoch 37, Batch 35, Train Loss: 0.042189087718725204, Memory (GB): 7\n",
      "Epoch 37, Batch 36, Train Loss: 0.03589262440800667, Memory (GB): 7\n",
      "Epoch 37, Batch 37, Train Loss: 0.042156320065259933, Memory (GB): 7\n",
      "Epoch 37, Batch 38, Train Loss: 0.045295294374227524, Memory (GB): 7\n",
      "Epoch 37, Batch 39, Train Loss: 0.03685851767659187, Memory (GB): 7\n",
      "Epoch 37, Batch 40, Train Loss: 0.04057415574789047, Memory (GB): 7\n",
      "Epoch 37, Batch 41, Train Loss: 0.040758274495601654, Memory (GB): 7\n",
      "Epoch 37, Batch 42, Train Loss: 0.03900504857301712, Memory (GB): 7\n",
      "Epoch 37, Batch 43, Train Loss: 0.03273230046033859, Memory (GB): 7\n",
      "Epoch 37, Batch 44, Train Loss: 0.03855964541435242, Memory (GB): 7\n",
      "Epoch 37, Batch 45, Train Loss: 0.05234665423631668, Memory (GB): 7\n",
      "Epoch 37, Batch 46, Train Loss: 0.03418739512562752, Memory (GB): 7\n",
      "Epoch 37, Batch 47, Train Loss: 0.042477864772081375, Memory (GB): 7\n",
      "Epoch 37, Batch 48, Train Loss: 0.03690486401319504, Memory (GB): 7\n",
      "Epoch 37, Batch 49, Train Loss: 0.03687012940645218, Memory (GB): 7\n",
      "Epoch 37, Batch 50, Train Loss: 0.04503834992647171, Memory (GB): 7\n",
      "Epoch 37, Batch 51, Train Loss: 0.044424425810575485, Memory (GB): 7\n",
      "Epoch 37, Batch 52, Train Loss: 0.04115615040063858, Memory (GB): 7\n",
      "Epoch 37, Batch 53, Train Loss: 0.03458642587065697, Memory (GB): 7\n",
      "Epoch 37, Batch 54, Train Loss: 0.03647618740797043, Memory (GB): 7\n",
      "Epoch 37, Batch 55, Train Loss: 0.040816813707351685, Memory (GB): 7\n",
      "Epoch 37, Batch 56, Train Loss: 0.03654925152659416, Memory (GB): 7\n",
      "Epoch 37, Batch 57, Train Loss: 0.04624054208397865, Memory (GB): 7\n",
      "Epoch 37, Batch 58, Train Loss: 0.04005267471075058, Memory (GB): 7\n",
      "Epoch 37, Batch 59, Train Loss: 0.04276084899902344, Memory (GB): 7\n",
      "Epoch 37, Batch 60, Train Loss: 0.04715511202812195, Memory (GB): 7\n",
      "Epoch 37, Batch 61, Train Loss: 0.03928763046860695, Memory (GB): 7\n",
      "Epoch 37, Batch 62, Train Loss: 0.03852299973368645, Memory (GB): 7\n",
      "Epoch 37, Batch 63, Train Loss: 0.035763341933488846, Memory (GB): 7\n",
      "Epoch 37, Batch 64, Train Loss: 0.04158059135079384, Memory (GB): 7\n",
      "Epoch 37, Batch 65, Train Loss: 0.04141281172633171, Memory (GB): 7\n",
      "Epoch 37, Batch 66, Train Loss: 0.051057349890470505, Memory (GB): 7\n",
      "Epoch 37, Batch 67, Train Loss: 0.035657722502946854, Memory (GB): 7\n",
      "Epoch 37, Batch 68, Train Loss: 0.03585049510002136, Memory (GB): 7\n",
      "Epoch 37, Batch 69, Train Loss: 0.037245042622089386, Memory (GB): 7\n",
      "Epoch 37, Batch 70, Train Loss: 0.04094431549310684, Memory (GB): 7\n",
      "Epoch 37, Batch 71, Train Loss: 0.03604584559798241, Memory (GB): 7\n",
      "Epoch 37, Batch 72, Train Loss: 0.04017098620533943, Memory (GB): 7\n",
      "Epoch 37, Batch 73, Train Loss: 0.03791521117091179, Memory (GB): 7\n",
      "Epoch 37, Batch 74, Train Loss: 0.04410626366734505, Memory (GB): 7\n",
      "Epoch 37, Batch 75, Train Loss: 0.03588343411684036, Memory (GB): 7\n",
      "Epoch 37, Batch 76, Train Loss: 0.035659920424222946, Memory (GB): 7\n",
      "Epoch 37, Batch 77, Train Loss: 0.039171840995550156, Memory (GB): 7\n",
      "Epoch 37, Batch 78, Train Loss: 0.04442821815609932, Memory (GB): 7\n",
      "Epoch 37, Batch 79, Train Loss: 0.04126909375190735, Memory (GB): 7\n",
      "Epoch 37, Batch 80, Train Loss: 0.04382937401533127, Memory (GB): 7\n",
      "Epoch 37, Batch 81, Train Loss: 0.0415232814848423, Memory (GB): 7\n",
      "Epoch 37, Batch 82, Train Loss: 0.051260534673929214, Memory (GB): 7\n",
      "Epoch 37, Batch 83, Train Loss: 0.03976232558488846, Memory (GB): 7\n",
      "Epoch 37, Batch 84, Train Loss: 0.042490750551223755, Memory (GB): 7\n",
      "Epoch 37, Batch 85, Train Loss: 0.04522832855582237, Memory (GB): 7\n",
      "Epoch 37, Batch 86, Train Loss: 0.03882904723286629, Memory (GB): 7\n",
      "Epoch 37, Batch 87, Train Loss: 0.039777059108018875, Memory (GB): 7\n",
      "Epoch 37, Batch 88, Train Loss: 0.05981762707233429, Memory (GB): 7\n",
      "Epoch 37, Batch 89, Train Loss: 0.04539380595088005, Memory (GB): 7\n",
      "Epoch 37, Batch 90, Train Loss: 0.03390330821275711, Memory (GB): 7\n",
      "Epoch 37, Batch 91, Train Loss: 0.05084467679262161, Memory (GB): 7\n",
      "Epoch 37, Batch 92, Train Loss: 0.03451098874211311, Memory (GB): 7\n",
      "Epoch 37, Batch 93, Train Loss: 0.04587777331471443, Memory (GB): 7\n",
      "Epoch 37, Batch 94, Train Loss: 0.04036949574947357, Memory (GB): 7\n",
      "Epoch 37, Batch 95, Train Loss: 0.03845365718007088, Memory (GB): 7\n",
      "Epoch 37, Batch 96, Train Loss: 0.04342031478881836, Memory (GB): 7\n",
      "Epoch 37, Batch 97, Train Loss: 0.03741869330406189, Memory (GB): 7\n",
      "Epoch 37, Batch 98, Train Loss: 0.04761826619505882, Memory (GB): 7\n",
      "Epoch 37, Batch 99, Train Loss: 0.041683491319417953, Memory (GB): 7\n",
      "Epoch 37, Batch 100, Train Loss: 0.0374191589653492, Memory (GB): 7\n",
      "Epoch 37, Batch 101, Train Loss: 0.034583039581775665, Memory (GB): 7\n",
      "Epoch 37, Batch 102, Train Loss: 0.04852442443370819, Memory (GB): 7\n",
      "Epoch 37, Batch 103, Train Loss: 0.04056825488805771, Memory (GB): 7\n",
      "Epoch 37, Batch 104, Train Loss: 0.040879521518945694, Memory (GB): 7\n",
      "Epoch 37, Batch 105, Train Loss: 0.03836463764309883, Memory (GB): 7\n",
      "Epoch 37, Batch 106, Train Loss: 0.03529538959264755, Memory (GB): 7\n",
      "Epoch 37, Batch 107, Train Loss: 0.03908313438296318, Memory (GB): 7\n",
      "Epoch 37, Batch 108, Train Loss: 0.03633199259638786, Memory (GB): 7\n",
      "Epoch 37, Batch 109, Train Loss: 0.0424472838640213, Memory (GB): 7\n",
      "Epoch 37, Batch 110, Train Loss: 0.03961225226521492, Memory (GB): 7\n",
      "Epoch 37, Batch 111, Train Loss: 0.03808209300041199, Memory (GB): 7\n",
      "Epoch 37, Batch 112, Train Loss: 0.03893599286675453, Memory (GB): 7\n",
      "Epoch 37, Batch 113, Train Loss: 0.03669756278395653, Memory (GB): 7\n",
      "Epoch 37, Batch 114, Train Loss: 0.0353098027408123, Memory (GB): 7\n",
      "Epoch 37, Batch 115, Train Loss: 0.0431707426905632, Memory (GB): 7\n",
      "Epoch 37, Batch 116, Train Loss: 0.040294744074344635, Memory (GB): 7\n",
      "Epoch 37, Batch 117, Train Loss: 0.03423188999295235, Memory (GB): 7\n",
      "Epoch 37, Batch 118, Train Loss: 0.04182571545243263, Memory (GB): 7\n",
      "Epoch 37, Batch 119, Train Loss: 0.04225798323750496, Memory (GB): 7\n",
      "Epoch 37, Batch 120, Train Loss: 0.04408491775393486, Memory (GB): 7\n",
      "Epoch 37, Batch 121, Train Loss: 0.03627565875649452, Memory (GB): 7\n",
      "Epoch 37, Batch 122, Train Loss: 0.046402398496866226, Memory (GB): 7\n",
      "Epoch 37, Batch 123, Train Loss: 0.03951271250844002, Memory (GB): 7\n",
      "Epoch 37, Batch 124, Train Loss: 0.036396436393260956, Memory (GB): 7\n",
      "Epoch 37, Batch 125, Train Loss: 0.03862530738115311, Memory (GB): 7\n",
      "Epoch 37, Batch 126, Train Loss: 0.04333040863275528, Memory (GB): 7\n",
      "Epoch 37, Batch 127, Train Loss: 0.04240971431136131, Memory (GB): 7\n",
      "Epoch 37, Batch 128, Train Loss: 0.04510701820254326, Memory (GB): 7\n",
      "Epoch 37, Batch 129, Train Loss: 0.044538334012031555, Memory (GB): 7\n",
      "Epoch 37, Batch 130, Train Loss: 0.041647978127002716, Memory (GB): 7\n",
      "Epoch 37, Batch 131, Train Loss: 0.05314026027917862, Memory (GB): 7\n",
      "Epoch 37, Batch 132, Train Loss: 0.043603815138339996, Memory (GB): 7\n",
      "Epoch 37, Batch 133, Train Loss: 0.047085583209991455, Memory (GB): 7\n",
      "Epoch 37, Batch 134, Train Loss: 0.04980872943997383, Memory (GB): 7\n",
      "Epoch 37, Batch 135, Train Loss: 0.045084573328495026, Memory (GB): 7\n",
      "Epoch 37, Batch 136, Train Loss: 0.04342786595225334, Memory (GB): 7\n",
      "Epoch 37, Batch 137, Train Loss: 0.03941147401928902, Memory (GB): 7\n",
      "Epoch 37, Batch 138, Train Loss: 0.040293432772159576, Memory (GB): 7\n",
      "Epoch 37, Batch 139, Train Loss: 0.048229195177555084, Memory (GB): 7\n",
      "Epoch 37, Batch 140, Train Loss: 0.046558912843465805, Memory (GB): 7\n",
      "Epoch 37, Batch 141, Train Loss: 0.0462290458381176, Memory (GB): 7\n",
      "Epoch 37, Batch 142, Train Loss: 0.0412166491150856, Memory (GB): 7\n",
      "Epoch 37, Batch 143, Train Loss: 0.045363325625658035, Memory (GB): 7\n",
      "Epoch 37, Batch 144, Train Loss: 0.04455776885151863, Memory (GB): 7\n",
      "Epoch 37, Batch 145, Train Loss: 0.037696678191423416, Memory (GB): 7\n",
      "Epoch 37, Batch 146, Train Loss: 0.04260341450572014, Memory (GB): 7\n",
      "Epoch 37, Batch 147, Train Loss: 0.04680145904421806, Memory (GB): 7\n",
      "Epoch 37, Batch 148, Train Loss: 0.046998150646686554, Memory (GB): 7\n",
      "Epoch 37, Batch 149, Train Loss: 0.0421430803835392, Memory (GB): 7\n",
      "Epoch 37, Batch 150, Train Loss: 0.04197181016206741, Memory (GB): 7\n",
      "Epoch 37, Batch 151, Train Loss: 0.04132997989654541, Memory (GB): 7\n",
      "Epoch 37, Batch 152, Train Loss: 0.0422479584813118, Memory (GB): 7\n",
      "Epoch 37, Batch 153, Train Loss: 0.04043835029006004, Memory (GB): 7\n",
      "Epoch 37, Batch 154, Train Loss: 0.039696626365184784, Memory (GB): 7\n",
      "Epoch 37, Batch 155, Train Loss: 0.04389176890254021, Memory (GB): 7\n",
      "Epoch 37, Batch 156, Train Loss: 0.04032086580991745, Memory (GB): 7\n",
      "Epoch 37, Batch 157, Train Loss: 0.04138457775115967, Memory (GB): 7\n",
      "Epoch 37, Batch 158, Train Loss: 0.04418189078569412, Memory (GB): 7\n",
      "Epoch 37, Batch 159, Train Loss: 0.04234946519136429, Memory (GB): 7\n",
      "Epoch 37, Batch 160, Train Loss: 0.04832680895924568, Memory (GB): 7\n",
      "Epoch 37, Batch 161, Train Loss: 0.04073401167988777, Memory (GB): 7\n",
      "Epoch 37, Batch 162, Train Loss: 0.04638797789812088, Memory (GB): 7\n",
      "Epoch 37, Batch 163, Train Loss: 0.05099465698003769, Memory (GB): 7\n",
      "Epoch 37, Batch 164, Train Loss: 0.04381658509373665, Memory (GB): 7\n",
      "Epoch 37, Batch 165, Train Loss: 0.04249217361211777, Memory (GB): 7\n",
      "Epoch 37, Batch 166, Train Loss: 0.049809932708740234, Memory (GB): 7\n",
      "Epoch 37, Batch 167, Train Loss: 0.04031043499708176, Memory (GB): 7\n",
      "Epoch 37, Batch 168, Train Loss: 0.047114547342061996, Memory (GB): 7\n",
      "Epoch 37, Batch 169, Train Loss: 0.036581818014383316, Memory (GB): 7\n",
      "Epoch 37, Batch 170, Train Loss: 0.040040429681539536, Memory (GB): 7\n",
      "Epoch 37, Batch 171, Train Loss: 0.0414029024541378, Memory (GB): 7\n",
      "Epoch 37, Batch 172, Train Loss: 0.0444004200398922, Memory (GB): 7\n",
      "Epoch 37, Batch 173, Train Loss: 0.05193787068128586, Memory (GB): 7\n",
      "Epoch 37, Batch 174, Train Loss: 0.038442932069301605, Memory (GB): 7\n",
      "Epoch 37, Batch 175, Train Loss: 0.037657640874385834, Memory (GB): 7\n",
      "Epoch 37, Batch 176, Train Loss: 0.037288397550582886, Memory (GB): 7\n",
      "Epoch 37, Batch 177, Train Loss: 0.04139149934053421, Memory (GB): 7\n",
      "Epoch 37, Batch 178, Train Loss: 0.04237350821495056, Memory (GB): 7\n",
      "Epoch 37, Batch 179, Train Loss: 0.03992875665426254, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 38, Batch 0, Train Loss: 0.04861176013946533, Memory (GB): 7\n",
      "Epoch 38, Batch 1, Train Loss: 0.03576855733990669, Memory (GB): 7\n",
      "Epoch 38, Batch 2, Train Loss: 0.036379437893629074, Memory (GB): 7\n",
      "Epoch 38, Batch 3, Train Loss: 0.04427172616124153, Memory (GB): 7\n",
      "Epoch 38, Batch 4, Train Loss: 0.03575257584452629, Memory (GB): 7\n",
      "Epoch 38, Batch 5, Train Loss: 0.05018974468111992, Memory (GB): 7\n",
      "Epoch 38, Batch 6, Train Loss: 0.03450976312160492, Memory (GB): 7\n",
      "Epoch 38, Batch 7, Train Loss: 0.0390256829559803, Memory (GB): 7\n",
      "Epoch 38, Batch 8, Train Loss: 0.03863360732793808, Memory (GB): 7\n",
      "Epoch 38, Batch 9, Train Loss: 0.038692161440849304, Memory (GB): 7\n",
      "Epoch 38, Batch 10, Train Loss: 0.03763856366276741, Memory (GB): 7\n",
      "Epoch 38, Batch 11, Train Loss: 0.03829231113195419, Memory (GB): 7\n",
      "Epoch 38, Batch 12, Train Loss: 0.04111088067293167, Memory (GB): 7\n",
      "Epoch 38, Batch 13, Train Loss: 0.053334418684244156, Memory (GB): 7\n",
      "Epoch 38, Batch 14, Train Loss: 0.036580462008714676, Memory (GB): 7\n",
      "Epoch 38, Batch 15, Train Loss: 0.043454114347696304, Memory (GB): 7\n",
      "Epoch 38, Batch 16, Train Loss: 0.039249125868082047, Memory (GB): 7\n",
      "Epoch 38, Batch 17, Train Loss: 0.03784139081835747, Memory (GB): 7\n",
      "Epoch 38, Batch 18, Train Loss: 0.035904936492443085, Memory (GB): 7\n",
      "Epoch 38, Batch 19, Train Loss: 0.036395683884620667, Memory (GB): 7\n",
      "Epoch 38, Batch 20, Train Loss: 0.04770147055387497, Memory (GB): 7\n",
      "Epoch 38, Batch 21, Train Loss: 0.04592883214354515, Memory (GB): 7\n",
      "Epoch 38, Batch 22, Train Loss: 0.03707977384328842, Memory (GB): 7\n",
      "Epoch 38, Batch 23, Train Loss: 0.03729866072535515, Memory (GB): 7\n",
      "Epoch 38, Batch 24, Train Loss: 0.04021893069148064, Memory (GB): 7\n",
      "Epoch 38, Batch 25, Train Loss: 0.041010115295648575, Memory (GB): 7\n",
      "Epoch 38, Batch 26, Train Loss: 0.041054148226976395, Memory (GB): 7\n",
      "Epoch 38, Batch 27, Train Loss: 0.03887459635734558, Memory (GB): 7\n",
      "Epoch 38, Batch 28, Train Loss: 0.03848917782306671, Memory (GB): 7\n",
      "Epoch 38, Batch 29, Train Loss: 0.039772696793079376, Memory (GB): 7\n",
      "Epoch 38, Batch 30, Train Loss: 0.03613365814089775, Memory (GB): 7\n",
      "Epoch 38, Batch 31, Train Loss: 0.036090999841690063, Memory (GB): 7\n",
      "Epoch 38, Batch 32, Train Loss: 0.036417052149772644, Memory (GB): 7\n",
      "Epoch 38, Batch 33, Train Loss: 0.037784844636917114, Memory (GB): 7\n",
      "Epoch 38, Batch 34, Train Loss: 0.036589961498975754, Memory (GB): 7\n",
      "Epoch 38, Batch 35, Train Loss: 0.04510526359081268, Memory (GB): 7\n",
      "Epoch 38, Batch 36, Train Loss: 0.041066769510507584, Memory (GB): 7\n",
      "Epoch 38, Batch 37, Train Loss: 0.03443441540002823, Memory (GB): 7\n",
      "Epoch 38, Batch 38, Train Loss: 0.036140747368335724, Memory (GB): 7\n",
      "Epoch 38, Batch 39, Train Loss: 0.03329476714134216, Memory (GB): 7\n",
      "Epoch 38, Batch 40, Train Loss: 0.040883224457502365, Memory (GB): 7\n",
      "Epoch 38, Batch 41, Train Loss: 0.04983634129166603, Memory (GB): 7\n",
      "Epoch 38, Batch 42, Train Loss: 0.03441306948661804, Memory (GB): 7\n",
      "Epoch 38, Batch 43, Train Loss: 0.03463226184248924, Memory (GB): 7\n",
      "Epoch 38, Batch 44, Train Loss: 0.038915399461984634, Memory (GB): 7\n",
      "Epoch 38, Batch 45, Train Loss: 0.03840347006917, Memory (GB): 7\n",
      "Epoch 38, Batch 46, Train Loss: 0.03644426167011261, Memory (GB): 7\n",
      "Epoch 38, Batch 47, Train Loss: 0.035165935754776, Memory (GB): 7\n",
      "Epoch 38, Batch 48, Train Loss: 0.03800477087497711, Memory (GB): 7\n",
      "Epoch 38, Batch 49, Train Loss: 0.04283640906214714, Memory (GB): 7\n",
      "Epoch 38, Batch 50, Train Loss: 0.0385022833943367, Memory (GB): 7\n",
      "Epoch 38, Batch 51, Train Loss: 0.043627530336380005, Memory (GB): 7\n",
      "Epoch 38, Batch 52, Train Loss: 0.041147440671920776, Memory (GB): 7\n",
      "Epoch 38, Batch 53, Train Loss: 0.04361346364021301, Memory (GB): 7\n",
      "Epoch 38, Batch 54, Train Loss: 0.03711757808923721, Memory (GB): 7\n",
      "Epoch 38, Batch 55, Train Loss: 0.03640102595090866, Memory (GB): 7\n",
      "Epoch 38, Batch 56, Train Loss: 0.049384184181690216, Memory (GB): 7\n",
      "Epoch 38, Batch 57, Train Loss: 0.03656923025846481, Memory (GB): 7\n",
      "Epoch 38, Batch 58, Train Loss: 0.03768010810017586, Memory (GB): 7\n",
      "Epoch 38, Batch 59, Train Loss: 0.04031359404325485, Memory (GB): 7\n",
      "Epoch 38, Batch 60, Train Loss: 0.03672792389988899, Memory (GB): 7\n",
      "Epoch 38, Batch 61, Train Loss: 0.038054320961236954, Memory (GB): 7\n",
      "Epoch 38, Batch 62, Train Loss: 0.038972314447164536, Memory (GB): 7\n",
      "Epoch 38, Batch 63, Train Loss: 0.03539362549781799, Memory (GB): 7\n",
      "Epoch 38, Batch 64, Train Loss: 0.041559796780347824, Memory (GB): 7\n",
      "Epoch 38, Batch 65, Train Loss: 0.03554621711373329, Memory (GB): 7\n",
      "Epoch 38, Batch 66, Train Loss: 0.045397449284791946, Memory (GB): 7\n",
      "Epoch 38, Batch 67, Train Loss: 0.042944762855768204, Memory (GB): 7\n",
      "Epoch 38, Batch 68, Train Loss: 0.03489996865391731, Memory (GB): 7\n",
      "Epoch 38, Batch 69, Train Loss: 0.0399208702147007, Memory (GB): 7\n",
      "Epoch 38, Batch 70, Train Loss: 0.03935132175683975, Memory (GB): 7\n",
      "Epoch 38, Batch 71, Train Loss: 0.036844391375780106, Memory (GB): 7\n",
      "Epoch 38, Batch 72, Train Loss: 0.03674996644258499, Memory (GB): 7\n",
      "Epoch 38, Batch 73, Train Loss: 0.038286276161670685, Memory (GB): 7\n",
      "Epoch 38, Batch 74, Train Loss: 0.034566931426525116, Memory (GB): 7\n",
      "Epoch 38, Batch 75, Train Loss: 0.036686092615127563, Memory (GB): 7\n",
      "Epoch 38, Batch 76, Train Loss: 0.03939536213874817, Memory (GB): 7\n",
      "Epoch 38, Batch 77, Train Loss: 0.0332263819873333, Memory (GB): 7\n",
      "Epoch 38, Batch 78, Train Loss: 0.040382202714681625, Memory (GB): 7\n",
      "Epoch 38, Batch 79, Train Loss: 0.038613710552453995, Memory (GB): 7\n",
      "Epoch 38, Batch 80, Train Loss: 0.0394321084022522, Memory (GB): 7\n",
      "Epoch 38, Batch 81, Train Loss: 0.038027554750442505, Memory (GB): 7\n",
      "Epoch 38, Batch 82, Train Loss: 0.043391142040491104, Memory (GB): 7\n",
      "Epoch 38, Batch 83, Train Loss: 0.037804149091243744, Memory (GB): 7\n",
      "Epoch 38, Batch 84, Train Loss: 0.038614410907030106, Memory (GB): 7\n",
      "Epoch 38, Batch 85, Train Loss: 0.04328661784529686, Memory (GB): 7\n",
      "Epoch 38, Batch 86, Train Loss: 0.04813757538795471, Memory (GB): 7\n",
      "Epoch 38, Batch 87, Train Loss: 0.04183666408061981, Memory (GB): 7\n",
      "Epoch 38, Batch 88, Train Loss: 0.03706049919128418, Memory (GB): 7\n",
      "Epoch 38, Batch 89, Train Loss: 0.036435555666685104, Memory (GB): 7\n",
      "Epoch 38, Batch 90, Train Loss: 0.040095727890729904, Memory (GB): 7\n",
      "Epoch 38, Batch 91, Train Loss: 0.03587225079536438, Memory (GB): 7\n",
      "Epoch 38, Batch 92, Train Loss: 0.042395107448101044, Memory (GB): 7\n",
      "Epoch 38, Batch 93, Train Loss: 0.03492363169789314, Memory (GB): 7\n",
      "Epoch 38, Batch 94, Train Loss: 0.03941236063838005, Memory (GB): 7\n",
      "Epoch 38, Batch 95, Train Loss: 0.04041439667344093, Memory (GB): 7\n",
      "Epoch 38, Batch 96, Train Loss: 0.03964908793568611, Memory (GB): 7\n",
      "Epoch 38, Batch 97, Train Loss: 0.04575534537434578, Memory (GB): 7\n",
      "Epoch 38, Batch 98, Train Loss: 0.04088692367076874, Memory (GB): 7\n",
      "Epoch 38, Batch 99, Train Loss: 0.039643414318561554, Memory (GB): 7\n",
      "Epoch 38, Batch 100, Train Loss: 0.037454910576343536, Memory (GB): 7\n",
      "Epoch 38, Batch 101, Train Loss: 0.039437711238861084, Memory (GB): 7\n",
      "Epoch 38, Batch 102, Train Loss: 0.04008769243955612, Memory (GB): 7\n",
      "Epoch 38, Batch 103, Train Loss: 0.03652659431099892, Memory (GB): 7\n",
      "Epoch 38, Batch 104, Train Loss: 0.03764701634645462, Memory (GB): 7\n",
      "Epoch 38, Batch 105, Train Loss: 0.03678033873438835, Memory (GB): 7\n",
      "Epoch 38, Batch 106, Train Loss: 0.036623574793338776, Memory (GB): 7\n",
      "Epoch 38, Batch 107, Train Loss: 0.04160772264003754, Memory (GB): 7\n",
      "Epoch 38, Batch 108, Train Loss: 0.045499175786972046, Memory (GB): 7\n",
      "Epoch 38, Batch 109, Train Loss: 0.04210231453180313, Memory (GB): 7\n",
      "Epoch 38, Batch 110, Train Loss: 0.03752434626221657, Memory (GB): 7\n",
      "Epoch 38, Batch 111, Train Loss: 0.04055909439921379, Memory (GB): 7\n",
      "Epoch 38, Batch 112, Train Loss: 0.03760433942079544, Memory (GB): 7\n",
      "Epoch 38, Batch 113, Train Loss: 0.03683975711464882, Memory (GB): 7\n",
      "Epoch 38, Batch 114, Train Loss: 0.04347866773605347, Memory (GB): 7\n",
      "Epoch 38, Batch 115, Train Loss: 0.03855011612176895, Memory (GB): 7\n",
      "Epoch 38, Batch 116, Train Loss: 0.044394537806510925, Memory (GB): 7\n",
      "Epoch 38, Batch 117, Train Loss: 0.04278065636754036, Memory (GB): 7\n",
      "Epoch 38, Batch 118, Train Loss: 0.03576839715242386, Memory (GB): 7\n",
      "Epoch 38, Batch 119, Train Loss: 0.034938424825668335, Memory (GB): 7\n",
      "Epoch 38, Batch 120, Train Loss: 0.035231608897447586, Memory (GB): 7\n",
      "Epoch 38, Batch 121, Train Loss: 0.043791763484478, Memory (GB): 7\n",
      "Epoch 38, Batch 122, Train Loss: 0.038238268345594406, Memory (GB): 7\n",
      "Epoch 38, Batch 123, Train Loss: 0.03880635276436806, Memory (GB): 7\n",
      "Epoch 38, Batch 124, Train Loss: 0.04006163775920868, Memory (GB): 7\n",
      "Epoch 38, Batch 125, Train Loss: 0.03813979774713516, Memory (GB): 7\n",
      "Epoch 38, Batch 126, Train Loss: 0.034154098480939865, Memory (GB): 7\n",
      "Epoch 38, Batch 127, Train Loss: 0.037002358585596085, Memory (GB): 7\n",
      "Epoch 38, Batch 128, Train Loss: 0.03623763844370842, Memory (GB): 7\n",
      "Epoch 38, Batch 129, Train Loss: 0.037669964134693146, Memory (GB): 7\n",
      "Epoch 38, Batch 130, Train Loss: 0.0362795889377594, Memory (GB): 7\n",
      "Epoch 38, Batch 131, Train Loss: 0.04037601128220558, Memory (GB): 7\n",
      "Epoch 38, Batch 132, Train Loss: 0.03861687704920769, Memory (GB): 7\n",
      "Epoch 38, Batch 133, Train Loss: 0.04045816510915756, Memory (GB): 7\n",
      "Epoch 38, Batch 134, Train Loss: 0.049573641270399094, Memory (GB): 7\n",
      "Epoch 38, Batch 135, Train Loss: 0.03634367510676384, Memory (GB): 7\n",
      "Epoch 38, Batch 136, Train Loss: 0.035803426057100296, Memory (GB): 7\n",
      "Epoch 38, Batch 137, Train Loss: 0.04990401118993759, Memory (GB): 7\n",
      "Epoch 38, Batch 138, Train Loss: 0.04242025315761566, Memory (GB): 7\n",
      "Epoch 38, Batch 139, Train Loss: 0.0400199368596077, Memory (GB): 7\n",
      "Epoch 38, Batch 140, Train Loss: 0.03979644551873207, Memory (GB): 7\n",
      "Epoch 38, Batch 141, Train Loss: 0.039356742054224014, Memory (GB): 7\n",
      "Epoch 38, Batch 142, Train Loss: 0.03945903107523918, Memory (GB): 7\n",
      "Epoch 38, Batch 143, Train Loss: 0.03387913852930069, Memory (GB): 7\n",
      "Epoch 38, Batch 144, Train Loss: 0.03412873297929764, Memory (GB): 7\n",
      "Epoch 38, Batch 145, Train Loss: 0.04543479532003403, Memory (GB): 7\n",
      "Epoch 38, Batch 146, Train Loss: 0.036971766501665115, Memory (GB): 7\n",
      "Epoch 38, Batch 147, Train Loss: 0.03848958760499954, Memory (GB): 7\n",
      "Epoch 38, Batch 148, Train Loss: 0.04368872940540314, Memory (GB): 7\n",
      "Epoch 38, Batch 149, Train Loss: 0.03901434689760208, Memory (GB): 7\n",
      "Epoch 38, Batch 150, Train Loss: 0.03360685706138611, Memory (GB): 7\n",
      "Epoch 38, Batch 151, Train Loss: 0.039563436061143875, Memory (GB): 7\n",
      "Epoch 38, Batch 152, Train Loss: 0.036596257239580154, Memory (GB): 7\n",
      "Epoch 38, Batch 153, Train Loss: 0.040000174194574356, Memory (GB): 7\n",
      "Epoch 38, Batch 154, Train Loss: 0.05117874592542648, Memory (GB): 7\n",
      "Epoch 38, Batch 155, Train Loss: 0.036648936569690704, Memory (GB): 7\n",
      "Epoch 38, Batch 156, Train Loss: 0.046422895044088364, Memory (GB): 7\n",
      "Epoch 38, Batch 157, Train Loss: 0.035877812653779984, Memory (GB): 7\n",
      "Epoch 38, Batch 158, Train Loss: 0.03867185860872269, Memory (GB): 7\n",
      "Epoch 38, Batch 159, Train Loss: 0.03635178133845329, Memory (GB): 7\n",
      "Epoch 38, Batch 160, Train Loss: 0.04242011532187462, Memory (GB): 7\n",
      "Epoch 38, Batch 161, Train Loss: 0.037978775799274445, Memory (GB): 7\n",
      "Epoch 38, Batch 162, Train Loss: 0.04463106021285057, Memory (GB): 7\n",
      "Epoch 38, Batch 163, Train Loss: 0.0387997105717659, Memory (GB): 7\n",
      "Epoch 38, Batch 164, Train Loss: 0.03969407081604004, Memory (GB): 7\n",
      "Epoch 38, Batch 165, Train Loss: 0.046232275664806366, Memory (GB): 7\n",
      "Epoch 38, Batch 166, Train Loss: 0.04408727213740349, Memory (GB): 7\n",
      "Epoch 38, Batch 167, Train Loss: 0.03515462577342987, Memory (GB): 7\n",
      "Epoch 38, Batch 168, Train Loss: 0.03929852321743965, Memory (GB): 7\n",
      "Epoch 38, Batch 169, Train Loss: 0.03922893479466438, Memory (GB): 7\n",
      "Epoch 38, Batch 170, Train Loss: 0.0402226522564888, Memory (GB): 7\n",
      "Epoch 38, Batch 171, Train Loss: 0.0448315367102623, Memory (GB): 7\n",
      "Epoch 38, Batch 172, Train Loss: 0.03326476737856865, Memory (GB): 7\n",
      "Epoch 38, Batch 173, Train Loss: 0.0379054881632328, Memory (GB): 7\n",
      "Epoch 38, Batch 174, Train Loss: 0.03845997527241707, Memory (GB): 7\n",
      "Epoch 38, Batch 175, Train Loss: 0.03703753277659416, Memory (GB): 7\n",
      "Epoch 38, Batch 176, Train Loss: 0.04189107567071915, Memory (GB): 7\n",
      "Epoch 38, Batch 177, Train Loss: 0.03451291844248772, Memory (GB): 7\n",
      "Epoch 38, Batch 178, Train Loss: 0.04009639471769333, Memory (GB): 7\n",
      "Epoch 38, Batch 179, Train Loss: 0.04894443601369858, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 39, Batch 0, Train Loss: 0.0346960723400116, Memory (GB): 7\n",
      "Epoch 39, Batch 1, Train Loss: 0.03594216704368591, Memory (GB): 7\n",
      "Epoch 39, Batch 2, Train Loss: 0.0322037935256958, Memory (GB): 7\n",
      "Epoch 39, Batch 3, Train Loss: 0.03801611810922623, Memory (GB): 7\n",
      "Epoch 39, Batch 4, Train Loss: 0.03415228798985481, Memory (GB): 7\n",
      "Epoch 39, Batch 5, Train Loss: 0.0338846780359745, Memory (GB): 7\n",
      "Epoch 39, Batch 6, Train Loss: 0.03887511044740677, Memory (GB): 7\n",
      "Epoch 39, Batch 7, Train Loss: 0.029409674927592278, Memory (GB): 7\n",
      "Epoch 39, Batch 8, Train Loss: 0.03603234142065048, Memory (GB): 7\n",
      "Epoch 39, Batch 9, Train Loss: 0.03425983339548111, Memory (GB): 7\n",
      "Epoch 39, Batch 10, Train Loss: 0.040995869785547256, Memory (GB): 7\n",
      "Epoch 39, Batch 11, Train Loss: 0.0438089556992054, Memory (GB): 7\n",
      "Epoch 39, Batch 12, Train Loss: 0.03259720280766487, Memory (GB): 7\n",
      "Epoch 39, Batch 13, Train Loss: 0.03275546059012413, Memory (GB): 7\n",
      "Epoch 39, Batch 14, Train Loss: 0.030459625646471977, Memory (GB): 7\n",
      "Epoch 39, Batch 15, Train Loss: 0.03955558314919472, Memory (GB): 7\n",
      "Epoch 39, Batch 16, Train Loss: 0.038325823843479156, Memory (GB): 7\n",
      "Epoch 39, Batch 17, Train Loss: 0.035289742052555084, Memory (GB): 7\n",
      "Epoch 39, Batch 18, Train Loss: 0.039910536259412766, Memory (GB): 7\n",
      "Epoch 39, Batch 19, Train Loss: 0.04055128991603851, Memory (GB): 7\n",
      "Epoch 39, Batch 20, Train Loss: 0.036207541823387146, Memory (GB): 7\n",
      "Epoch 39, Batch 21, Train Loss: 0.03575163334608078, Memory (GB): 7\n",
      "Epoch 39, Batch 22, Train Loss: 0.035819679498672485, Memory (GB): 7\n",
      "Epoch 39, Batch 23, Train Loss: 0.0368315689265728, Memory (GB): 7\n",
      "Epoch 39, Batch 24, Train Loss: 0.040958721190690994, Memory (GB): 7\n",
      "Epoch 39, Batch 25, Train Loss: 0.03781374543905258, Memory (GB): 7\n",
      "Epoch 39, Batch 26, Train Loss: 0.0377860851585865, Memory (GB): 7\n",
      "Epoch 39, Batch 27, Train Loss: 0.03508299961686134, Memory (GB): 7\n",
      "Epoch 39, Batch 28, Train Loss: 0.039747290313243866, Memory (GB): 7\n",
      "Epoch 39, Batch 29, Train Loss: 0.03636261820793152, Memory (GB): 7\n",
      "Epoch 39, Batch 30, Train Loss: 0.031126288697123528, Memory (GB): 7\n",
      "Epoch 39, Batch 31, Train Loss: 0.038427192717790604, Memory (GB): 7\n",
      "Epoch 39, Batch 32, Train Loss: 0.034927479922771454, Memory (GB): 7\n",
      "Epoch 39, Batch 33, Train Loss: 0.03840199485421181, Memory (GB): 7\n",
      "Epoch 39, Batch 34, Train Loss: 0.03517764061689377, Memory (GB): 7\n",
      "Epoch 39, Batch 35, Train Loss: 0.03391237556934357, Memory (GB): 7\n",
      "Epoch 39, Batch 36, Train Loss: 0.03420800343155861, Memory (GB): 7\n",
      "Epoch 39, Batch 37, Train Loss: 0.03733770549297333, Memory (GB): 7\n",
      "Epoch 39, Batch 38, Train Loss: 0.039596062153577805, Memory (GB): 7\n",
      "Epoch 39, Batch 39, Train Loss: 0.036352917551994324, Memory (GB): 7\n",
      "Epoch 39, Batch 40, Train Loss: 0.03562727943062782, Memory (GB): 7\n",
      "Epoch 39, Batch 41, Train Loss: 0.034703273326158524, Memory (GB): 7\n",
      "Epoch 39, Batch 42, Train Loss: 0.03441357612609863, Memory (GB): 7\n",
      "Epoch 39, Batch 43, Train Loss: 0.042130887508392334, Memory (GB): 7\n",
      "Epoch 39, Batch 44, Train Loss: 0.03183666244149208, Memory (GB): 7\n",
      "Epoch 39, Batch 45, Train Loss: 0.03679667040705681, Memory (GB): 7\n",
      "Epoch 39, Batch 46, Train Loss: 0.03667096793651581, Memory (GB): 7\n",
      "Epoch 39, Batch 47, Train Loss: 0.033724647015333176, Memory (GB): 7\n",
      "Epoch 39, Batch 48, Train Loss: 0.038202449679374695, Memory (GB): 7\n",
      "Epoch 39, Batch 49, Train Loss: 0.03438962250947952, Memory (GB): 7\n",
      "Epoch 39, Batch 50, Train Loss: 0.035062696784734726, Memory (GB): 7\n",
      "Epoch 39, Batch 51, Train Loss: 0.04364617541432381, Memory (GB): 7\n",
      "Epoch 39, Batch 52, Train Loss: 0.03615962713956833, Memory (GB): 7\n",
      "Epoch 39, Batch 53, Train Loss: 0.03435783088207245, Memory (GB): 7\n",
      "Epoch 39, Batch 54, Train Loss: 0.03263276070356369, Memory (GB): 7\n",
      "Epoch 39, Batch 55, Train Loss: 0.03281623125076294, Memory (GB): 7\n",
      "Epoch 39, Batch 56, Train Loss: 0.033729780465364456, Memory (GB): 7\n",
      "Epoch 39, Batch 57, Train Loss: 0.042258232831954956, Memory (GB): 7\n",
      "Epoch 39, Batch 58, Train Loss: 0.03617331385612488, Memory (GB): 7\n",
      "Epoch 39, Batch 59, Train Loss: 0.039977360516786575, Memory (GB): 7\n",
      "Epoch 39, Batch 60, Train Loss: 0.04786389693617821, Memory (GB): 7\n",
      "Epoch 39, Batch 61, Train Loss: 0.03986256197094917, Memory (GB): 7\n",
      "Epoch 39, Batch 62, Train Loss: 0.03429269790649414, Memory (GB): 7\n",
      "Epoch 39, Batch 63, Train Loss: 0.03751523792743683, Memory (GB): 7\n",
      "Epoch 39, Batch 64, Train Loss: 0.036587052047252655, Memory (GB): 7\n",
      "Epoch 39, Batch 65, Train Loss: 0.04075493663549423, Memory (GB): 7\n",
      "Epoch 39, Batch 66, Train Loss: 0.03382394462823868, Memory (GB): 7\n",
      "Epoch 39, Batch 67, Train Loss: 0.03488462045788765, Memory (GB): 7\n",
      "Epoch 39, Batch 68, Train Loss: 0.0334339439868927, Memory (GB): 7\n",
      "Epoch 39, Batch 69, Train Loss: 0.0386674590408802, Memory (GB): 7\n",
      "Epoch 39, Batch 70, Train Loss: 0.03527598828077316, Memory (GB): 7\n",
      "Epoch 39, Batch 71, Train Loss: 0.04037760570645332, Memory (GB): 7\n",
      "Epoch 39, Batch 72, Train Loss: 0.03745035454630852, Memory (GB): 7\n",
      "Epoch 39, Batch 73, Train Loss: 0.04397664591670036, Memory (GB): 7\n",
      "Epoch 39, Batch 74, Train Loss: 0.0384802371263504, Memory (GB): 7\n",
      "Epoch 39, Batch 75, Train Loss: 0.03842825070023537, Memory (GB): 7\n",
      "Epoch 39, Batch 76, Train Loss: 0.03340337425470352, Memory (GB): 7\n",
      "Epoch 39, Batch 77, Train Loss: 0.0348464660346508, Memory (GB): 7\n",
      "Epoch 39, Batch 78, Train Loss: 0.03552238270640373, Memory (GB): 7\n",
      "Epoch 39, Batch 79, Train Loss: 0.033630479127168655, Memory (GB): 7\n",
      "Epoch 39, Batch 80, Train Loss: 0.03395220637321472, Memory (GB): 7\n",
      "Epoch 39, Batch 81, Train Loss: 0.04109160602092743, Memory (GB): 7\n",
      "Epoch 39, Batch 82, Train Loss: 0.03213316202163696, Memory (GB): 7\n",
      "Epoch 39, Batch 83, Train Loss: 0.03209906443953514, Memory (GB): 7\n",
      "Epoch 39, Batch 84, Train Loss: 0.0341007262468338, Memory (GB): 7\n",
      "Epoch 39, Batch 85, Train Loss: 0.03458792343735695, Memory (GB): 7\n",
      "Epoch 39, Batch 86, Train Loss: 0.03488253057003021, Memory (GB): 7\n",
      "Epoch 39, Batch 87, Train Loss: 0.03443119302392006, Memory (GB): 7\n",
      "Epoch 39, Batch 88, Train Loss: 0.0337456613779068, Memory (GB): 7\n",
      "Epoch 39, Batch 89, Train Loss: 0.04623233526945114, Memory (GB): 7\n",
      "Epoch 39, Batch 90, Train Loss: 0.03427617996931076, Memory (GB): 7\n",
      "Epoch 39, Batch 91, Train Loss: 0.03689077869057655, Memory (GB): 7\n",
      "Epoch 39, Batch 92, Train Loss: 0.03684123232960701, Memory (GB): 7\n",
      "Epoch 39, Batch 93, Train Loss: 0.036153607070446014, Memory (GB): 7\n",
      "Epoch 39, Batch 94, Train Loss: 0.03519849479198456, Memory (GB): 7\n",
      "Epoch 39, Batch 95, Train Loss: 0.03839249908924103, Memory (GB): 7\n",
      "Epoch 39, Batch 96, Train Loss: 0.03629269078373909, Memory (GB): 7\n",
      "Epoch 39, Batch 97, Train Loss: 0.04133929684758186, Memory (GB): 7\n",
      "Epoch 39, Batch 98, Train Loss: 0.03598054498434067, Memory (GB): 7\n",
      "Epoch 39, Batch 99, Train Loss: 0.03564907982945442, Memory (GB): 7\n",
      "Epoch 39, Batch 100, Train Loss: 0.036140076816082, Memory (GB): 7\n",
      "Epoch 39, Batch 101, Train Loss: 0.03712567687034607, Memory (GB): 7\n",
      "Epoch 39, Batch 102, Train Loss: 0.03608483821153641, Memory (GB): 7\n",
      "Epoch 39, Batch 103, Train Loss: 0.03410795331001282, Memory (GB): 7\n",
      "Epoch 39, Batch 104, Train Loss: 0.037704672664403915, Memory (GB): 7\n",
      "Epoch 39, Batch 105, Train Loss: 0.035455603152513504, Memory (GB): 7\n",
      "Epoch 39, Batch 106, Train Loss: 0.03893967345356941, Memory (GB): 7\n",
      "Epoch 39, Batch 107, Train Loss: 0.03448494151234627, Memory (GB): 7\n",
      "Epoch 39, Batch 108, Train Loss: 0.03356321156024933, Memory (GB): 7\n",
      "Epoch 39, Batch 109, Train Loss: 0.035208698362112045, Memory (GB): 7\n",
      "Epoch 39, Batch 110, Train Loss: 0.0346195250749588, Memory (GB): 7\n",
      "Epoch 39, Batch 111, Train Loss: 0.036393750458955765, Memory (GB): 7\n",
      "Epoch 39, Batch 112, Train Loss: 0.03664396330714226, Memory (GB): 7\n",
      "Epoch 39, Batch 113, Train Loss: 0.03787437081336975, Memory (GB): 7\n",
      "Epoch 39, Batch 114, Train Loss: 0.04479295387864113, Memory (GB): 7\n",
      "Epoch 39, Batch 115, Train Loss: 0.034063104540109634, Memory (GB): 7\n",
      "Epoch 39, Batch 116, Train Loss: 0.03707899525761604, Memory (GB): 7\n",
      "Epoch 39, Batch 117, Train Loss: 0.03393407538533211, Memory (GB): 7\n",
      "Epoch 39, Batch 118, Train Loss: 0.03649025037884712, Memory (GB): 7\n",
      "Epoch 39, Batch 119, Train Loss: 0.0341801717877388, Memory (GB): 7\n",
      "Epoch 39, Batch 120, Train Loss: 0.03496547415852547, Memory (GB): 7\n",
      "Epoch 39, Batch 121, Train Loss: 0.03863278031349182, Memory (GB): 7\n",
      "Epoch 39, Batch 122, Train Loss: 0.03469882160425186, Memory (GB): 7\n",
      "Epoch 39, Batch 123, Train Loss: 0.04555134102702141, Memory (GB): 7\n",
      "Epoch 39, Batch 124, Train Loss: 0.04572661221027374, Memory (GB): 7\n",
      "Epoch 39, Batch 125, Train Loss: 0.0377061553299427, Memory (GB): 7\n",
      "Epoch 39, Batch 126, Train Loss: 0.0345197357237339, Memory (GB): 7\n",
      "Epoch 39, Batch 127, Train Loss: 0.0349503830075264, Memory (GB): 7\n",
      "Epoch 39, Batch 128, Train Loss: 0.03911186009645462, Memory (GB): 7\n",
      "Epoch 39, Batch 129, Train Loss: 0.03560895100235939, Memory (GB): 7\n",
      "Epoch 39, Batch 130, Train Loss: 0.03388816863298416, Memory (GB): 7\n",
      "Epoch 39, Batch 131, Train Loss: 0.03703509643673897, Memory (GB): 7\n",
      "Epoch 39, Batch 132, Train Loss: 0.034899573773145676, Memory (GB): 7\n",
      "Epoch 39, Batch 133, Train Loss: 0.04742702469229698, Memory (GB): 7\n",
      "Epoch 39, Batch 134, Train Loss: 0.033945925533771515, Memory (GB): 7\n",
      "Epoch 39, Batch 135, Train Loss: 0.034750692546367645, Memory (GB): 7\n",
      "Epoch 39, Batch 136, Train Loss: 0.03708750009536743, Memory (GB): 7\n",
      "Epoch 39, Batch 137, Train Loss: 0.03420788794755936, Memory (GB): 7\n",
      "Epoch 39, Batch 138, Train Loss: 0.048824407160282135, Memory (GB): 7\n",
      "Epoch 39, Batch 139, Train Loss: 0.03820136561989784, Memory (GB): 7\n",
      "Epoch 39, Batch 140, Train Loss: 0.03355809673666954, Memory (GB): 7\n",
      "Epoch 39, Batch 141, Train Loss: 0.037571925669908524, Memory (GB): 7\n",
      "Epoch 39, Batch 142, Train Loss: 0.04339282959699631, Memory (GB): 7\n",
      "Epoch 39, Batch 143, Train Loss: 0.03898213431239128, Memory (GB): 7\n",
      "Epoch 39, Batch 144, Train Loss: 0.04267345741391182, Memory (GB): 7\n",
      "Epoch 39, Batch 145, Train Loss: 0.04326872527599335, Memory (GB): 7\n",
      "Epoch 39, Batch 146, Train Loss: 0.035269878804683685, Memory (GB): 7\n",
      "Epoch 39, Batch 147, Train Loss: 0.04304876923561096, Memory (GB): 7\n",
      "Epoch 39, Batch 148, Train Loss: 0.03626159578561783, Memory (GB): 7\n",
      "Epoch 39, Batch 149, Train Loss: 0.04021492600440979, Memory (GB): 7\n",
      "Epoch 39, Batch 150, Train Loss: 0.03259752690792084, Memory (GB): 7\n",
      "Epoch 39, Batch 151, Train Loss: 0.033991120755672455, Memory (GB): 7\n",
      "Epoch 39, Batch 152, Train Loss: 0.044329266995191574, Memory (GB): 7\n",
      "Epoch 39, Batch 153, Train Loss: 0.041032738983631134, Memory (GB): 7\n",
      "Epoch 39, Batch 154, Train Loss: 0.043234825134277344, Memory (GB): 7\n",
      "Epoch 39, Batch 155, Train Loss: 0.05427587032318115, Memory (GB): 7\n",
      "Epoch 39, Batch 156, Train Loss: 0.038476474583148956, Memory (GB): 7\n",
      "Epoch 39, Batch 157, Train Loss: 0.0427311472594738, Memory (GB): 7\n",
      "Epoch 39, Batch 158, Train Loss: 0.040568914264440536, Memory (GB): 7\n",
      "Epoch 39, Batch 159, Train Loss: 0.03459475189447403, Memory (GB): 7\n",
      "Epoch 39, Batch 160, Train Loss: 0.04135877639055252, Memory (GB): 7\n",
      "Epoch 39, Batch 161, Train Loss: 0.04637373238801956, Memory (GB): 7\n",
      "Epoch 39, Batch 162, Train Loss: 0.035876236855983734, Memory (GB): 7\n",
      "Epoch 39, Batch 163, Train Loss: 0.03661821037530899, Memory (GB): 7\n",
      "Epoch 39, Batch 164, Train Loss: 0.044437605887651443, Memory (GB): 7\n",
      "Epoch 39, Batch 165, Train Loss: 0.03394951671361923, Memory (GB): 7\n",
      "Epoch 39, Batch 166, Train Loss: 0.04559073969721794, Memory (GB): 7\n",
      "Epoch 39, Batch 167, Train Loss: 0.03740145266056061, Memory (GB): 7\n",
      "Epoch 39, Batch 168, Train Loss: 0.04075188189744949, Memory (GB): 7\n",
      "Epoch 39, Batch 169, Train Loss: 0.034779179841279984, Memory (GB): 7\n",
      "Epoch 39, Batch 170, Train Loss: 0.036897800862789154, Memory (GB): 7\n",
      "Epoch 39, Batch 171, Train Loss: 0.033492762595415115, Memory (GB): 7\n",
      "Epoch 39, Batch 172, Train Loss: 0.037463657557964325, Memory (GB): 7\n",
      "Epoch 39, Batch 173, Train Loss: 0.03522360697388649, Memory (GB): 7\n",
      "Epoch 39, Batch 174, Train Loss: 0.036621540784835815, Memory (GB): 7\n",
      "Epoch 39, Batch 175, Train Loss: 0.03743649274110794, Memory (GB): 7\n",
      "Epoch 39, Batch 176, Train Loss: 0.03539865463972092, Memory (GB): 7\n",
      "Epoch 39, Batch 177, Train Loss: 0.03685127943754196, Memory (GB): 7\n",
      "Epoch 39, Batch 178, Train Loss: 0.034125667065382004, Memory (GB): 7\n",
      "Epoch 39, Batch 179, Train Loss: 0.03550777956843376, Memory (GB): 7\n",
      "[0.0001, 9.7e-05, 9.409e-05, 9.12673e-05, 8.8529281e-05, 8.587340257e-05, 8.329720049289999e-05, 8.079828447811299e-05, 7.83743359437696e-05, 7.60231058654565e-05, 7.37424126894928e-05, 7.1530140308808e-05, 6.938423609954376e-05, 6.730270901655745e-05, 6.528362774606072e-05, 6.332511891367889e-05, 6.142536534626853e-05, 5.958260438588047e-05, 5.7795126254304054e-05, 5.606127246667493e-05, 5.437943429267468e-05, 5.274805126389444e-05, 5.11656097259776e-05, 4.9630641434198275e-05, 4.814172219117233e-05, 4.669747052543716e-05, 4.5296546409674046e-05, 4.393765001738382e-05, 4.2619520516862305e-05, 4.134093490135643e-05, 4.010070685431574e-05, 3.889768564868627e-05, 3.773075507922568e-05, 3.659883242684891e-05, 3.550086745404344e-05, 3.4435841430422135e-05, 3.340276618750947e-05, 3.240068320188419e-05, 3.142866270582766e-05, 3.0485802824652827e-05, 2.957122873991324e-05, 2.8684091877715842e-05, 2.7823569121384366e-05, 2.6988862047742834e-05, 2.617919618631055e-05, 2.5393820300721232e-05, 2.4632005691699594e-05, 2.3893045520948606e-05, 2.3176254155320148e-05, 2.248096653066054e-05, 2.1806537534740726e-05, 2.1152341408698503e-05, 2.051777116643755e-05, 1.9902238031444422e-05, 1.930517089050109e-05, 1.8726015763786056e-05, 1.8164235290872473e-05, 1.7619308232146297e-05, 1.709072898518191e-05, 1.657800711562645e-05, 1.6080666902157655e-05, 1.5598246895092924e-05, 1.5130299488240135e-05, 1.467639050359293e-05, 1.4236098788485143e-05, 1.3809015824830588e-05, 1.339474535008567e-05, 1.29929029895831e-05, 1.2603115899895607e-05, 1.2225022422898738e-05, 1.1858271750211776e-05, 1.1502523597705423e-05, 1.115744788977426e-05, 1.0822724453081032e-05, 1.04980427194886e-05, 1.0183101437903942e-05, 9.877608394766824e-06, 9.58128014292382e-06, 9.293841738636104e-06, 9.01502648647702e-06, 8.74457569188271e-06, 8.482238421126228e-06, 8.227771268492441e-06, 7.980938130437667e-06, 7.741509986524537e-06, 7.509264686928801e-06, 7.283986746320937e-06, 7.065467143931308e-06, 6.853503129613368e-06, 6.647898035724967e-06, 6.448461094653218e-06, 6.255007261813621e-06, 6.067357043959212e-06, 5.885336332640436e-06, 5.708776242661223e-06, 5.537512955381386e-06, 5.371387566719945e-06, 5.210245939718346e-06, 5.053938561526796e-06, 4.902320404680992e-06, 4.755250792540562e-06, 4.612593268764345e-06, 4.474215470701414e-06, 4.339989006580372e-06, 4.209789336382961e-06, 4.083495656291472e-06, 3.9609907866027276e-06, 3.842161063004645e-06, 3.726896231114506e-06, 3.615089344181071e-06, 3.5066366638556385e-06, 3.401437563939969e-06, 3.2993944370217698e-06, 3.2004126039111165e-06, 3.1044002257937827e-06, 3.011268219019969e-06, 2.92093017244937e-06, 2.8333022672758887e-06, 2.748303199257612e-06, 2.665854103279883e-06, 2.5858784801814865e-06, 2.508302125776042e-06, 2.4330530620027607e-06, 2.360061470142678e-06, 2.2892596260383974e-06, 2.2205818372572452e-06, 2.1539643821395277e-06, 2.089345450675342e-06, 2.0266650871550817e-06, 1.965865134540429e-06, 1.906889180504216e-06, 1.8496825050890897e-06, 1.7941920299364168e-06, 1.7403662690383242e-06, 1.6881552809671744e-06, 1.6375106225381592e-06, 1.5883853038620143e-06, 1.5407337447461538e-06, 1.4945117324037692e-06, 1.4496763804316562e-06, 1.4061860890187065e-06, 1.3640005063481453e-06, 1.323080491157701e-06, 1.28338807642297e-06, 1.2448864341302807e-06, 1.2075398411063722e-06, 1.171313645873181e-06, 1.1361742364969855e-06, 1.102089009402076e-06, 1.0690263391200138e-06, 1.0369555489464133e-06, 1.005846882478021e-06, 9.756714760036803e-07, 9.464013317235699e-07, 9.180092917718628e-07, 8.904690130187069e-07, 8.637549426281456e-07, 8.378422943493012e-07, 8.127070255188221e-07, 7.883258147532575e-07, 7.646760403106598e-07, 7.4173575910134e-07, 7.194836863282998e-07, 6.978991757384507e-07, 6.769622004662972e-07, 6.566533344523082e-07, 6.36953734418739e-07, 6.178451223861768e-07, 5.993097687145914e-07, 5.813304756531536e-07, 5.63890561383559e-07, 5.469738445420523e-07, 5.305646292057906e-07, 5.146476903296169e-07, 4.992082596197284e-07, 4.842320118311365e-07, 4.697050514762024e-07, 4.556138999319163e-07, 4.419454829339588e-07, 4.2868711844594e-07, 4.158265048925618e-07, 4.0335170974578496e-07, 3.912511584534114e-07, 3.795136236998091e-07, 3.681282149888148e-07, 3.570843685391504e-07, 3.4637183748297585e-07, 3.359806823584866e-07, 3.25901261887732e-07, 3.161242240311e-07, 3.06640497310167e-07, 2.9744128239086196e-07, 2.885180439191361e-07, 2.79862502601562e-07, 2.7146662752351515e-07, 2.633226286978097e-07, 2.554229498368754e-07, 2.477602613417691e-07, 2.4032745350151604e-07]\n",
      "Epoch 40, Batch 0, Train Loss: 0.03344977647066116, Memory (GB): 7\n",
      "Epoch 40, Batch 1, Train Loss: 0.03292836621403694, Memory (GB): 7\n",
      "Epoch 40, Batch 2, Train Loss: 0.04411223530769348, Memory (GB): 7\n",
      "Epoch 40, Batch 3, Train Loss: 0.04279685020446777, Memory (GB): 7\n",
      "Epoch 40, Batch 4, Train Loss: 0.036106135696172714, Memory (GB): 7\n",
      "Epoch 40, Batch 5, Train Loss: 0.032921236008405685, Memory (GB): 7\n",
      "Epoch 40, Batch 6, Train Loss: 0.03790375590324402, Memory (GB): 7\n",
      "Epoch 40, Batch 7, Train Loss: 0.03197536990046501, Memory (GB): 7\n",
      "Epoch 40, Batch 8, Train Loss: 0.034524302929639816, Memory (GB): 7\n",
      "Epoch 40, Batch 9, Train Loss: 0.030406072735786438, Memory (GB): 7\n",
      "Epoch 40, Batch 10, Train Loss: 0.03201967850327492, Memory (GB): 7\n",
      "Epoch 40, Batch 11, Train Loss: 0.041454069316387177, Memory (GB): 7\n",
      "Epoch 40, Batch 12, Train Loss: 0.03221962973475456, Memory (GB): 7\n",
      "Epoch 40, Batch 13, Train Loss: 0.041790641844272614, Memory (GB): 7\n",
      "Epoch 40, Batch 14, Train Loss: 0.04006631672382355, Memory (GB): 7\n",
      "Epoch 40, Batch 15, Train Loss: 0.033635932952165604, Memory (GB): 7\n",
      "Epoch 40, Batch 16, Train Loss: 0.03481638431549072, Memory (GB): 7\n",
      "Epoch 40, Batch 17, Train Loss: 0.03340637683868408, Memory (GB): 7\n",
      "Epoch 40, Batch 18, Train Loss: 0.04163600131869316, Memory (GB): 7\n",
      "Epoch 40, Batch 19, Train Loss: 0.04344872012734413, Memory (GB): 7\n",
      "Epoch 40, Batch 20, Train Loss: 0.031830817461013794, Memory (GB): 7\n",
      "Epoch 40, Batch 21, Train Loss: 0.031912557780742645, Memory (GB): 7\n",
      "Epoch 40, Batch 22, Train Loss: 0.03600139170885086, Memory (GB): 7\n",
      "Epoch 40, Batch 23, Train Loss: 0.030755076557397842, Memory (GB): 7\n",
      "Epoch 40, Batch 24, Train Loss: 0.03520556911826134, Memory (GB): 7\n",
      "Epoch 40, Batch 25, Train Loss: 0.03475847840309143, Memory (GB): 7\n",
      "Epoch 40, Batch 26, Train Loss: 0.03194022178649902, Memory (GB): 7\n",
      "Epoch 40, Batch 27, Train Loss: 0.032246027141809464, Memory (GB): 7\n",
      "Epoch 40, Batch 28, Train Loss: 0.04740815609693527, Memory (GB): 7\n",
      "Epoch 40, Batch 29, Train Loss: 0.033378105610609055, Memory (GB): 7\n",
      "Epoch 40, Batch 30, Train Loss: 0.032362885773181915, Memory (GB): 7\n",
      "Epoch 40, Batch 31, Train Loss: 0.03308909386396408, Memory (GB): 7\n",
      "Epoch 40, Batch 32, Train Loss: 0.032631732523441315, Memory (GB): 7\n",
      "Epoch 40, Batch 33, Train Loss: 0.034492671489715576, Memory (GB): 7\n",
      "Epoch 40, Batch 34, Train Loss: 0.03345312923192978, Memory (GB): 7\n",
      "Epoch 40, Batch 35, Train Loss: 0.03345821052789688, Memory (GB): 7\n",
      "Epoch 40, Batch 36, Train Loss: 0.034233931452035904, Memory (GB): 7\n",
      "Epoch 40, Batch 37, Train Loss: 0.044511016458272934, Memory (GB): 7\n",
      "Epoch 40, Batch 38, Train Loss: 0.03648271784186363, Memory (GB): 7\n",
      "Epoch 40, Batch 39, Train Loss: 0.03452683612704277, Memory (GB): 7\n",
      "Epoch 40, Batch 40, Train Loss: 0.03172535076737404, Memory (GB): 7\n",
      "Epoch 40, Batch 41, Train Loss: 0.032969482243061066, Memory (GB): 7\n",
      "Epoch 40, Batch 42, Train Loss: 0.02901715412735939, Memory (GB): 7\n",
      "Epoch 40, Batch 43, Train Loss: 0.032898154109716415, Memory (GB): 7\n",
      "Epoch 40, Batch 44, Train Loss: 0.03073832578957081, Memory (GB): 7\n",
      "Epoch 40, Batch 45, Train Loss: 0.032935887575149536, Memory (GB): 7\n",
      "Epoch 40, Batch 46, Train Loss: 0.03314785286784172, Memory (GB): 7\n",
      "Epoch 40, Batch 47, Train Loss: 0.03434138000011444, Memory (GB): 7\n",
      "Epoch 40, Batch 48, Train Loss: 0.034745316952466965, Memory (GB): 7\n",
      "Epoch 40, Batch 49, Train Loss: 0.03355645388364792, Memory (GB): 7\n",
      "Epoch 40, Batch 50, Train Loss: 0.03820206597447395, Memory (GB): 7\n",
      "Epoch 40, Batch 51, Train Loss: 0.039800889790058136, Memory (GB): 7\n",
      "Epoch 40, Batch 52, Train Loss: 0.03606889769434929, Memory (GB): 7\n",
      "Epoch 40, Batch 53, Train Loss: 0.03362945467233658, Memory (GB): 7\n",
      "Epoch 40, Batch 54, Train Loss: 0.03971527889370918, Memory (GB): 7\n",
      "Epoch 40, Batch 55, Train Loss: 0.033128347247838974, Memory (GB): 7\n",
      "Epoch 40, Batch 56, Train Loss: 0.03549596294760704, Memory (GB): 7\n",
      "Epoch 40, Batch 57, Train Loss: 0.03780517354607582, Memory (GB): 7\n",
      "Epoch 40, Batch 58, Train Loss: 0.0391475073993206, Memory (GB): 7\n",
      "Epoch 40, Batch 59, Train Loss: 0.03401670232415199, Memory (GB): 7\n",
      "Epoch 40, Batch 60, Train Loss: 0.032423846423625946, Memory (GB): 7\n",
      "Epoch 40, Batch 61, Train Loss: 0.03716461360454559, Memory (GB): 7\n",
      "Epoch 40, Batch 62, Train Loss: 0.032413918524980545, Memory (GB): 7\n",
      "Epoch 40, Batch 63, Train Loss: 0.03566371276974678, Memory (GB): 7\n",
      "Epoch 40, Batch 64, Train Loss: 0.031648047268390656, Memory (GB): 7\n",
      "Epoch 40, Batch 65, Train Loss: 0.03079211711883545, Memory (GB): 7\n",
      "Epoch 40, Batch 66, Train Loss: 0.032593801617622375, Memory (GB): 7\n",
      "Epoch 40, Batch 67, Train Loss: 0.04007353261113167, Memory (GB): 7\n",
      "Epoch 40, Batch 68, Train Loss: 0.0371767058968544, Memory (GB): 7\n",
      "Epoch 40, Batch 69, Train Loss: 0.03411659598350525, Memory (GB): 7\n",
      "Epoch 40, Batch 70, Train Loss: 0.03720447048544884, Memory (GB): 7\n",
      "Epoch 40, Batch 71, Train Loss: 0.03865590691566467, Memory (GB): 7\n",
      "Epoch 40, Batch 72, Train Loss: 0.03549490496516228, Memory (GB): 7\n",
      "Epoch 40, Batch 73, Train Loss: 0.03225712105631828, Memory (GB): 7\n",
      "Epoch 40, Batch 74, Train Loss: 0.03280780836939812, Memory (GB): 7\n",
      "Epoch 40, Batch 75, Train Loss: 0.032917458564043045, Memory (GB): 7\n",
      "Epoch 40, Batch 76, Train Loss: 0.043049272149801254, Memory (GB): 7\n",
      "Epoch 40, Batch 77, Train Loss: 0.03752724081277847, Memory (GB): 7\n",
      "Epoch 40, Batch 78, Train Loss: 0.036608438938856125, Memory (GB): 7\n",
      "Epoch 40, Batch 79, Train Loss: 0.03533545508980751, Memory (GB): 7\n",
      "Epoch 40, Batch 80, Train Loss: 0.033036425709724426, Memory (GB): 7\n",
      "Epoch 40, Batch 81, Train Loss: 0.03606817498803139, Memory (GB): 7\n",
      "Epoch 40, Batch 82, Train Loss: 0.035484544932842255, Memory (GB): 7\n",
      "Epoch 40, Batch 83, Train Loss: 0.04326909780502319, Memory (GB): 7\n",
      "Epoch 40, Batch 84, Train Loss: 0.03591101989150047, Memory (GB): 7\n",
      "Epoch 40, Batch 85, Train Loss: 0.030970150604844093, Memory (GB): 7\n",
      "Epoch 40, Batch 86, Train Loss: 0.03058306686580181, Memory (GB): 7\n",
      "Epoch 40, Batch 87, Train Loss: 0.038670748472213745, Memory (GB): 7\n",
      "Epoch 40, Batch 88, Train Loss: 0.032034363597631454, Memory (GB): 7\n",
      "Epoch 40, Batch 89, Train Loss: 0.034452080726623535, Memory (GB): 7\n",
      "Epoch 40, Batch 90, Train Loss: 0.03375353664159775, Memory (GB): 7\n",
      "Epoch 40, Batch 91, Train Loss: 0.04311376065015793, Memory (GB): 7\n",
      "Epoch 40, Batch 92, Train Loss: 0.04154727980494499, Memory (GB): 7\n",
      "Epoch 40, Batch 93, Train Loss: 0.032702066004276276, Memory (GB): 7\n",
      "Epoch 40, Batch 94, Train Loss: 0.04023208096623421, Memory (GB): 7\n",
      "Epoch 40, Batch 95, Train Loss: 0.03436030074954033, Memory (GB): 7\n",
      "Epoch 40, Batch 96, Train Loss: 0.04090127348899841, Memory (GB): 7\n",
      "Epoch 40, Batch 97, Train Loss: 0.04005890712141991, Memory (GB): 7\n",
      "Epoch 40, Batch 98, Train Loss: 0.037012845277786255, Memory (GB): 7\n",
      "Epoch 40, Batch 99, Train Loss: 0.03570995852351189, Memory (GB): 7\n",
      "Epoch 40, Batch 100, Train Loss: 0.03503437712788582, Memory (GB): 7\n",
      "Epoch 40, Batch 101, Train Loss: 0.0401892364025116, Memory (GB): 7\n",
      "Epoch 40, Batch 102, Train Loss: 0.030695410445332527, Memory (GB): 7\n",
      "Epoch 40, Batch 103, Train Loss: 0.04409581795334816, Memory (GB): 7\n",
      "Epoch 40, Batch 104, Train Loss: 0.04716693237423897, Memory (GB): 7\n",
      "Epoch 40, Batch 105, Train Loss: 0.04467935860157013, Memory (GB): 7\n",
      "Epoch 40, Batch 106, Train Loss: 0.04193905368447304, Memory (GB): 7\n",
      "Epoch 40, Batch 107, Train Loss: 0.032807979732751846, Memory (GB): 7\n",
      "Epoch 40, Batch 108, Train Loss: 0.040082480758428574, Memory (GB): 7\n",
      "Epoch 40, Batch 109, Train Loss: 0.0358162447810173, Memory (GB): 7\n",
      "Epoch 40, Batch 110, Train Loss: 0.035261742770671844, Memory (GB): 7\n",
      "Epoch 40, Batch 111, Train Loss: 0.03450995311141014, Memory (GB): 7\n",
      "Epoch 40, Batch 112, Train Loss: 0.03972427919507027, Memory (GB): 7\n",
      "Epoch 40, Batch 113, Train Loss: 0.0328444242477417, Memory (GB): 7\n",
      "Epoch 40, Batch 114, Train Loss: 0.03369811177253723, Memory (GB): 7\n",
      "Epoch 40, Batch 115, Train Loss: 0.0352996289730072, Memory (GB): 7\n",
      "Epoch 40, Batch 116, Train Loss: 0.03792497515678406, Memory (GB): 7\n",
      "Epoch 40, Batch 117, Train Loss: 0.03245635703206062, Memory (GB): 7\n",
      "Epoch 40, Batch 118, Train Loss: 0.05580544471740723, Memory (GB): 7\n",
      "Epoch 40, Batch 119, Train Loss: 0.035528842359781265, Memory (GB): 7\n",
      "Epoch 40, Batch 120, Train Loss: 0.03418431058526039, Memory (GB): 7\n",
      "Epoch 40, Batch 121, Train Loss: 0.039078861474990845, Memory (GB): 7\n",
      "Epoch 40, Batch 122, Train Loss: 0.03485844284296036, Memory (GB): 7\n",
      "Epoch 40, Batch 123, Train Loss: 0.03558642417192459, Memory (GB): 7\n",
      "Epoch 40, Batch 124, Train Loss: 0.03600802272558212, Memory (GB): 7\n",
      "Epoch 40, Batch 125, Train Loss: 0.034107692539691925, Memory (GB): 7\n",
      "Epoch 40, Batch 126, Train Loss: 0.037971533834934235, Memory (GB): 7\n",
      "Epoch 40, Batch 127, Train Loss: 0.03279639035463333, Memory (GB): 7\n",
      "Epoch 40, Batch 128, Train Loss: 0.03828826919198036, Memory (GB): 7\n",
      "Epoch 40, Batch 129, Train Loss: 0.03692105412483215, Memory (GB): 7\n",
      "Epoch 40, Batch 130, Train Loss: 0.0293692946434021, Memory (GB): 7\n",
      "Epoch 40, Batch 131, Train Loss: 0.03378083556890488, Memory (GB): 7\n",
      "Epoch 40, Batch 132, Train Loss: 0.03173113986849785, Memory (GB): 7\n",
      "Epoch 40, Batch 133, Train Loss: 0.03388827294111252, Memory (GB): 7\n",
      "Epoch 40, Batch 134, Train Loss: 0.0326080322265625, Memory (GB): 7\n",
      "Epoch 40, Batch 135, Train Loss: 0.03239530697464943, Memory (GB): 7\n",
      "Epoch 40, Batch 136, Train Loss: 0.031083345413208008, Memory (GB): 7\n",
      "Epoch 40, Batch 137, Train Loss: 0.04171999171376228, Memory (GB): 7\n",
      "Epoch 40, Batch 138, Train Loss: 0.036083828657865524, Memory (GB): 7\n",
      "Epoch 40, Batch 139, Train Loss: 0.034000854939222336, Memory (GB): 7\n",
      "Epoch 40, Batch 140, Train Loss: 0.03608110547065735, Memory (GB): 7\n",
      "Epoch 40, Batch 141, Train Loss: 0.03517790138721466, Memory (GB): 7\n",
      "Epoch 40, Batch 142, Train Loss: 0.03647931292653084, Memory (GB): 7\n",
      "Epoch 40, Batch 143, Train Loss: 0.03410103917121887, Memory (GB): 7\n",
      "Epoch 40, Batch 144, Train Loss: 0.03255604952573776, Memory (GB): 7\n",
      "Epoch 40, Batch 145, Train Loss: 0.036434803158044815, Memory (GB): 7\n",
      "Epoch 40, Batch 146, Train Loss: 0.0385519340634346, Memory (GB): 7\n",
      "Epoch 40, Batch 147, Train Loss: 0.03398812934756279, Memory (GB): 7\n",
      "Epoch 40, Batch 148, Train Loss: 0.03425833582878113, Memory (GB): 7\n",
      "Epoch 40, Batch 149, Train Loss: 0.035458337515592575, Memory (GB): 7\n",
      "Epoch 40, Batch 150, Train Loss: 0.034027621150016785, Memory (GB): 7\n",
      "Epoch 40, Batch 151, Train Loss: 0.03581234812736511, Memory (GB): 7\n",
      "Epoch 40, Batch 152, Train Loss: 0.03568175435066223, Memory (GB): 7\n",
      "Epoch 40, Batch 153, Train Loss: 0.03649074211716652, Memory (GB): 7\n",
      "Epoch 40, Batch 154, Train Loss: 0.03524264693260193, Memory (GB): 7\n",
      "Epoch 40, Batch 155, Train Loss: 0.0316554419696331, Memory (GB): 7\n",
      "Epoch 40, Batch 156, Train Loss: 0.035727400332689285, Memory (GB): 7\n",
      "Epoch 40, Batch 157, Train Loss: 0.04073754698038101, Memory (GB): 7\n",
      "Epoch 40, Batch 158, Train Loss: 0.03567295894026756, Memory (GB): 7\n",
      "Epoch 40, Batch 159, Train Loss: 0.03196961060166359, Memory (GB): 7\n",
      "Epoch 40, Batch 160, Train Loss: 0.03579689562320709, Memory (GB): 7\n",
      "Epoch 40, Batch 161, Train Loss: 0.032692842185497284, Memory (GB): 7\n",
      "Epoch 40, Batch 162, Train Loss: 0.036221496760845184, Memory (GB): 7\n",
      "Epoch 40, Batch 163, Train Loss: 0.03569282963871956, Memory (GB): 7\n",
      "Epoch 40, Batch 164, Train Loss: 0.04177761822938919, Memory (GB): 7\n",
      "Epoch 40, Batch 165, Train Loss: 0.03700736537575722, Memory (GB): 7\n",
      "Epoch 40, Batch 166, Train Loss: 0.03764631599187851, Memory (GB): 7\n",
      "Epoch 40, Batch 167, Train Loss: 0.03971732407808304, Memory (GB): 7\n",
      "Epoch 40, Batch 168, Train Loss: 0.0321987085044384, Memory (GB): 7\n",
      "Epoch 40, Batch 169, Train Loss: 0.03298508748412132, Memory (GB): 7\n",
      "Epoch 40, Batch 170, Train Loss: 0.03540476784110069, Memory (GB): 7\n",
      "Epoch 40, Batch 171, Train Loss: 0.034273918718099594, Memory (GB): 7\n",
      "Epoch 40, Batch 172, Train Loss: 0.03777236118912697, Memory (GB): 7\n",
      "Epoch 40, Batch 173, Train Loss: 0.03367107734084129, Memory (GB): 7\n",
      "Epoch 40, Batch 174, Train Loss: 0.03543827682733536, Memory (GB): 7\n",
      "Epoch 40, Batch 175, Train Loss: 0.02967272512614727, Memory (GB): 7\n",
      "Epoch 40, Batch 176, Train Loss: 0.04644954949617386, Memory (GB): 7\n",
      "Epoch 40, Batch 177, Train Loss: 0.039055150002241135, Memory (GB): 7\n",
      "Epoch 40, Batch 178, Train Loss: 0.03146728500723839, Memory (GB): 7\n",
      "Epoch 40, Batch 179, Train Loss: 0.02998632937669754, Memory (GB): 7\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "fine_tune(model, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdd1ad5f-6b2f-4d95-b21f-7028f65fd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"temp_model_parameters40.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "48e5c591-fd6e-49b2-9dbe-4722f9cd0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data(\"chart_data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b616ce1-65a7-4bc1-a2d2-1d309943645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "import gc\n",
    "#model.eval()\n",
    "# .35 after 26 epochs\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = 96, shuffle=True) \n",
    "\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with autocast():  # Mixed precision\n",
    "        loss, _, _ = model(batch, mask_ratio=1/(model.patch_embed.patch_size[0] - 2))\n",
    "\n",
    "    print(\"Random batch validation loss:\", loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d69e07-dd56-4220-855e-b230006270d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 84,  83,  84],\n",
       "         [ 84,  83,  84],\n",
       "         [ 84,  83,  84],\n",
       "         ...,\n",
       "         [ 84,  83,  84],\n",
       "         [ 84,  83,  84],\n",
       "         [ 84,  83,  84]],\n",
       "\n",
       "        [[254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         ...,\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255]],\n",
       "\n",
       "        [[254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         ...,\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         ...,\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255]],\n",
       "\n",
       "        [[254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         ...,\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255]],\n",
       "\n",
       "        [[254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         ...,\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255],\n",
       "         [254, 255, 255]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test = load_data(\"chart_data/test\")\n",
    "img = test[924].to(device) #50\n",
    "#42\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "show_image(img, title = 'sample stock chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4a56a6eb-ff22-4d71-b502-fea1021ffc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def dominant_color(image):\n",
    "\n",
    "    # Convert image to a numpy array\n",
    "    #image_array = np.array(image)\n",
    "    \n",
    " \n",
    "    # Convert image to a numpy array (OpenCV uses BGR, so convert to array first)\n",
    "    #print(image.shape)\n",
    "    image = image.astype(np.uint8)\n",
    "    image_array = np.array(image).reshape((224, 5, 3))\n",
    "    #print(image_array.shape)\n",
    "    #image_array = image_array[:, 2:3, :]\n",
    "\n",
    "    np.set_printoptions(threshold=np.inf)  # This disables truncation of large arrays\n",
    "    \n",
    "    # Convert to HSV color space using OpenCV (it expects BGR, so reorder)\n",
    "    #image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
    "    image_hsv = cv2.cvtColor(image_array, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    # Extract HSV channels\n",
    "    hue = image_hsv[:, :, 0]\n",
    "    #print(hue)\n",
    "    saturation = image_hsv[:, :, 1]\n",
    "    value = image_hsv[:, :, 2]\n",
    "\n",
    "     # Define Red and Green Hue Ranges more carefully\n",
    "    # Red Hue Range: 0-10 and 170-180 degrees\n",
    "    red_pixels = ((hue >= 0) & (hue <= 12)) | ((hue >= 165) & (hue <= 180))\n",
    "    \n",
    "    # Green Hue Range: 35-85 degrees\n",
    "    green_pixels = (hue >= 30) & (hue <= 90)\n",
    "    \n",
    "    # Optional: Apply thresholds for saturation and brightness\n",
    "    # Filter out low saturation and low value pixels that might cause misclassification\n",
    "    high_saturation = saturation > 2.0  # Threshold for saturation\n",
    "    high_value = value > 2.8  # Threshold for brightness (value)\n",
    "    \n",
    "    # Apply these thresholds to both red and green pixels\n",
    "    red_pixels = red_pixels & high_saturation & high_value\n",
    "    green_pixels = green_pixels & high_saturation & high_value\n",
    "\n",
    "    # Apply the non-white pixel mask to both red and green pixels\n",
    "    #red_pixels = red_pixels & non_white_pixels\n",
    "    #green_pixels = green_pixels & non_white_pixels\n",
    "    \n",
    "    # Calculate the weighted sum of intensity (saturation and value) for red and green\n",
    "    red_intensity = np.sum(value[red_pixels])\n",
    "    green_intensity = np.sum(value[green_pixels])\n",
    "    \n",
    "    # Compare the summed intensities of red and green\n",
    "    if red_intensity > green_intensity:\n",
    "        return 0\n",
    "    elif green_intensity > red_intensity:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#result = dominant_color(np.array(output[:, -15:-10, :]))\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7eafe06-5ff7-46e9-8158-3a56d375e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_or_down(outputs):\n",
    "    \"\"\"\n",
    "        outputs: shape [N, 3, 224, 224]\n",
    "        output: shape [N, 2]\n",
    "    \"\"\"\n",
    "    N = outputs.size()[0]\n",
    "   \n",
    "    images = (torch.einsum('nchw->nhwc', outputs).cpu() * imagenet_std + imagenet_mean) - 1 # subtract white\n",
    "    sec_last, last = torch.split(images[:, 1:, -15:-5,:], 5, dim=2) # 2, [N, 223, 5, 3]\n",
    "\n",
    "    res = torch.empty((N, 2))\n",
    "   \n",
    "    for idx, slice in enumerate(sec_last):\n",
    "        slice = slice[torch.linalg.vector_norm(slice, dim=2) >= 1e-4] # remove white pixels\n",
    "        if slice.size()[0] == 0:\n",
    "            continue # bad image\n",
    "        res[idx, 0] = torch.mode(slice.argmax(dim=1)).values.item()\n",
    "\n",
    "    for idx, slice in enumerate(last):\n",
    "        slice = slice[torch.linalg.vector_norm(slice, dim=2) >= 1e-4] # remove white pixels\n",
    "        if slice.size()[0] == 0:\n",
    "            continue # bad image\n",
    "        res[idx, 1] = torch.mode(slice.argmax(dim=1)).values.item()\n",
    "       \n",
    "    return res[0, 0], res[0, 1]\n",
    "\n",
    "#up_or_down(torch.reshape(img, (1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ccae8464-adf7-4bef-8791-0ef38e057d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "Next Day Accuracy (%):  65.44916090819348\n",
      "Second Day Accuracy (%):  62.94833826916749\n",
      "Both Day Accuracy (%):  42.61270154656137\n"
     ]
    }
   ],
   "source": [
    "test = load_data(\"chart_data/test\")\n",
    "correct_day1_predictions = 0\n",
    "correct_day2_predictions = 0\n",
    "correct_days_predictions = 0\n",
    "cor_candle1_ind = []\n",
    "incorrect = []\n",
    "cor_candle2_ind = []\n",
    "cor_candles_ind = []\n",
    "candle_1_output_labels = []\n",
    "candle_1_true_labels = []\n",
    "matplotlib.use('Agg')\n",
    "for i in range(len(test)):\n",
    "    #print(i)\n",
    "    img = test[i].to(device)\n",
    "    output = run_one_image(img, model)\n",
    "    candle_1_output = dominant_color(np.array(output[:, -15:-10, :]))\n",
    "    candle_2_output = dominant_color(np.array(output[:, -10:-5, :]))\n",
    "    candle1_img, candle2_img = up_or_down(torch.reshape(img, (1, 3, 224, 224)))\n",
    "    candle_1_true_labels.append(candle1_img)\n",
    "    candle_1_output_labels.append(candle_1_output)\n",
    "    if candle_1_output == candle1_img: \n",
    "        correct_day1_predictions += 1\n",
    "        cor_candle1_ind.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)\n",
    "    if candle_2_output == candle2_img: \n",
    "        correct_day2_predictions += 1\n",
    "        cor_candle2_ind.append(i)\n",
    "    if candle_2_output == candle2_img and candle_1_output == candle1_img:\n",
    "        correct_days_predictions += 1\n",
    "        cor_candles_ind.append(i)\n",
    "        \n",
    "print(\"Next Day Accuracy (%): \", correct_day1_predictions/len(test) * 100)\n",
    "print(\"Second Day Accuracy (%): \", correct_day2_predictions/len(test) * 100)\n",
    "print(\"Both Day Accuracy (%): \", correct_days_predictions/len(test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "080bba5a-d6ac-48e2-8910-17ef0af3bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 1224\n",
      "False Positives (FP): 429\n",
      "True Negatives (TN): 765\n",
      "False Negatives (FN): 617\n",
      "Precision: 0.7405\n",
      "Recall: 0.6649\n",
      "F1-score: 0.7006\n",
      "Accuracy: 0.6554\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array(candle_1_output_labels)\n",
    "y_true = np.array(candle_1_true_labels)\n",
    "# Compute TP, FP, TN, FN\n",
    "TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "# Compute precision, recall, f1-score, accuracy\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0  # Avoid division by zero\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b0206456-29d2-48bd-828a-8ac683d8eddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "Next Day Accuracy (%):  55.45851528384279\n",
      "Second Day Accuracy (%):  40.61135371179039\n",
      "Both Day Accuracy (%):  23.362445414847162\n"
     ]
    }
   ],
   "source": [
    "test = load_data(\"chartsfinal\")\n",
    "correct_day1_predictions = 0\n",
    "correct_day2_predictions = 0\n",
    "correct_days_predictions = 0\n",
    "cor_candle1_ind = []\n",
    "cor_candle2_ind = []\n",
    "cor_candles_ind = []\n",
    "candle_1_output_labels = []\n",
    "candle_1_true_labels = []\n",
    "matplotlib.use('Agg')\n",
    "for i in range(len(test)):\n",
    "    #print(i)\n",
    "    img = test[i].to(device)\n",
    "    output = run_one_image(img, model)\n",
    "    candle_1_output = dominant_color(np.array(output[:, -15:-10, :]))\n",
    "    candle_2_output = dominant_color(np.array(output[:, -10:-5, :]))\n",
    "    candle1_img, candle2_img = up_or_down(torch.reshape(img, (1, 3, 224, 224)))\n",
    "    candle_1_true_labels.append(candle1_img)\n",
    "    candle_1_output_labels.append(candle_1_output)\n",
    "    if candle_1_output == candle1_img: \n",
    "        correct_day1_predictions += 1\n",
    "        cor_candle1_ind.append(i)\n",
    "    if candle_2_output == candle2_img: \n",
    "        correct_day2_predictions += 1\n",
    "        cor_candle2_ind.append(i)\n",
    "    if candle_2_output == candle2_img and candle_1_output == candle1_img:\n",
    "        correct_days_predictions += 1\n",
    "        cor_candles_ind.append(i)\n",
    "        \n",
    "print(\"Next Day Accuracy (%): \", correct_day1_predictions/len(test) * 100)\n",
    "print(\"Second Day Accuracy (%): \", correct_day2_predictions/len(test) * 100)\n",
    "print(\"Both Day Accuracy (%): \", correct_days_predictions/len(test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dc6dce8d-dce2-479e-b8ea-824139755895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 108\n",
      "False Positives (FP): 133\n",
      "True Negatives (TN): 146\n",
      "False Negatives (FN): 71\n",
      "Precision: 0.4481\n",
      "Recall: 0.6034\n",
      "F1-score: 0.5143\n",
      "Accuracy: 0.5546\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array(candle_1_output_labels)\n",
    "y_true = np.array(candle_1_true_labels)\n",
    "# Compute TP, FP, TN, FN\n",
    "TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "# Compute precision, recall, f1-score, accuracy\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0  # Avoid division by zero\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "# Print the results\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c4361910-0f9c-4034-a875-65fff6b8049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "186\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "2941\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "398\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def results_figure(indices, model, filename):\n",
    "    # make the plt figure larger\n",
    "    #plt.rcParams['figure.figsize'] = [8, 5]\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 18))\n",
    "    sample_indices = random.sample(indices, 4)\n",
    "    row = 0\n",
    "    for i in sample_indices:\n",
    "        print(i)\n",
    "        img = test[i].to(device)\n",
    "        x = img.unsqueeze(dim=0)\n",
    "    \n",
    "        # run MAE\n",
    "        with torch.no_grad():\n",
    "            _, y, mask = model(x.float(), mask_ratio=1/(model.patch_embed.patch_size[0] - 2))\n",
    "            y = model.unpatchify(y)\n",
    "            y = y.detach()\n",
    "    \n",
    "        # visualize the mask\n",
    "        mask = mask.detach()\n",
    "        mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
    "        mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "        mask = mask.detach()\n",
    "        \n",
    "        print(mask.size(), x.size())\n",
    "        \n",
    "        # masked image\n",
    "        im_masked = x * (1 - mask)\n",
    "    \n",
    "        # MAE reconstruction pasted with visible patches\n",
    "        im_paste = x * (1 - mask) + y * mask\n",
    "    \n",
    "    \n",
    "        #plt.subplot(row, 3, 1)\n",
    "        #show_image(x[0], \"original\")\n",
    "        x1 = torch.einsum('chw->hwc', x[0]) \n",
    "        axes[row, 0].imshow(torch.clip((x1.cpu() * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    \n",
    "        #plt.subplot(row, 3, 2)\n",
    "        #show_image(im_masked[0], \"masked\")\n",
    "        im_masked1 = torch.einsum('chw->hwc', im_masked[0]) \n",
    "        axes[row, 1].imshow(torch.clip((im_masked1.cpu() * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "\n",
    "\n",
    "        im_paste1 = torch.einsum('chw->hwc', im_paste[0]) \n",
    "        axes[row, 2].imshow(torch.clip((im_paste1.cpu() * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "\n",
    "        axes[row, 0].axis('off')\n",
    "        axes[row, 1].axis('off')\n",
    "        axes[row, 2].axis('off')\n",
    "        #plt.subplot(1, 4, 3)\n",
    "        #show_image(y[0], \"reconstruction\")\n",
    "    \n",
    "        #plt.subplot(row, 3, 3)\n",
    "        #output = show_image(im_paste[0], \"reconstruction + visible\")\n",
    "        # Plot data on each subplot\n",
    "        row+= 1\n",
    "    # Add column labels manually\n",
    "    fig.text(0.15, 0.99, 'Original', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    fig.text(0.5, 0.99, 'Masked', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    fig.text(0.85, 0.99, 'Reconstruction + Original', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    #plt.subplots_adjust(wspace=0.5, hspace=0)  # Remove all space between subplots\n",
    "    plt.tight_layout(pad=1)  # Ensure subplots are as tight as possible with no extra padding\n",
    "    plt.savefig(filename)\n",
    "results_figure(cor_candle1_ind, model, 'correct1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "593a4121-7006-4c76-86d7-1307146cebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2420\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "845\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "1858\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "2520\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "results_figure(cor_candles_ind, model, 'correctboth.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "07d3d19f-9234-492a-9060-a6e381e951e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1266\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "2764\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "1015\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "1441\n",
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "results_figure(incorrect, model, 'incorrect1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "673b9688-dd54-4b24-a5eb-9dd544f66abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data(\"chart_data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c157ae7e-c047-4f07-ad9f-b3456cc9834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 3, 224, 224])\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = test[2339].to(device)\n",
    "output = run_one_image(img, model)\n",
    "print(dominant_color(np.array(output[:, -15:-10, :])))\n",
    "dominant_color(np.array(output[:, -10:-5, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "18dc83f0-547f-43e7-978b-8e14231276cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_or_down(torch.reshape(img, (1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d184ce-c0f0-485a-9c8f-6fe17b190bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
